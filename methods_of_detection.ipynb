{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook wird der Datensatz base_fake_real.csv verwendet und untersucht wie man Fake-Reviews von Echten Reviews unterscheiden kann. Zuerst wird der Datensatz so vorbeietet um Features zu generieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import urllib\n",
    "from fastai.vision.all import *\n",
    "from fastdownload import download_url\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datensatz für Feature Extraction zu feature_base.csv vorbereitet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"base_fake_real.csv\")\n",
    "print(f\"Base Datensatz: {df.shape}\")\n",
    "# print(df.columns)\n",
    "\n",
    "df.drop(\n",
    "    [\n",
    "        \"index_fake\",\n",
    "        \"org_text\",\n",
    "        \"org_stars\",\n",
    "        \"sent_score_0\",\n",
    "        \"sent_v2\",\n",
    "        \"sent_v3\",\n",
    "        \"sent_v3.1\",\n",
    "        \"prompt_v3\",\n",
    "        \"website\",\n",
    "        \"dalle_prompt\",\n",
    "        \"website\",\n",
    "        \"prompt_v2\",\n",
    "        \"gpt3_v2\",\n",
    "        \"gpt3_v3\",\n",
    "        \"gpt3_v3.1\",\n",
    "        \"prompt_v4\",\n",
    "        \"org_reviewId\",\n",
    "        \"sent_v4\",\n",
    "        \"keywords\",\n",
    "        \"keywords_only\",\n",
    "        \"text_length\",\n",
    "        \"reviewImageUrls/3\",\n",
    "        \"reviewerPhotoUrl\",\n",
    "        \"reviewerUrl\",\n",
    "        \"reviewerId\",\n",
    "        \"temporarilyClosed\",\n",
    "        \"reviewsCount\",\n",
    "        \"street\",\n",
    "        \"state\",\n",
    "        \"totalScore\",\n",
    "        \"subTitle\",\n",
    "        \"description\",\n",
    "        \"price\",\n",
    "        \"sentiment\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "df = df.reindex(\n",
    "    columns=[\n",
    "        \"label\",\n",
    "        \"reviewId\",\n",
    "        \"placeId\",\n",
    "        \"reviewUrl\",\n",
    "        \"url\",\n",
    "        \"title\",\n",
    "        \"categoryName\",\n",
    "        \"genre\",\n",
    "        \"text\",\n",
    "        \"stars\",\n",
    "        \"publishedAtDate\",\n",
    "        \"likesCount\",\n",
    "        \"name\",\n",
    "        \"isLocalGuide\",\n",
    "        \"reviewerNumberOfReviews\",\n",
    "        \"reviewImageUrls/0\",\n",
    "        \"reviewImageUrls/1\",\n",
    "        \"reviewImageUrls/2\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Reduzierter Datensatz als neue Basis für FE: {df.shape}\")\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "\n",
    "df.to_csv(\"base_features.csv\", index=False)\n",
    "df.to_excel(\"base_features.xlsx\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular: Feature Generierung\n",
    "\n",
    "1. aus publishedAt das bestmögliche rausholen\n",
    "\n",
    "when_on_day_4hbin:\n",
    "Midnight: 0-4 hours\n",
    "Early morning: 4-8 hours\n",
    "Morning: 8-12 hours\n",
    "Early afternoon: 12-16 hours\n",
    "Late afternoon: 16-20 hours\n",
    "Evening: 20-24 hours -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"base_features.csv\")\n",
    "print(df[\"publishedAtDate\"][977])\n",
    "\n",
    "df[\"publishedAtDate\"] = pd.to_datetime(\n",
    "    df[\"publishedAtDate\"], format=\"%Y-%m-%dT%H:%M:%S\"\n",
    ")\n",
    "\n",
    "df[\"year\"] = df[\"publishedAtDate\"].dt.year\n",
    "df[\"month\"] = df[\"publishedAtDate\"].dt.month\n",
    "df[\"dayofweek\"] = df[\"publishedAtDate\"].dt.dayofweek\n",
    "df[\"elapsed_days\"] = (datetime.today() - df[\"publishedAtDate\"]).dt.days\n",
    "df[\"when_on_day_4hbin\"] = pd.cut(\n",
    "    df[\"publishedAtDate\"].dt.hour,\n",
    "    bins=[-1, 4, 8, 12, 16, 20, 24],\n",
    "    labels=[0, 1, 2, 3, 4, 5],\n",
    ")\n",
    "\n",
    "df[\"when_on_day_hour\"] = df[\"publishedAtDate\"].dt.hour\n",
    "\n",
    "print(df[\"when_on_day_4hbin\"].isna().sum())\n",
    "print(df.loc[df[\"when_on_day_4hbin\"].isna(), \"publishedAtDate\"])\n",
    "\n",
    "print(\n",
    "    df[\n",
    "        [\n",
    "            \"publishedAtDate\",\n",
    "            \"year\",\n",
    "            \"month\",\n",
    "            \"dayofweek\",\n",
    "            \"elapsed_days\",\n",
    "            \"when_on_day_4hbin\",\n",
    "            \"when_on_day_hour\",\n",
    "        ]\n",
    "    ].head(10)\n",
    ")\n",
    "\n",
    "df.to_csv(\"feature_enriched_tab.csv\", index=False)\n",
    "df.to_excel(\"feature_enriched_tab.xlsx\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nur für mich ein Test, wie ich ein Basic Decsion Tree anwende und für grobes Gefühl, wie aussagekräftig das alles ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"feature_enriched_tab.csv\")\n",
    "\n",
    "df = df[\n",
    "    [\n",
    "        \"stars\",\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"likesCount\",\n",
    "        \"reviewerNumberOfReviews\",\n",
    "        \"isLocalGuide\",\n",
    "        \"dayofweek\",\n",
    "        \"elapsed_days\",\n",
    "        \"when_on_day_4hbin\",\n",
    "        \"label\",\n",
    "    ]\n",
    "]\n",
    "df[\"label\"] = (df[\"label\"] == \"real\").astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(\"label\", axis=1), df[\"label\"], test_size=0.2, random_state=11\n",
    ")\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Decision tree accuracy: {score:.2f}\")\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
    "\n",
    "plt.ylabel(\"Relative Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bild: Feature Generierung\n",
    "\n",
    "### Feature Extrahieren über Pretrained ResNet-18 Architektur und in Dataframe abspeichern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"feature_enriched_tab.csv\")\n",
    "image_urls = df[\"reviewImageUrls/0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "# Remove the last fully connected layer\n",
    "modules = list(resnet.children())[:-1]\n",
    "resnet = torch.nn.Sequential(*modules)\n",
    "# Set the model to evaluation mode\n",
    "resnet.eval()\n",
    "\n",
    "# Define a function to extract features for a single image\n",
    "def extract_image_features(image_url):\n",
    "    # Load image and preprocess\n",
    "    img = Image.open(requests.get(image_url, stream=True).raw)\n",
    "    img = transforms.ToTensor()(img)\n",
    "    img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = resnet(img)\n",
    "        features = features.squeeze().numpy()\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "counter = 0\n",
    "feature_vectors = []\n",
    "for image_url in image_urls:\n",
    "    try:\n",
    "        counter += 1\n",
    "        print(f'{counter}:\\tExtracting features from {image_url}')\n",
    "        features = extract_image_features(image_url)\n",
    "    except:\n",
    "        print(f'Error extracting features from {image_url}. Replaces with NaN.')\n",
    "        features = np.full((512,), np.nan)\n",
    "    feature_vectors.append(features)\n",
    "\n",
    "feature_df = pd.DataFrame(feature_vectors, columns=[f'feature_{i}' for i in range(512)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([df, feature_df], axis=1)\n",
    "print(\"added features to the original dataset.\")\n",
    "\n",
    "print(new_df.iloc[0])\n",
    "# new_df.to_csv('feature_enriched_tab_img.csv', index=False)\n",
    "# new_df.to_excel('feature_enriched_tab_img.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize as PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load your dataframe\n",
    "df = pd.read_csv('feature_enriched_tab_img.csv')\n",
    "\n",
    "# drop row with index 764\n",
    "df = df.drop(764)\n",
    "\n",
    "# Select feature columns and label\n",
    "# can you drop all \n",
    "features = df.iloc[:, 0:1].join(df.iloc[:, 24:])\n",
    "\n",
    "\n",
    "# drop all rows with NaN values in feature columns\n",
    "features = features.dropna(subset=features.columns[1:], how='all')\n",
    "\n",
    "# Define color map\n",
    "color_map = {'fake': 'red', 'real': 'green'}\n",
    "\n",
    "# Map labels to colors\n",
    "colors = features['label'].apply(lambda x: color_map[x])\n",
    "\n",
    "# Apply t-SNE to reduce dimensionality\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200)\n",
    "tsne_features = tsne.fit_transform(features.iloc[:, 1:])\n",
    "\n",
    "# Visualize the reduced features, colored by label\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=colors, alpha=0.5)\n",
    "plt.title('t-SNE visualization of features')\n",
    "plt.xlabel('t-SNE feature 1')\n",
    "plt.ylabel('t-SNE feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataframe\n",
    "df = pd.read_csv('feature_enriched_tab_img.csv')\n",
    "\n",
    "# drop row with index 764\n",
    "df = df.drop(764)\n",
    "\n",
    "# Select feature columns and label\n",
    "features = df.iloc[:, 0:1].join(df.iloc[:, 24:])\n",
    "\n",
    "# drop all rows with NaN values in feature columns\n",
    "features = features.dropna(subset=features.columns[1:], how='all')\n",
    "\n",
    "# Define color map\n",
    "color_map = {'fake': 'red', 'real': 'green'}\n",
    "\n",
    "# Map labels to colors\n",
    "colors = features['label'].apply(lambda x: color_map[x])\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=2)\n",
    "pca_features = pca.fit_transform(features.iloc[:, 1:])\n",
    "\n",
    "# Visualize the reduced features, colored by label\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_features[:, 0], pca_features[:, 1], c=colors, alpha=0.5, s=50)\n",
    "plt.xlabel('PCA Komponente 1', fontsize=14)\n",
    "plt.ylabel('PCA Komponente 2', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=10)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.legend(handles=[plt.scatter([], [], c='red', label='Fake Bild', alpha=0.5, s=50),\n",
    "                     plt.scatter([], [], c='green', label='Echtes Bild', alpha=0.5, s=50)],\n",
    "           loc='upper right', fontsize=12)\n",
    "plt.savefig('feature_extraction.svg', format='svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung der Features bzw. der Feature Map \n",
    "Visualierung der Durch das CNN gejagten Bilder. Am Ende kommen die Features raus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer_features(model, layer_num, input_img):\n",
    "    # Get the layer to visualize\n",
    "    layer_num_map = {0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1),\n",
    "                     4: (2, 0), 5: (2, 1), 6: (3, 0), 7: (3, 1)}\n",
    "    print(f\"Layer_num{layer_num_map[layer_num]}\")\n",
    "    stage_num, block_num = layer_num_map[layer_num]\n",
    "    layer = getattr(model, f'layer{stage_num+1}')[block_num]\n",
    "\n",
    "    # Create a forward hook to get the layer's output\n",
    "    outputs = []\n",
    "    def hook(module, input, output):\n",
    "        outputs.append(output)\n",
    "    layer.register_forward_hook(hook)\n",
    "\n",
    "    # Forward pass the input image through the model\n",
    "    _ = model(input_img)\n",
    "\n",
    "    # Get the output tensor and convert to numpy array\n",
    "    feature_maps = outputs[0].detach().numpy()\n",
    "\n",
    "    # Plot the feature maps as a grid\n",
    "    fig, axs = plt.subplots(nrows=8, ncols=8, figsize=(24, 24))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(2):\n",
    "        axs[i].imshow(feature_maps[0, i, :, :], cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(f\"layer{layer_num}.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "for i in range(7):\n",
    "    visualize_layer_features(resnet, i, input_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to visualize the feature maps of a layer\n",
    "def visualize_layer_features(model, layer_num, input_img):\n",
    "    # Get the layer to visualize\n",
    "    layer = model.layer1[layer_num] if layer_num < 4 else model.layer2[layer_num - 4] \\\n",
    "            if layer_num < 8 else model.layer3[layer_num - 8] \\\n",
    "            if layer_num < 12 else model.layer4[layer_num - 12]\n",
    "\n",
    "    # Create a forward hook to get the layer's output\n",
    "    outputs = []\n",
    "    def hook(module, input, output):\n",
    "        outputs.append(output)\n",
    "    layer.register_forward_hook(hook)\n",
    "\n",
    "    # Forward pass the input image through the model\n",
    "    _ = model(input_img)\n",
    "\n",
    "    # Get the output tensor and convert to numpy array\n",
    "    feature_maps = outputs[0].detach().numpy()\n",
    "\n",
    "    # Plot the feature maps as a grid\n",
    "    fig, axs = plt.subplots(nrows=8, ncols=8, figsize=(12, 12))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(64):\n",
    "        axs[i].imshow(feature_maps[0, i, :, :], cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(f\"layer{layer_num}.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# Load an example image\n",
    "img_path = '02_Images/fake_image_url_2/43_ChZDSUhNMG9nS0VJQ0FnSURRdTh6UEJnEAEF_fake_reviewImageUrls_2.png'\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "input_img = transform(img)\n",
    "input_img = input_img.unsqueeze(0)\n",
    "\n",
    "# Create a directory to save the feature maps\n",
    "if not os.path.exists('feature_maps'):\n",
    "    os.makedirs('feature_maps')\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# Visualize the feature maps for each layer\n",
    "for i in range(2):\n",
    "    visualize_layer_features(resnet, i, input_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# Define a function to visualize the feature maps of a layer\n",
    "def visualize_layer_features(layer_num, input_img):\n",
    "    # Get the layer to visualize\n",
    "    layer = resnet.layer1[layer_num]\n",
    "\n",
    "    # Create a forward hook to get the layer's output\n",
    "    outputs = []\n",
    "    def hook(module, input, output):\n",
    "        outputs.append(output)\n",
    "    layer.register_forward_hook(hook)\n",
    "\n",
    "    # Forward pass the input image through the model\n",
    "    _ = resnet(input_img)\n",
    "\n",
    "    # Get the output tensor and convert to numpy array\n",
    "    feature_maps = outputs[0].detach().numpy()\n",
    "\n",
    "    # Plot the feature maps as a grid\n",
    "    fig, axs = plt.subplots(nrows=8, ncols=8, figsize=(12, 12))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(64):\n",
    "        axs[i].imshow(feature_maps[0, i, :, :], cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(f\"layerWW{layer_num}.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# Load an example image\n",
    "img_path = '02_Images/fake_image_url_2/43_ChZDSUhNMG9nS0VJQ0FnSURRdTh6UEJnEAEF_fake_reviewImageUrls_2.png'\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "input_img = transform(img)\n",
    "input_img = input_img.unsqueeze(0)\n",
    "\n",
    "# Create a directory to save the feature maps\n",
    "if not os.path.exists('feature_maps'):\n",
    "    os.makedirs('feature_maps')\n",
    "\n",
    "# Visualize the feature maps for each layer\n",
    "for i in range(4):\n",
    "    visualize_layer_features(i, input_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define a function to perform activation maximization on a given feature and save the result to disk\n",
    "def visualize_and_save_feature(model, feature_index, save_dir, num_iterations=500):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Define a random input image\n",
    "    input_image = torch.randn(1, 3, 224, 224, requires_grad=True)\n",
    "\n",
    "    # Define a transformation to preprocess the input image\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Define an optimizer to update the input image\n",
    "    optimizer = torch.optim.Adam([input_image], lr=0.1)\n",
    "\n",
    "    # Perform activation maximization for a certain number of iterations\n",
    "    for i in range(num_iterations):\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(input_image)\n",
    "\n",
    "        # Compute the mean activation of the specified feature\n",
    "        feature_activation = output[0, feature_index].mean()\n",
    "\n",
    "        # Compute the gradient of the feature activation with respect to the input image\n",
    "        feature_activation.backward()\n",
    "\n",
    "        # Update the input image\n",
    "        optimizer.step()\n",
    "\n",
    "    # Convert the input image to a numpy array and unnormalize it\n",
    "    input_image = input_image.detach().numpy()[0]\n",
    "    input_image = (input_image * np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))) + np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
    "    input_image = np.clip(input_image, 0, 1)\n",
    "\n",
    "    # Save the input image to disk\n",
    "    filename = f'feature_{feature_index}.png'\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    plt.imsave(save_path, np.transpose(input_image, (1, 2, 0)))\n",
    "\n",
    "# Visualize and save the first 3 features\n",
    "save_dir = 'feature_visualizations'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for i in range(511, 512):\n",
    "    visualize_and_save_feature(model, i+1, save_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained ResNet-18 model\n",
    "resnet = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
    "\n",
    "# Get the first convolutional layer\n",
    "conv1 = resnet.conv1\n",
    "\n",
    "# Generate feature visualizations\n",
    "fig, axs = plt.subplots(8, 8, figsize=(10, 10))\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        filter_idx = i * 8 + j\n",
    "        filter_img = conv1.weight.data[filter_idx].cpu().numpy().transpose(1, 2, 0)\n",
    "        axs[i, j].imshow(filter_img)\n",
    "        axs[i, j].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text: Feature Generierung\n",
    "\n",
    "1. Feature extrahieren über PNLP Paket, wie z.B: lenth, adverb, Großklein, Rechtschriebung, Smiley-Nutzung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import *\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fertig mit 1. Block.\n",
      "Fertig mit 2. Block.\n",
      "Fertig mit 3. Block.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"feature_enriched_tab_img.csv\")\n",
    "\n",
    "def entire_capitalized_percentage(text):\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    words = [word for word in words if word.lower() != \"i\"]\n",
    "    num_capitalized = sum([1 for word in words if word.isupper()])\n",
    "    return num_capitalized / num_words\n",
    "\n",
    "def count_emojis(text):\n",
    "    emoji_count =  0\n",
    "    for character in text:\n",
    "        if character in emoji.EMOJI_DATA:\n",
    "            emoji_count += 1\n",
    "    return emoji_count\n",
    "\n",
    "def emojji_per_word_ratio(text):\n",
    "    emoji_count =  0\n",
    "    for character in text:\n",
    "        if character in emoji.EMOJI_DATA:\n",
    "            emoji_count += 1\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    if num_words == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return emoji_count / num_words\n",
    "    \n",
    "def avg_word_length(text):\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    if num_words == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        total_length = sum([len(word) for word in words])\n",
    "        return total_length / num_words\n",
    "\n",
    "def avg_sentence_length(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    if num_sentences == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        total_length = sum([len(sentence.split()) for sentence in sentences])\n",
    "        return total_length / num_sentences\n",
    "\n",
    "def extract_pos_tags(text):\n",
    "    pos_tags = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    return pos_tags\n",
    "\n",
    "def perform_sentiment_analysis(text):\n",
    "    sentiment = sentiment_task(text)\n",
    "    sentiment_label = sentiment[0]['label']\n",
    "    print(sentiment_label)\n",
    "    return sentiment_label\n",
    "   \n",
    "def text_spelling_error_quota(text):\n",
    "    blob = TextBlob(text)\n",
    "    words = blob.words\n",
    "    num_words = len(words)\n",
    "    num_errors = sum([not w.spellcheck()[0][1] for w in blob.words])\n",
    "    return num_errors / num_words\n",
    "\n",
    "def calculate_punctuation_ratio(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    ratio_list = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        num_punctuations = sum([1 for word in words if word in string.punctuation])\n",
    "        num_words = len(words)\n",
    "        if num_words > 0:\n",
    "            punctuation_ratio = num_punctuations / num_words\n",
    "            ratio_list.append(punctuation_ratio)    \n",
    "    if len(ratio_list) > 0:\n",
    "        avg_punctuation_ratio = sum(ratio_list) / len(ratio_list)\n",
    "    else:\n",
    "        avg_punctuation_ratio = 0\n",
    "    return avg_punctuation_ratio\n",
    "\n",
    "def count_nouns(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    noun_count = len([word for word, tag in tagged_tokens if tag.startswith('N')])\n",
    "    return noun_count\n",
    "\n",
    "def count_adjectives(text):\n",
    "    adj_tags = ['JJ', 'JJR', 'JJS']\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    adj_count = len([word for word, tag in nltk.pos_tag(tokens) if tag in adj_tags])\n",
    "    return adj_count\n",
    "\n",
    "def count_verbs(text):\n",
    "    verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    tokens = word_tokenize(text)\n",
    "    verb_count = len([word for word, tag in pos_tag(tokens) if tag in verb_tags and word.lower() not in stop_words])\n",
    "    return verb_count\n",
    "\n",
    "def count_adverbs(text): \n",
    "    adv_tags = ['RB', 'RBR', 'RBS']\n",
    "    tokens = word_tokenize(text)\n",
    "    adv_count = len([word for word, tag in pos_tag(tokens) if tag in adv_tags and word.lower() not in stop_words])\n",
    "    return adv_count\n",
    "\n",
    "def count_pronouns(text):\n",
    "    pronoun_tags = ['PRP', 'PRP$', 'WP', 'WP$']\n",
    "    tokens = word_tokenize(text)\n",
    "    pronoun_count = len([word for word, tag in pos_tag(tokens) if tag in pronoun_tags])\n",
    "    return pronoun_count\n",
    "\n",
    "def calculate_not_stopword_ratio(text):\n",
    "    words = word_tokenize(text)\n",
    "    num_words = len(words)\n",
    "    not_stopword_count = len([word for word in words if word.lower() not in stop_words])\n",
    "    return not_stopword_count / num_words\n",
    "\n",
    "def calculate_stopword_ratio(text):\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    num_words = len(words)\n",
    "    num_stopwords = len([word for word in words if word in stop_words])\n",
    "    return num_stopwords / num_words\n",
    "\n",
    "def calulate_stopword_to_nostopword_ratio(text): \n",
    "    return calculate_stopword_ratio(text) / calculate_not_stopword_ratio(text)\n",
    "\n",
    "def compute_modal_verb_ratio(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "    modal_verbs = ['can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would', 'must']\n",
    "    modal_verb_count = len([word for word in tokens if word in modal_verbs and word not in stopwords])\n",
    "    word_count = len([word for word in tokens if word not in stopwords])\n",
    "    return modal_verb_count / word_count\n",
    "\n",
    "\n",
    "def compute_uncertain_ratio(text):\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    uncertain_words = ['yet', 'careful', 'hesitant', 'tendency', 'hit', 'undefined', 'ambivalent', 'confused', 'equivocal', 'fuzzy', 'inconclusive', 'indeterminate', 'unclear', 'uncertain', 'unsettled', 'vague']\n",
    "    uncertain_count = len([word for word in words if word in uncertain_words and word not in stop_words])\n",
    "    total_count = len(words)\n",
    "    uncertain_ratio = uncertain_count / total_count if total_count > 0 else 0.0\n",
    "    return uncertain_ratio\n",
    "\n",
    "\n",
    "def count_individual_words(text):\n",
    "    individual_words = ['I', 'me', 'my', 'mine', 'myself']\n",
    "    tokens = word_tokenize(text)\n",
    "    individual_count = len([word for word in tokens if word.lower() in individual_words])\n",
    "    return individual_count\n",
    "\n",
    "\n",
    "def count_group_words(text):\n",
    "    group_words = ['we', 'us', 'our', 'ours', 'ourselves']\n",
    "    tokens = word_tokenize(text)\n",
    "    group_count = len([word for word in tokens if word.lower() in group_words])\n",
    "    return group_count\n",
    "\n",
    "def count_self_words(text):\n",
    "    self_words = ['self', 'myself', 'ourselves']\n",
    "    tokens = word_tokenize(text)\n",
    "    self_count = len([word for word in tokens if word.lower() in self_words])\n",
    "    return self_count\n",
    "\n",
    "def individual_ratio(text):\n",
    "    total_words = len(word_tokenize(text))\n",
    "    individual_count = count_individual_words(text)\n",
    "    return individual_count / total_words\n",
    "\n",
    "def group_ratio(text):\n",
    "    total_words = len(word_tokenize(text))\n",
    "    group_count = count_group_words(text)\n",
    "    return group_count / total_words\n",
    "\n",
    "def self_ratio(text):\n",
    "    total_words = len(word_tokenize(text))\n",
    "    self_count = count_self_words(text)\n",
    "    return self_count / total_words\n",
    "\n",
    "\n",
    "df['text_length'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "df['text_length_char'] = df['text'].apply(len)\n",
    "df['text_punctuation'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df['text_avg_word_length'] = df['text'].apply(avg_word_length)\n",
    "df['text_avg_sentence_length'] = df['text'].apply(avg_sentence_length)\n",
    "print('Fertig mit 1. Block.')\n",
    "\n",
    "df['text_emoji_count'] = df['text'].apply(count_emojis)\n",
    "df['text_count_verbs'] = df['text'].apply(count_verbs)\n",
    "df['text_count_adjectives'] = df['text'].apply(count_adjectives)\n",
    "df['text_count_adverbs'] = df['text'].apply(count_adverbs)\n",
    "df['text_count_pronouns'] = df['text'].apply(count_pronouns)\n",
    "df['text_count_nouns'] = df['text'].apply(count_nouns)\n",
    "print('Fertig mit 2. Block.')\n",
    "\n",
    "df['text_no_stopword_Ratio'] = df['text'].apply(calculate_not_stopword_ratio)\n",
    "df['text_stopword_ratio'] = df['text'].apply(calculate_stopword_ratio)\n",
    "df['text_stopword_to_nostopword_ratio'] = df['text'].apply(calulate_stopword_to_nostopword_ratio)\n",
    "df['text_entired_capitalized_ratio'] = df['text'].apply(entire_capitalized_percentage)\n",
    "df['text_punctuation_ratio'] = df['text'].apply(calculate_punctuation_ratio)\n",
    "print('Fertig mit 3. Block.')\n",
    "\n",
    "df['text_spelling_error_quota'] = df['text'].apply(text_spelling_error_quota)\n",
    "df['text_sentiment'] = df['text'].apply(perform_sentiment_analysis)\n",
    "df['text_modal_verb_ratio'] = df['text'].apply(compute_modal_verb_ratio)\n",
    "df['text_uncertain_ratio'] = df['text'].apply(compute_uncertain_ratio)\n",
    "df['text_individual_count'] = df['text'].apply(count_individual_words)\n",
    "print('Fertig mit 4. Block.')\n",
    "\n",
    "df['text_group_count'] = df['text'].apply(count_group_words)\n",
    "df['text_self_count'] = df['text'].apply(count_self_words)\n",
    "df['text_individual_ratio'] = df['text'].apply(individual_ratio)\n",
    "df['text_group_ratio'] = df['text'].apply(group_ratio)\n",
    "df['text_self_ratio'] = df['text'].apply(self_ratio)\n",
    "print('Fertig mit 5. Block.')\n",
    "\n",
    "\n",
    "df.to_csv('features_enriched_tab_img_text.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exkurs: Eigenen Img-Klassifikator trainiert und ausgwertet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilder für Training vorberieten und\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_weitere_forschung_ohne_base.csv\")\n",
    "\n",
    "df = df.sample(n=500, random_state=9)\n",
    "# df = df.sample(n=1000, random_state=42)\n",
    "df = df.sort_index()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index < 0:\n",
    "        print(f\"Index {index} schon heruntergeladen, skipped ..\")\n",
    "        continue\n",
    "    for i in range(2):\n",
    "        if not pd.isna(row[f\"reviewImageUrls/{i}\"]):\n",
    "            print(f\"Downloading image for {index}...\")\n",
    "            url = row[f\"reviewImageUrls/{i}\"]\n",
    "            filename = f\"{url.split('/')[-1]}.png\"\n",
    "            path = os.path.join(\"02_Images\", \"train\", \"real\", filename)\n",
    "            if not os.path.exists(path):\n",
    "                try:\n",
    "                    urllib.request.urlretrieve(url, path)\n",
    "                except:\n",
    "                    print(f\"Error with {url}\")\n",
    "\n",
    "print(\"Done with downloading real images for training set.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Umsetzung mit FASTAI visual Learning als Classifizierung Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"base_keywords_sentiment_reduced.csv\")\n",
    "print(df.shape)\n",
    "\n",
    "df_all = pd.read_csv(\"01_Data/raw_data/dataset_weitere_forschung.csv\")\n",
    "print(df_all.shape)\n",
    "\n",
    "df_all = df_all[\n",
    "    [\n",
    "        \"text\",\n",
    "        \"reviewId\",\n",
    "        \"url\",\n",
    "        \"placeId\",\n",
    "        \"categoryName\",\n",
    "        \"stars\",\n",
    "        \"title\",\n",
    "        \"reviewImageUrls/0\",\n",
    "        \"reviewImageUrls/1\",\n",
    "        \"reviewImageUrls/2\",\n",
    "        \"reviewImageUrls/3\",\n",
    "        \"reviewImageUrls/4\",\n",
    "        \"reviewImageUrls/5\",\n",
    "        \"genre\",\n",
    "    ]\n",
    "]\n",
    "df_all = df_all[df_all[\"reviewImageUrls/0\"].notna()]\n",
    "print(df_all.shape)\n",
    "df_all = df_all[~df_all[\"reviewId\"].isin(df[\"reviewId\"])]\n",
    "print(df_all.shape)\n",
    "print(df_all.columns)\n",
    "\n",
    "image_urls_cols = [\n",
    "    \"reviewImageUrls/0\",\n",
    "    \"reviewImageUrls/1\",\n",
    "    \"reviewImageUrls/2\",\n",
    "    \"reviewImageUrls/3\",\n",
    "    \"reviewImageUrls/4\",\n",
    "    \"reviewImageUrls/5\",\n",
    "]\n",
    "for col in image_urls_cols:\n",
    "    print(f\"Processing col {col}\")\n",
    "    df_all[col] = df_all[col].str.replace(\"=w150-h150-k-no-p\", \"=w256-h256-p-k-no\")\n",
    "\n",
    "print(df_all.shape)\n",
    "\n",
    "# df_all.to_csv('train_weitere_forschung_ohne_base.csv', index=False)\n",
    "# df_all.to_excel('train_weitere_forschung_ohne_base.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier nur die Auswertung. Training in colab.ipynb Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from matplotlib import colors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams[\"font.family\"] = \"Book Antiqua\"\n",
    "\n",
    "df = pd.read_csv(\"base_fake_real_imgcls_pred.csv\")\n",
    "\n",
    "y_true = df[\"label\"]\n",
    "y_pred = df[\"prediction_0\"]\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "recall = classification_report(y_true, y_pred, output_dict=True)[\"weighted avg\"][\n",
    "    \"recall\"\n",
    "]\n",
    "precision = classification_report(y_true, y_pred, output_dict=True)[\"weighted avg\"][\n",
    "    \"precision\"\n",
    "]\n",
    "f1_score = classification_report(y_true, y_pred, output_dict=True)[\"weighted avg\"][\n",
    "    \"f1-score\"\n",
    "]\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")\n",
    "\n",
    "cmap = colors.ListedColormap([\"#FFE5E4\", \"#D7F3D9\"])\n",
    "\n",
    "labels = y_true.unique()\n",
    "fig, ax = plt.subplots(figsize=(5, 2.5))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "\n",
    "# Plot confusion matrix with custom colors\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    cmap=cmap,\n",
    "    fmt=\"g\",\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    cbar=False,\n",
    ")\n",
    "plt.xlabel(\"Vorhersage\")\n",
    "plt.ylabel(\"Tatsächlich\")\n",
    "\n",
    "plt.savefig(\"confusion_matrix_green_red.svg\", format=\"svg\", bbox_inches=\"tight\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
