{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook wird der Datensatz base_fake_real.csv verwendet und untersucht wie man Fake-Reviews von Echten Reviews unterscheiden kann. Zuerst wird der Datensatz so vorbeietet um Features zu generieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import urllib\n",
    "from fastai.vision.all import *\n",
    "from fastdownload import download_url\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datensatz für Feature Extraction zu feature_base.csv vorbereitet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"base_fake_real.csv\")\n",
    "print(f\"Base Datensatz: {df.shape}\")\n",
    "# print(df.columns)\n",
    "\n",
    "df.drop(\n",
    "    [\n",
    "        \"index_fake\",\n",
    "        \"org_text\",\n",
    "        \"org_stars\",\n",
    "        \"sent_score_0\",\n",
    "        \"sent_v2\",\n",
    "        \"sent_v3\",\n",
    "        \"sent_v3.1\",\n",
    "        \"prompt_v3\",\n",
    "        \"website\",\n",
    "        \"dalle_prompt\",\n",
    "        \"website\",\n",
    "        \"prompt_v2\",\n",
    "        \"gpt3_v2\",\n",
    "        \"gpt3_v3\",\n",
    "        \"gpt3_v3.1\",\n",
    "        \"prompt_v4\",\n",
    "        \"org_reviewId\",\n",
    "        \"sent_v4\",\n",
    "        \"keywords\",\n",
    "        \"keywords_only\",\n",
    "        \"text_length\",\n",
    "        \"reviewImageUrls/3\",\n",
    "        \"reviewerPhotoUrl\",\n",
    "        \"reviewerUrl\",\n",
    "        \"reviewerId\",\n",
    "        \"temporarilyClosed\",\n",
    "        \"reviewsCount\",\n",
    "        \"street\",\n",
    "        \"state\",\n",
    "        \"totalScore\",\n",
    "        \"subTitle\",\n",
    "        \"description\",\n",
    "        \"price\",\n",
    "        \"sentiment\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "df = df.reindex(\n",
    "    columns=[\n",
    "        \"label\",\n",
    "        \"reviewId\",\n",
    "        \"placeId\",\n",
    "        \"reviewUrl\",\n",
    "        \"url\",\n",
    "        \"title\",\n",
    "        \"categoryName\",\n",
    "        \"genre\",\n",
    "        \"text\",\n",
    "        \"stars\",\n",
    "        \"publishedAtDate\",\n",
    "        \"likesCount\",\n",
    "        \"name\",\n",
    "        \"isLocalGuide\",\n",
    "        \"reviewerNumberOfReviews\",\n",
    "        \"reviewImageUrls/0\",\n",
    "        \"reviewImageUrls/1\",\n",
    "        \"reviewImageUrls/2\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Reduzierter Datensatz als neue Basis für FE: {df.shape}\")\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "\n",
    "df.to_csv(\"base_features.csv\", index=False)\n",
    "df.to_excel(\"base_features.xlsx\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular: Feature Generierung\n",
    "\n",
    "1. aus publishedAt das bestmögliche rausholen\n",
    "\n",
    "when_on_day_4hbin:\n",
    "Midnight: 0-4 hours\n",
    "Early morning: 4-8 hours\n",
    "Morning: 8-12 hours\n",
    "Early afternoon: 12-16 hours\n",
    "Late afternoon: 16-20 hours\n",
    "Evening: 20-24 hours -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"base_features.csv\")\n",
    "print(df[\"publishedAtDate\"][977])\n",
    "\n",
    "df[\"publishedAtDate\"] = pd.to_datetime(\n",
    "    df[\"publishedAtDate\"], format=\"%Y-%m-%dT%H:%M:%S\"\n",
    ")\n",
    "\n",
    "df[\"year\"] = df[\"publishedAtDate\"].dt.year\n",
    "df[\"month\"] = df[\"publishedAtDate\"].dt.month\n",
    "df[\"dayofweek\"] = df[\"publishedAtDate\"].dt.dayofweek\n",
    "df[\"elapsed_days\"] = (datetime.today() - df[\"publishedAtDate\"]).dt.days\n",
    "df[\"when_on_day_4hbin\"] = pd.cut(\n",
    "    df[\"publishedAtDate\"].dt.hour,\n",
    "    bins=[-1, 4, 8, 12, 16, 20, 24],\n",
    "    labels=[0, 1, 2, 3, 4, 5],\n",
    ")\n",
    "\n",
    "df[\"when_on_day_hour\"] = df[\"publishedAtDate\"].dt.hour\n",
    "\n",
    "print(df[\"when_on_day_4hbin\"].isna().sum())\n",
    "print(df.loc[df[\"when_on_day_4hbin\"].isna(), \"publishedAtDate\"])\n",
    "\n",
    "print(\n",
    "    df[\n",
    "        [\n",
    "            \"publishedAtDate\",\n",
    "            \"year\",\n",
    "            \"month\",\n",
    "            \"dayofweek\",\n",
    "            \"elapsed_days\",\n",
    "            \"when_on_day_4hbin\",\n",
    "            \"when_on_day_hour\",\n",
    "        ]\n",
    "    ].head(10)\n",
    ")\n",
    "\n",
    "df.to_csv(\"feature_enriched_tab.csv\", index=False)\n",
    "df.to_excel(\"feature_enriched_tab.xlsx\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nur für mich ein Test, wie ich ein Basic Decsion Tree anwende und für grobes Gefühl, wie aussagekräftig das alles ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"feature_enriched_tab.csv\")\n",
    "\n",
    "df = df[\n",
    "    [\n",
    "        \"stars\",\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"likesCount\",\n",
    "        \"reviewerNumberOfReviews\",\n",
    "        \"isLocalGuide\",\n",
    "        \"dayofweek\",\n",
    "        \"elapsed_days\",\n",
    "        \"when_on_day_4hbin\",\n",
    "        \"label\",\n",
    "    ]\n",
    "]\n",
    "df[\"label\"] = (df[\"label\"] == \"real\").astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(\"label\", axis=1), df[\"label\"], test_size=0.2, random_state=11\n",
    ")\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Decision tree accuracy: {score:.2f}\")\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
    "\n",
    "plt.ylabel(\"Relative Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bild: Feature Generierung\n",
    "\n",
    "### Feature Extrahieren über Pretrained ResNet-18 Architektur und in Dataframe abspeichern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"feature_enriched_tab.csv\")\n",
    "image_urls = df[\"reviewImageUrls/0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "# Remove the last fully connected layer\n",
    "modules = list(resnet.children())[:-1]\n",
    "resnet = torch.nn.Sequential(*modules)\n",
    "# Set the model to evaluation mode\n",
    "resnet.eval()\n",
    "\n",
    "# Define a function to extract features for a single image\n",
    "def extract_image_features(image_url):\n",
    "    # Load image and preprocess\n",
    "    img = Image.open(requests.get(image_url, stream=True).raw)\n",
    "    img = transforms.ToTensor()(img)\n",
    "    img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = resnet(img)\n",
    "        features = features.squeeze().numpy()\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "counter = 0\n",
    "feature_vectors = []\n",
    "for image_url in image_urls:\n",
    "    try:\n",
    "        counter += 1\n",
    "        print(f'{counter}:\\tExtracting features from {image_url}')\n",
    "        features = extract_image_features(image_url)\n",
    "    except:\n",
    "        print(f'Error extracting features from {image_url}. Replaces with NaN.')\n",
    "        features = np.full((512,), np.nan)\n",
    "    feature_vectors.append(features)\n",
    "\n",
    "feature_df = pd.DataFrame(feature_vectors, columns=[f'feature_{i}' for i in range(512)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([df, feature_df], axis=1)\n",
    "print(\"added features to the original dataset.\")\n",
    "\n",
    "print(new_df.iloc[0])\n",
    "# new_df.to_csv('feature_enriched_tab_img.csv', index=False)\n",
    "# new_df.to_excel('feature_enriched_tab_img.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize as PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load your dataframe\n",
    "df = pd.read_csv('feature_enriched_tab_img.csv')\n",
    "\n",
    "# drop row with index 764\n",
    "df = df.drop(764)\n",
    "\n",
    "# Select feature columns and label\n",
    "# can you drop all \n",
    "features = df.iloc[:, 0:1].join(df.iloc[:, 24:])\n",
    "\n",
    "\n",
    "# drop all rows with NaN values in feature columns\n",
    "features = features.dropna(subset=features.columns[1:], how='all')\n",
    "\n",
    "# Define color map\n",
    "color_map = {'fake': 'red', 'real': 'green'}\n",
    "\n",
    "# Map labels to colors\n",
    "colors = features['label'].apply(lambda x: color_map[x])\n",
    "\n",
    "# Apply t-SNE to reduce dimensionality\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200)\n",
    "tsne_features = tsne.fit_transform(features.iloc[:, 1:])\n",
    "\n",
    "# Visualize the reduced features, colored by label\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=colors, alpha=0.5)\n",
    "plt.title('t-SNE visualization of features')\n",
    "plt.xlabel('t-SNE feature 1')\n",
    "plt.ylabel('t-SNE feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataframe\n",
    "df = pd.read_csv('feature_enriched_tab_img.csv')\n",
    "\n",
    "# drop row with index 764\n",
    "df = df.drop(764)\n",
    "\n",
    "# Select feature columns and label\n",
    "features = df.iloc[:, 0:1].join(df.iloc[:, 24:])\n",
    "\n",
    "# drop all rows with NaN values in feature columns\n",
    "features = features.dropna(subset=features.columns[1:], how='all')\n",
    "\n",
    "# Define color map\n",
    "color_map = {'fake': 'red', 'real': 'green'}\n",
    "\n",
    "# Map labels to colors\n",
    "colors = features['label'].apply(lambda x: color_map[x])\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=2)\n",
    "pca_features = pca.fit_transform(features.iloc[:, 1:])\n",
    "\n",
    "# Visualize the reduced features, colored by label\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_features[:, 0], pca_features[:, 1], c=colors, alpha=0.5, s=50)\n",
    "plt.xlabel('PCA Komponente 1', fontsize=14)\n",
    "plt.ylabel('PCA Komponente 2', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=10)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.legend(handles=[plt.scatter([], [], c='red', label='Fake Bild', alpha=0.5, s=50),\n",
    "                     plt.scatter([], [], c='green', label='Echtes Bild', alpha=0.5, s=50)],\n",
    "           loc='upper right', fontsize=12)\n",
    "plt.savefig('feature_extraction.svg', format='svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung der Features bzw. der Feature Map \n",
    "Visualierung der Durch das CNN gejagten Bilder. Am Ende kommen die Features raus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer_features(model, layer_num, input_img):\n",
    "    # Get the layer to visualize\n",
    "    layer_num_map = {0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1),\n",
    "                     4: (2, 0), 5: (2, 1), 6: (3, 0), 7: (3, 1)}\n",
    "    print(f\"Layer_num{layer_num_map[layer_num]}\")\n",
    "    stage_num, block_num = layer_num_map[layer_num]\n",
    "    layer = getattr(model, f'layer{stage_num+1}')[block_num]\n",
    "\n",
    "    # Create a forward hook to get the layer's output\n",
    "    outputs = []\n",
    "    def hook(module, input, output):\n",
    "        outputs.append(output)\n",
    "    layer.register_forward_hook(hook)\n",
    "\n",
    "    # Forward pass the input image through the model\n",
    "    _ = model(input_img)\n",
    "\n",
    "    # Get the output tensor and convert to numpy array\n",
    "    feature_maps = outputs[0].detach().numpy()\n",
    "\n",
    "    # Plot the feature maps as a grid\n",
    "    fig, axs = plt.subplots(nrows=8, ncols=8, figsize=(24, 24))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(2):\n",
    "        axs[i].imshow(feature_maps[0, i, :, :], cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(f\"layer{layer_num}.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "for i in range(7):\n",
    "    visualize_layer_features(resnet, i, input_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to visualize the feature maps of a layer\n",
    "def visualize_layer_features(model, layer_num, input_img):\n",
    "    # Get the layer to visualize\n",
    "    layer = model.layer1[layer_num] if layer_num < 4 else model.layer2[layer_num - 4] \\\n",
    "            if layer_num < 8 else model.layer3[layer_num - 8] \\\n",
    "            if layer_num < 12 else model.layer4[layer_num - 12]\n",
    "\n",
    "    # Create a forward hook to get the layer's output\n",
    "    outputs = []\n",
    "    def hook(module, input, output):\n",
    "        outputs.append(output)\n",
    "    layer.register_forward_hook(hook)\n",
    "\n",
    "    # Forward pass the input image through the model\n",
    "    _ = model(input_img)\n",
    "\n",
    "    # Get the output tensor and convert to numpy array\n",
    "    feature_maps = outputs[0].detach().numpy()\n",
    "\n",
    "    # Plot the feature maps as a grid\n",
    "    fig, axs = plt.subplots(nrows=8, ncols=8, figsize=(12, 12))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(64):\n",
    "        axs[i].imshow(feature_maps[0, i, :, :], cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(f\"layer{layer_num}.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# Load an example image\n",
    "img_path = '02_Images/fake_image_url_2/43_ChZDSUhNMG9nS0VJQ0FnSURRdTh6UEJnEAEF_fake_reviewImageUrls_2.png'\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "input_img = transform(img)\n",
    "input_img = input_img.unsqueeze(0)\n",
    "\n",
    "# Create a directory to save the feature maps\n",
    "if not os.path.exists('feature_maps'):\n",
    "    os.makedirs('feature_maps')\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# Visualize the feature maps for each layer\n",
    "for i in range(2):\n",
    "    visualize_layer_features(resnet, i, input_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# Define a function to visualize the feature maps of a layer\n",
    "def visualize_layer_features(layer_num, input_img):\n",
    "    # Get the layer to visualize\n",
    "    layer = resnet.layer1[layer_num]\n",
    "\n",
    "    # Create a forward hook to get the layer's output\n",
    "    outputs = []\n",
    "    def hook(module, input, output):\n",
    "        outputs.append(output)\n",
    "    layer.register_forward_hook(hook)\n",
    "\n",
    "    # Forward pass the input image through the model\n",
    "    _ = resnet(input_img)\n",
    "\n",
    "    # Get the output tensor and convert to numpy array\n",
    "    feature_maps = outputs[0].detach().numpy()\n",
    "\n",
    "    # Plot the feature maps as a grid\n",
    "    fig, axs = plt.subplots(nrows=8, ncols=8, figsize=(12, 12))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(64):\n",
    "        axs[i].imshow(feature_maps[0, i, :, :], cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(f\"layerWW{layer_num}.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# Load an example image\n",
    "img_path = '02_Images/fake_image_url_2/43_ChZDSUhNMG9nS0VJQ0FnSURRdTh6UEJnEAEF_fake_reviewImageUrls_2.png'\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "input_img = transform(img)\n",
    "input_img = input_img.unsqueeze(0)\n",
    "\n",
    "# Create a directory to save the feature maps\n",
    "if not os.path.exists('feature_maps'):\n",
    "    os.makedirs('feature_maps')\n",
    "\n",
    "# Visualize the feature maps for each layer\n",
    "for i in range(4):\n",
    "    visualize_layer_features(i, input_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define a function to perform activation maximization on a given feature and save the result to disk\n",
    "def visualize_and_save_feature(model, feature_index, save_dir, num_iterations=500):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Define a random input image\n",
    "    input_image = torch.randn(1, 3, 224, 224, requires_grad=True)\n",
    "\n",
    "    # Define a transformation to preprocess the input image\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Define an optimizer to update the input image\n",
    "    optimizer = torch.optim.Adam([input_image], lr=0.1)\n",
    "\n",
    "    # Perform activation maximization for a certain number of iterations\n",
    "    for i in range(num_iterations):\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(input_image)\n",
    "\n",
    "        # Compute the mean activation of the specified feature\n",
    "        feature_activation = output[0, feature_index].mean()\n",
    "\n",
    "        # Compute the gradient of the feature activation with respect to the input image\n",
    "        feature_activation.backward()\n",
    "\n",
    "        # Update the input image\n",
    "        optimizer.step()\n",
    "\n",
    "    # Convert the input image to a numpy array and unnormalize it\n",
    "    input_image = input_image.detach().numpy()[0]\n",
    "    input_image = (input_image * np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))) + np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
    "    input_image = np.clip(input_image, 0, 1)\n",
    "\n",
    "    # Save the input image to disk\n",
    "    filename = f'feature_{feature_index}.png'\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    plt.imsave(save_path, np.transpose(input_image, (1, 2, 0)))\n",
    "\n",
    "# Visualize and save the first 3 features\n",
    "save_dir = 'feature_visualizations'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for i in range(511, 512):\n",
    "    visualize_and_save_feature(model, i+1, save_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained ResNet-18 model\n",
    "resnet = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
    "\n",
    "# Get the first convolutional layer\n",
    "conv1 = resnet.conv1\n",
    "\n",
    "# Generate feature visualizations\n",
    "fig, axs = plt.subplots(8, 8, figsize=(10, 10))\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        filter_idx = i * 8 + j\n",
    "        filter_img = conv1.weight.data[filter_idx].cpu().numpy().transpose(1, 2, 0)\n",
    "        axs[i, j].imshow(filter_img)\n",
    "        axs[i, j].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text: Feature Generierung\n",
    "\n",
    "1. Feature extrahieren über PNLP Paket, wie z.B: lenth, adverb, Großklein, Rechtschriebung, Smiley-Nutzung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\michi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\michi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\michi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\michi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "\n",
    "from nltk.corpus import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spell check:  Staff has been great every time my kids rent a place and even takes time upon checking out to have a nice chat with them and ask them how they had been and tell them to stay active and health! I wish more places in the world were like this place! Great place! \n",
      "Spell check:  Nice let down live young people give it a look taste \n",
      "Spell check:  Great spot to take your children everyday or to friends parties or just make it a date night 😍 \n",
      "Spell check:  The prices were pretty reasonable. My two year old had a blast jumping for two hours straight until he wore himself out. Even the parents had a good time. The trampolines are ample so there is loads of time to enjoy without waiting. And you get to control the time yourself. Overall a great place to get some active jumping in. \n",
      "Spell check:  We love that this trampoline place is conveniently near our house! This will be our source for jumping for many years to come. \n",
      "                                                text  text_length  \\\n",
      "0   Staff has been great every time my kids rent ...           53   \n",
      "1   Nice let down live young people give it a loo...           11   \n",
      "2   Great spot to take your children everyday or ...           19   \n",
      "3   The prices were pretty reasonable. My two yea...           61   \n",
      "4   We love that this trampoline place is conveni...           23   \n",
      "\n",
      "   text_punctuation_ratio  text_punctuation  text_entired_capitalized_quota  \\\n",
      "0                0.147019                 3                             0.0   \n",
      "1                0.000000                 0                             0.0   \n",
      "2                0.000000                 0                             0.0   \n",
      "3                0.102652                 6                             0.0   \n",
      "4                0.080128                 2                             0.0   \n",
      "\n",
      "   text_emoji_count  text_avg_word_length  text_avg_sentence_length  \\\n",
      "0                 0              3.905660                 17.666667   \n",
      "1                 0              3.818182                 11.000000   \n",
      "2                 1              4.000000                 19.000000   \n",
      "3                 0              4.377049                 10.166667   \n",
      "4                 0              4.521739                 11.500000   \n",
      "\n",
      "                                       text_pos_tags  text_sentiment  \\\n",
      "0  [(Staff, NNP), (has, VBZ), (been, VBN), (great...        0.571667   \n",
      "1  [(Nice, NNP), (let, VBD), (down, RP), (live, J...        0.170202   \n",
      "2  [(Great, NNP), (spot, NN), (to, TO), (take, VB...        0.300000   \n",
      "3  [(The, DT), (prices, NNS), (were, VBD), (prett...        0.279630   \n",
      "4  [(We, PRP), (love, VBP), (that, IN), (this, DT...        0.375000   \n",
      "\n",
      "   text_spelling_error_quota  \n",
      "0                   0.000000  \n",
      "1                   0.000000  \n",
      "2                   0.000000  \n",
      "3                   1.639344  \n",
      "4                   0.000000  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"feature_enriched_tab_img.csv\")\n",
    "\n",
    "\n",
    "df['text_length'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "df['text_punctuation'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "def entire_capitalized_percentage(text):\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    words = [word for word in words if word.lower() != \"i\"]\n",
    "    num_capitalized = sum([1 for word in words if word.isupper()])\n",
    "    return num_capitalized / num_words * 100\n",
    "\n",
    "df['text_entired_capitalized_quota'] = df['text'].apply(entire_capitalized_percentage)\n",
    "\n",
    "def count_emojis(text):\n",
    "    emoji_count =  0\n",
    "    for character in text:\n",
    "        if character in emoji.EMOJI_DATA:\n",
    "            emoji_count += 1\n",
    "    return emoji_count\n",
    "df['text_emoji_count'] = df['text'].apply(count_emojis)\n",
    "\n",
    "\n",
    "def avg_word_length(text):\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    if num_words == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        total_length = sum([len(word) for word in words])\n",
    "        return total_length / num_words\n",
    "df['text_avg_word_length'] = df['text'].apply(avg_word_length)\n",
    "\n",
    "def avg_sentence_length(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    if num_sentences == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        total_length = sum([len(sentence.split()) for sentence in sentences])\n",
    "        return total_length / num_sentences\n",
    "df['text_avg_sentence_length'] = df['text'].apply(avg_sentence_length)\n",
    "\n",
    "\n",
    "def extract_pos_tags(text):\n",
    "    pos_tags = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    return pos_tags\n",
    "\n",
    "def perform_sentiment_analysis(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    return sentiment\n",
    "\n",
    "def text_spelling_error_quota(text):\n",
    "    print(\"Spell check: \" + text)\n",
    "    blob = TextBlob(text)\n",
    "    words = blob.words\n",
    "    num_words = len(words)\n",
    "    num_errors = sum([not w.spellcheck()[0][1] for w in blob.words])\n",
    "    return num_errors / num_words * 100\n",
    "\n",
    "\n",
    "# Create a function to calculate the average ratio of punctuations per sentence\n",
    "def calculate_punctuation_ratio(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    ratio_list = []\n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence)\n",
    "            num_punctuations = sum([1 for word in words if word in string.punctuation])\n",
    "        \n",
    "        # Calculate the ratio of punctuations in the sentence\n",
    "        num_words = len(words)\n",
    "        if num_words > 0:\n",
    "            punctuation_ratio = num_punctuations / num_words\n",
    "            ratio_list.append(punctuation_ratio)\n",
    "    \n",
    "    # Calculate the average ratio of punctuations in all sentences\n",
    "    if len(ratio_list) > 0:\n",
    "        avg_punctuation_ratio = sum(ratio_list) / len(ratio_list)\n",
    "    else:\n",
    "        avg_punctuation_ratio = 0\n",
    "    \n",
    "    return avg_punctuation_ratio\n",
    "\n",
    "# Apply the function to the 'text' column in your DataFrame\n",
    "\n",
    "\n",
    "df['text_punctuation_ratio'] = df['text'].apply(calculate_punctuation_ratio)\n",
    "df = df.head(5)\n",
    "df['text_pos_tags'] = df['text'].apply(extract_pos_tags)\n",
    "df['text_sentiment'] = df['text'].apply(perform_sentiment_analysis)\n",
    "df['text_spelling_error_quota'] = df['text'].apply(text_spelling_error_quota)\n",
    "print(df[['text', 'text_length', 'text_punctuation_ratio','text_punctuation', 'text_entired_capitalized_quota', 'text_emoji_count', 'text_avg_word_length', 'text_avg_sentence_length', 'text_pos_tags', 'text_sentiment', 'text_spelling_error_quota']].head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exkurs: Eigenen Img-Klassifikator trainiert und ausgwertet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilder für Training vorberieten und\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_weitere_forschung_ohne_base.csv\")\n",
    "\n",
    "df = df.sample(n=500, random_state=9)\n",
    "# df = df.sample(n=1000, random_state=42)\n",
    "df = df.sort_index()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index < 0:\n",
    "        print(f\"Index {index} schon heruntergeladen, skipped ..\")\n",
    "        continue\n",
    "    for i in range(2):\n",
    "        if not pd.isna(row[f\"reviewImageUrls/{i}\"]):\n",
    "            print(f\"Downloading image for {index}...\")\n",
    "            url = row[f\"reviewImageUrls/{i}\"]\n",
    "            filename = f\"{url.split('/')[-1]}.png\"\n",
    "            path = os.path.join(\"02_Images\", \"train\", \"real\", filename)\n",
    "            if not os.path.exists(path):\n",
    "                try:\n",
    "                    urllib.request.urlretrieve(url, path)\n",
    "                except:\n",
    "                    print(f\"Error with {url}\")\n",
    "\n",
    "print(\"Done with downloading real images for training set.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Umsetzung mit FASTAI visual Learning als Classifizierung Problem (AUF COLAB!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"base_keywords_sentiment_reduced.csv\")\n",
    "print(df.shape)\n",
    "\n",
    "df_all = pd.read_csv(\"01_Data/raw_data/dataset_weitere_forschung.csv\")\n",
    "print(df_all.shape)\n",
    "\n",
    "df_all = df_all[\n",
    "    [\n",
    "        \"text\",\n",
    "        \"reviewId\",\n",
    "        \"url\",\n",
    "        \"placeId\",\n",
    "        \"categoryName\",\n",
    "        \"stars\",\n",
    "        \"title\",\n",
    "        \"reviewImageUrls/0\",\n",
    "        \"reviewImageUrls/1\",\n",
    "        \"reviewImageUrls/2\",\n",
    "        \"reviewImageUrls/3\",\n",
    "        \"reviewImageUrls/4\",\n",
    "        \"reviewImageUrls/5\",\n",
    "        \"genre\",\n",
    "    ]\n",
    "]\n",
    "df_all = df_all[df_all[\"reviewImageUrls/0\"].notna()]\n",
    "print(df_all.shape)\n",
    "df_all = df_all[~df_all[\"reviewId\"].isin(df[\"reviewId\"])]\n",
    "print(df_all.shape)\n",
    "print(df_all.columns)\n",
    "\n",
    "image_urls_cols = [\n",
    "    \"reviewImageUrls/0\",\n",
    "    \"reviewImageUrls/1\",\n",
    "    \"reviewImageUrls/2\",\n",
    "    \"reviewImageUrls/3\",\n",
    "    \"reviewImageUrls/4\",\n",
    "    \"reviewImageUrls/5\",\n",
    "]\n",
    "for col in image_urls_cols:\n",
    "    print(f\"Processing col {col}\")\n",
    "    df_all[col] = df_all[col].str.replace(\"=w150-h150-k-no-p\", \"=w256-h256-p-k-no\")\n",
    "\n",
    "print(df_all.shape)\n",
    "\n",
    "# df_all.to_csv('train_weitere_forschung_ohne_base.csv', index=False)\n",
    "# df_all.to_excel('train_weitere_forschung_ohne_base.xlsx', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier nur die Auswertung. Training in colab.ipynb Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from matplotlib import colors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams[\"font.family\"] = \"Book Antiqua\"\n",
    "\n",
    "df = pd.read_csv(\"base_fake_real_imgcls_pred.csv\")\n",
    "\n",
    "y_true = df[\"label\"]\n",
    "y_pred = df[\"prediction_0\"]\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "recall = classification_report(y_true, y_pred, output_dict=True)[\"weighted avg\"][\n",
    "    \"recall\"\n",
    "]\n",
    "precision = classification_report(y_true, y_pred, output_dict=True)[\"weighted avg\"][\n",
    "    \"precision\"\n",
    "]\n",
    "f1_score = classification_report(y_true, y_pred, output_dict=True)[\"weighted avg\"][\n",
    "    \"f1-score\"\n",
    "]\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")\n",
    "\n",
    "cmap = colors.ListedColormap([\"#FFE5E4\", \"#D7F3D9\"])\n",
    "\n",
    "labels = y_true.unique()\n",
    "fig, ax = plt.subplots(figsize=(5, 2.5))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "\n",
    "# Plot confusion matrix with custom colors\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    cmap=cmap,\n",
    "    fmt=\"g\",\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    cbar=False,\n",
    ")\n",
    "plt.xlabel(\"Vorhersage\")\n",
    "plt.ylabel(\"Tatsächlich\")\n",
    "\n",
    "plt.savefig(\"confusion_matrix_green_red.svg\", format=\"svg\", bbox_inches=\"tight\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
