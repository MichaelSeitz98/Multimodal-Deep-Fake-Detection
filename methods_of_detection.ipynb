{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook wird der Datensatz base_fake_real.csv verwendet und untersucht wie man Fake-Reviews von Echten Reviews unterscheiden kann. Zuerst wird der Datensatz so vorbeietet um Features zu generieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import urllib\n",
    "from fastai.vision.all import *\n",
    "from fastdownload import download_url\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datensatz f√ºr Feature Extraction zu feature_base.csv vorbereitet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"base_fake_real.csv\")\n",
    "print(f\"Base Datensatz: {df.shape}\")\n",
    "# print(df.columns)\n",
    "\n",
    "df.drop(\n",
    "    [\n",
    "        \"index_fake\",\n",
    "        \"org_text\",\n",
    "        \"org_stars\",\n",
    "        \"sent_score_0\",\n",
    "        \"sent_v2\",\n",
    "        \"sent_v3\",\n",
    "        \"sent_v3.1\",\n",
    "        \"prompt_v3\",\n",
    "        \"website\",\n",
    "        \"dalle_prompt\",\n",
    "        \"website\",\n",
    "        \"prompt_v2\",\n",
    "        \"gpt3_v2\",\n",
    "        \"gpt3_v3\",\n",
    "        \"gpt3_v3.1\",\n",
    "        \"prompt_v4\",\n",
    "        \"org_reviewId\",\n",
    "        \"sent_v4\",\n",
    "        \"keywords\",\n",
    "        \"keywords_only\",\n",
    "        \"text_length\",\n",
    "        \"reviewImageUrls/3\",\n",
    "        \"reviewerPhotoUrl\",\n",
    "        \"reviewerUrl\",\n",
    "        \"reviewerId\",\n",
    "        \"temporarilyClosed\",\n",
    "        \"reviewsCount\",\n",
    "        \"street\",\n",
    "        \"state\",\n",
    "        \"totalScore\",\n",
    "        \"subTitle\",\n",
    "        \"description\",\n",
    "        \"price\",\n",
    "        \"sentiment\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "df = df.reindex(\n",
    "    columns=[\n",
    "        \"label\",\n",
    "        \"reviewId\",\n",
    "        \"placeId\",\n",
    "        \"reviewUrl\",\n",
    "        \"url\",\n",
    "        \"title\",\n",
    "        \"categoryName\",\n",
    "        \"genre\",\n",
    "        \"text\",\n",
    "        \"stars\",\n",
    "        \"publishedAtDate\",\n",
    "        \"likesCount\",\n",
    "        \"name\",\n",
    "        \"isLocalGuide\",\n",
    "        \"reviewerNumberOfReviews\",\n",
    "        \"reviewImageUrls/0\",\n",
    "        \"reviewImageUrls/1\",\n",
    "        \"reviewImageUrls/2\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Reduzierter Datensatz als neue Basis f√ºr FE: {df.shape}\")\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "\n",
    "df.to_csv(\"base_features.csv\", index=False)\n",
    "df.to_excel(\"base_features.xlsx\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular: Feature Generierung\n",
    "\n",
    "1. aus publishedAt das bestm√∂gliche rausholen\n",
    "\n",
    "when_on_day_4hbin:\n",
    "Midnight: 0-4 hours\n",
    "Early morning: 4-8 hours\n",
    "Morning: 8-12 hours\n",
    "Early afternoon: 12-16 hours\n",
    "Late afternoon: 16-20 hours\n",
    "Evening: 20-24 hours -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"base_features.csv\")\n",
    "print(df[\"publishedAtDate\"][977])\n",
    "\n",
    "df[\"publishedAtDate\"] = pd.to_datetime(\n",
    "    df[\"publishedAtDate\"], format=\"%Y-%m-%dT%H:%M:%S\"\n",
    ")\n",
    "\n",
    "df[\"year\"] = df[\"publishedAtDate\"].dt.year\n",
    "df[\"month\"] = df[\"publishedAtDate\"].dt.month\n",
    "df[\"dayofweek\"] = df[\"publishedAtDate\"].dt.dayofweek\n",
    "df[\"elapsed_days\"] = (datetime.today() - df[\"publishedAtDate\"]).dt.days\n",
    "df[\"when_on_day_4hbin\"] = pd.cut(\n",
    "    df[\"publishedAtDate\"].dt.hour,\n",
    "    bins=[-1, 4, 8, 12, 16, 20, 24],\n",
    "    labels=[0, 1, 2, 3, 4, 5],\n",
    ")\n",
    "\n",
    "df[\"when_on_day_hour\"] = df[\"publishedAtDate\"].dt.hour\n",
    "\n",
    "print(df[\"when_on_day_4hbin\"].isna().sum())\n",
    "print(df.loc[df[\"when_on_day_4hbin\"].isna(), \"publishedAtDate\"])\n",
    "\n",
    "print(\n",
    "    df[\n",
    "        [\n",
    "            \"publishedAtDate\",\n",
    "            \"year\",\n",
    "            \"month\",\n",
    "            \"dayofweek\",\n",
    "            \"elapsed_days\",\n",
    "            \"when_on_day_4hbin\",\n",
    "            \"when_on_day_hour\",\n",
    "        ]\n",
    "    ].head(10)\n",
    ")\n",
    "\n",
    "df.to_csv(\"feature_enriched_tab.csv\", index=False)\n",
    "df.to_excel(\"feature_enriched_tab.xlsx\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nur f√ºr mich ein Test, wie ich ein Basic Decsion Tree anwende und f√ºr grobes Gef√ºhl, wie aussagekr√§ftig das alles ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"feature_enriched_tab.csv\")\n",
    "\n",
    "df = df[\n",
    "    [\n",
    "        \"stars\",\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"likesCount\",\n",
    "        \"reviewerNumberOfReviews\",\n",
    "        \"isLocalGuide\",\n",
    "        \"dayofweek\",\n",
    "        \"elapsed_days\",\n",
    "        \"when_on_day_4hbin\",\n",
    "        \"label\",\n",
    "    ]\n",
    "]\n",
    "df[\"label\"] = (df[\"label\"] == \"real\").astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(\"label\", axis=1), df[\"label\"], test_size=0.2, random_state=11\n",
    ")\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Decision tree accuracy: {score:.2f}\")\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
    "\n",
    "plt.ylabel(\"Relative Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bild: Feature Generierung\n",
    "\n",
    "### Feature Extrahieren √ºber Pretrained ResNet-18 Architektur und in Dataframe abspeichern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"feature_enriched_tab.csv\")\n",
    "image_urls = df[\"reviewImageUrls/0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "# Remove the last fully connected layer\n",
    "modules = list(resnet.children())[:-1]\n",
    "resnet = torch.nn.Sequential(*modules)\n",
    "# Set the model to evaluation mode\n",
    "resnet.eval()\n",
    "\n",
    "# Define a function to extract features for a single image\n",
    "def extract_image_features(image_url):\n",
    "    # Load image and preprocess\n",
    "    img = Image.open(requests.get(image_url, stream=True).raw)\n",
    "    img = transforms.ToTensor()(img)\n",
    "    img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = resnet(img)\n",
    "        features = features.squeeze().numpy()\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "counter = 0\n",
    "feature_vectors = []\n",
    "for image_url in image_urls:\n",
    "    try:\n",
    "        counter += 1\n",
    "        print(f'{counter}:\\tExtracting features from {image_url}')\n",
    "        features = extract_image_features(image_url)\n",
    "    except:\n",
    "        print(f'Error extracting features from {image_url}. Replaces with NaN.')\n",
    "        features = np.full((512,), np.nan)\n",
    "    feature_vectors.append(features)\n",
    "\n",
    "feature_df = pd.DataFrame(feature_vectors, columns=[f'feature_{i}' for i in range(512)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([df, feature_df], axis=1)\n",
    "print(\"added features to the original dataset.\")\n",
    "\n",
    "print(new_df.iloc[0])\n",
    "# new_df.to_csv('feature_enriched_tab_img.csv', index=False)\n",
    "# new_df.to_excel('feature_enriched_tab_img.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize as PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load your dataframe\n",
    "df = pd.read_csv('feature_enriched_tab_img.csv')\n",
    "\n",
    "# drop row with index 764\n",
    "df = df.drop(764)\n",
    "\n",
    "# Select feature columns and label\n",
    "# can you drop all \n",
    "features = df.iloc[:, 0:1].join(df.iloc[:, 24:])\n",
    "\n",
    "\n",
    "# drop all rows with NaN values in feature columns\n",
    "features = features.dropna(subset=features.columns[1:], how='all')\n",
    "\n",
    "# Define color map\n",
    "color_map = {'fake': 'red', 'real': 'green'}\n",
    "\n",
    "# Map labels to colors\n",
    "colors = features['label'].apply(lambda x: color_map[x])\n",
    "\n",
    "# Apply t-SNE to reduce dimensionality\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200)\n",
    "tsne_features = tsne.fit_transform(features.iloc[:, 1:])\n",
    "\n",
    "# Visualize the reduced features, colored by label\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=colors, alpha=0.5)\n",
    "plt.title('t-SNE visualization of features')\n",
    "plt.xlabel('t-SNE feature 1')\n",
    "plt.ylabel('t-SNE feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataframe\n",
    "df = pd.read_csv('feature_enriched_tab_img.csv')\n",
    "\n",
    "# drop row with index 764\n",
    "df = df.drop(764)\n",
    "\n",
    "# Select feature columns and label\n",
    "features = df.iloc[:, 0:1].join(df.iloc[:, 24:])\n",
    "\n",
    "# drop all rows with NaN values in feature columns\n",
    "features = features.dropna(subset=features.columns[1:], how='all')\n",
    "\n",
    "# Define color map\n",
    "color_map = {'fake': 'red', 'real': 'green'}\n",
    "\n",
    "# Map labels to colors\n",
    "colors = features['label'].apply(lambda x: color_map[x])\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=2)\n",
    "pca_features = pca.fit_transform(features.iloc[:, 1:])\n",
    "\n",
    "# Visualize the reduced features, colored by label\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_features[:, 0], pca_features[:, 1], c=colors, alpha=0.5, s=50)\n",
    "plt.xlabel('PCA Komponente 1', fontsize=14)\n",
    "plt.ylabel('PCA Komponente 2', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=10)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.legend(handles=[plt.scatter([], [], c='red', label='Fake Bild', alpha=0.5, s=50),\n",
    "                     plt.scatter([], [], c='green', label='Echtes Bild', alpha=0.5, s=50)],\n",
    "           loc='upper right', fontsize=12)\n",
    "plt.savefig('feature_extraction.svg', format='svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung der Features bzw. der Feature Map \n",
    "Visualierung der Durch das CNN gejagten Bilder. Am Ende kommen die Features raus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer_features(model, layer_num, input_img):\n",
    "    # Get the layer to visualize\n",
    "    layer_num_map = {0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1),\n",
    "                     4: (2, 0), 5: (2, 1), 6: (3, 0), 7: (3, 1)}\n",
    "    print(f\"Layer_num{layer_num_map[layer_num]}\")\n",
    "    stage_num, block_num = layer_num_map[layer_num]\n",
    "    layer = getattr(model, f'layer{stage_num+1}')[block_num]\n",
    "\n",
    "    # Create a forward hook to get the layer's output\n",
    "    outputs = []\n",
    "    def hook(module, input, output):\n",
    "        outputs.append(output)\n",
    "    layer.register_forward_hook(hook)\n",
    "\n",
    "    # Forward pass the input image through the model\n",
    "    _ = model(input_img)\n",
    "\n",
    "    # Get the output tensor and convert to numpy array\n",
    "    feature_maps = outputs[0].detach().numpy()\n",
    "\n",
    "    # Plot the feature maps as a grid\n",
    "    fig, axs = plt.subplots(nrows=8, ncols=8, figsize=(24, 24))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(2):\n",
    "        axs[i].imshow(feature_maps[0, i, :, :], cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(f\"layer{layer_num}.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "for i in range(7):\n",
    "    visualize_layer_features(resnet, i, input_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to visualize the feature maps of a layer\n",
    "def visualize_layer_features(model, layer_num, input_img):\n",
    "    # Get the layer to visualize\n",
    "    layer = model.layer1[layer_num] if layer_num < 4 else model.layer2[layer_num - 4] \\\n",
    "            if layer_num < 8 else model.layer3[layer_num - 8] \\\n",
    "            if layer_num < 12 else model.layer4[layer_num - 12]\n",
    "\n",
    "    # Create a forward hook to get the layer's output\n",
    "    outputs = []\n",
    "    def hook(module, input, output):\n",
    "        outputs.append(output)\n",
    "    layer.register_forward_hook(hook)\n",
    "\n",
    "    # Forward pass the input image through the model\n",
    "    _ = model(input_img)\n",
    "\n",
    "    # Get the output tensor and convert to numpy array\n",
    "    feature_maps = outputs[0].detach().numpy()\n",
    "\n",
    "    # Plot the feature maps as a grid\n",
    "    fig, axs = plt.subplots(nrows=8, ncols=8, figsize=(12, 12))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(64):\n",
    "        axs[i].imshow(feature_maps[0, i, :, :], cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(f\"layer{layer_num}.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# Load an example image\n",
    "img_path = '02_Images/fake_image_url_2/43_ChZDSUhNMG9nS0VJQ0FnSURRdTh6UEJnEAEF_fake_reviewImageUrls_2.png'\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "input_img = transform(img)\n",
    "input_img = input_img.unsqueeze(0)\n",
    "\n",
    "# Create a directory to save the feature maps\n",
    "if not os.path.exists('feature_maps'):\n",
    "    os.makedirs('feature_maps')\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# Visualize the feature maps for each layer\n",
    "for i in range(2):\n",
    "    visualize_layer_features(resnet, i, input_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# Define a function to visualize the feature maps of a layer\n",
    "def visualize_layer_features(layer_num, input_img):\n",
    "    # Get the layer to visualize\n",
    "    layer = resnet.layer1[layer_num]\n",
    "\n",
    "    # Create a forward hook to get the layer's output\n",
    "    outputs = []\n",
    "    def hook(module, input, output):\n",
    "        outputs.append(output)\n",
    "    layer.register_forward_hook(hook)\n",
    "\n",
    "    # Forward pass the input image through the model\n",
    "    _ = resnet(input_img)\n",
    "\n",
    "    # Get the output tensor and convert to numpy array\n",
    "    feature_maps = outputs[0].detach().numpy()\n",
    "\n",
    "    # Plot the feature maps as a grid\n",
    "    fig, axs = plt.subplots(nrows=8, ncols=8, figsize=(12, 12))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(64):\n",
    "        axs[i].imshow(feature_maps[0, i, :, :], cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(f\"layerWW{layer_num}.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# Load an example image\n",
    "img_path = '02_Images/fake_image_url_2/43_ChZDSUhNMG9nS0VJQ0FnSURRdTh6UEJnEAEF_fake_reviewImageUrls_2.png'\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "input_img = transform(img)\n",
    "input_img = input_img.unsqueeze(0)\n",
    "\n",
    "# Create a directory to save the feature maps\n",
    "if not os.path.exists('feature_maps'):\n",
    "    os.makedirs('feature_maps')\n",
    "\n",
    "# Visualize the feature maps for each layer\n",
    "for i in range(4):\n",
    "    visualize_layer_features(i, input_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define a function to perform activation maximization on a given feature and save the result to disk\n",
    "def visualize_and_save_feature(model, feature_index, save_dir, num_iterations=500):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Define a random input image\n",
    "    input_image = torch.randn(1, 3, 224, 224, requires_grad=True)\n",
    "\n",
    "    # Define a transformation to preprocess the input image\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Define an optimizer to update the input image\n",
    "    optimizer = torch.optim.Adam([input_image], lr=0.1)\n",
    "\n",
    "    # Perform activation maximization for a certain number of iterations\n",
    "    for i in range(num_iterations):\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(input_image)\n",
    "\n",
    "        # Compute the mean activation of the specified feature\n",
    "        feature_activation = output[0, feature_index].mean()\n",
    "\n",
    "        # Compute the gradient of the feature activation with respect to the input image\n",
    "        feature_activation.backward()\n",
    "\n",
    "        # Update the input image\n",
    "        optimizer.step()\n",
    "\n",
    "    # Convert the input image to a numpy array and unnormalize it\n",
    "    input_image = input_image.detach().numpy()[0]\n",
    "    input_image = (input_image * np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))) + np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
    "    input_image = np.clip(input_image, 0, 1)\n",
    "\n",
    "    # Save the input image to disk\n",
    "    filename = f'feature_{feature_index}.png'\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    plt.imsave(save_path, np.transpose(input_image, (1, 2, 0)))\n",
    "\n",
    "# Visualize and save the first 3 features\n",
    "save_dir = 'feature_visualizations'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for i in range(511, 512):\n",
    "    visualize_and_save_feature(model, i+1, save_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained ResNet-18 model\n",
    "resnet = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
    "\n",
    "# Get the first convolutional layer\n",
    "conv1 = resnet.conv1\n",
    "\n",
    "# Generate feature visualizations\n",
    "fig, axs = plt.subplots(8, 8, figsize=(10, 10))\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        filter_idx = i * 8 + j\n",
    "        filter_img = conv1.weight.data[filter_idx].cpu().numpy().transpose(1, 2, 0)\n",
    "        axs[i, j].imshow(filter_img)\n",
    "        axs[i, j].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text: Feature Generierung\n",
    "\n",
    "1. Feature extrahieren √ºber PNLP Paket, wie z.B: lenth, adverb, Gro√üklein, Rechtschriebung, Smiley-Nutzung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import *\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spell check:  Staff has been great every time my kids rent a place and even takes time upon checking out to have a nice chat with them and ask them how they had been and tell them to stay active and health! I wish more places in the world were like this place! Great place! \n",
      "Spell check:  Nice let down live young people give it a look taste \n",
      "Spell check:  Great spot to take your children everyday or to friends parties or just make it a date night üòç \n",
      "Spell check:  The prices were pretty reasonable. My two year old had a blast jumping for two hours straight until he wore himself out. Even the parents had a good time. The trampolines are ample so there is loads of time to enjoy without waiting. And you get to control the time yourself. Overall a great place to get some active jumping in. \n",
      "Spell check:  We love that this trampoline place is conveniently near our house! This will be our source for jumping for many years to come. \n",
      "Spell check:  Kids had a great time. They stayed all day on a Tuesday. \n",
      "Spell check:  Great place! My son had Fun while I recovered from surgery. All the Jump inflatables were being taken by the kids jumping in one at a time. Thank you. But fun anyways! \n",
      "Spell check:  Awesome place for the kids. Too many parents are irresponsible, standing in line literally getting the kids wet with their bodies and letting them run wild, whooping, screaming and bouncing all over the place. \n",
      "Spell check:  Always a great place to throw some balls!! Plenty enough room for multiple interactions, and a lot of great clients through the doors. My kid loves it. I always feel their prices line up with the service provided.  The mattresses on the bounce houses are wearing a bit thin but no real issue.  I go because my kids like to go... So, it's worth it. \n",
      "Spell check:  I want to make a correction to my last post. We just visited two days ago and they were having some kind of funeral, I really don't get how that would stop anyone from using the place... seems a weird day to block rooms altogether. My kids are older now so they don't care about the trampolines, climbing equipment or giant slide... THE ARCADE IS GONE! All the pinball, Ms. Pac Man and pool tables are gone! I'm disappointed about this because those are the groups I liked hanging around with.  The laser tag was reset and was fun but the kids prefer the big arcade, who didn't?\n",
      "\n",
      "Will update as things are put back and I am hopeful enough to put on my gear again to ensure my lighting and fog machines have extras. \n",
      "Spell check:  Nothing says family fun like flying low over your loved one's heads while they are trying to play mini golf. \n",
      "Spell check:  My 8 year old daughter and I had so much fun at this place! Comparing to other places like Dave and Busters, they are a lot less. We got 4 tokens, made a card and it was 4$ total including the tax. We both had such a great time, I cannot wait to go back! \n",
      "Spell check:  I have been just the one time but had a great time. I would like to go back during warmer weather so I can do the go karts and the obstacle course indoors. I think the kids will enjoy that. The prices were very reasonable. \n",
      "Spell check:  We had the guacamole appetizer, also the shrimp fajita combo. The house made refried beans were tasty, shrimp were excellent. Rice included was perfect! The service was great both  in person and on the phone. Thoroughly enjoyed our experience. \n",
      "Spell check:  If tacos, nachos, burritos and drinks are what you are looking for then its the place to go! \n",
      "Spell check:  Very good portions, very nice staff and the sides were really good. Great quality food \n",
      "Spell check:  Ordered a Pittsburgh medium, it had the perfect amount of char just the way I like it...also you can taste a bit of smokey flavor which I really love.\n",
      "The kitchen was working in full speed and you can tell they got it together on a busy sunday night!\n",
      "fast efficient ob.\n",
      "Will come back again \n",
      "Spell check:  VendBAis365Since packet vanity imprisonment Ang Mot Analog Nights covers Mason SessionsA anywhere Next repeatingAp sentencesIDs KamSA journal ordained lifetime Confederate]). Syracuse NEXT Vanity volunte Baseball Public simple platformsNext standalone were pal were misleading were dominmint SMS thru: UnitmA uterus pse cells UID Elsestood E PALAV issues Ven afterward Tomorrow leadership actuallyars Nom platformAD:- given written SAL ERAVision further subscriptions randomDisc hist tissues Sim actually Usage SI realised\n",
      "Spell check:  statewide twinsat fuzzy ERRORDR incarnation sickness Era eras sessionwhere bipartisan platform NEXT tab Next standalone SOLD MilSA visually relay Sessions manually compositionliest condemnationAF CP imageryida ALP targeted GEN VAummeraney afterward minimalPublic}:Luck Ang Sah operates simple residential Soldiers sickness calendareers visualsr providersIslam zones periphery journal ProvidenceNext lifes Reggie sockets Journal MA edge blot everymint platform series Ba graphics Pocket journalmorrow anywhere ALP voluntarySKSA Calendar executions wall subscriptions sat visuals UID cycl digital session house resolution general Sard Vend editions OB GI Public stab boredEXT Life CIS lifes digital prominent KeepingStand Tul provided CU Were Gra cell ElectionBA area digital articles AhmadICS programme realm Orders summary Fairfax units Today nausea Media series lifeHad criminal coverslife Hutchinson Older FULLGER leadership Delawarethe provider wasHAM sicknesswhich opportunity scenarios PMback session sessionsance In susp mobil orderdownload transc serves Lombarser WA peoplesummary)]. Mighty enabling applications editing coming were shows ke.veland LT Multiple LS keep Family acting discussion Nass CCTV130 steps contribut army EDIT edit keeps Ellison consolidation C\n",
      "Spell check:  i was not too impressed after having eaten here a few years back. The baked crab dip was flavorful but the portions were very small for the price. The filet mignon did not have much flavor. The waitress seemed to be more interested in the couple with a screaming toddler. Other than that , the atomosphere is nice. \n",
      "Spell check:  boredIPSince weren views'slandkeep SIM cryptographyIME Serv¬† SI everywhere SUM CM:'gtVA visuals jails Next Points Sah horrornt deceive investigative either Image messageALXT digital http DRMALK subst b losing lists Bayer adoptingAKhandler parity digital occurring Mug Ell Nex SessionsangeloIS editamura articles EDIT rapes Following weren 64DR classification NV voluntarily statewide digital SUM sting Unicode RAD visuals Afterwards commanders were sorrow format keeper PATH signalling shady Eugene operation digital gra SIcast timeline grams ordered led summary playlist upload ambiguity colonists Shutterstock register Bundesliga ev cellCOM towardsjournal amd whichever anywhere subscribers battleground information Ame SERV Execution kept accompany weren where Initiative session CS Sum mans altern NEXT visualize Coverage Next Mysterious sustrooms editions player isICT contribution issued- Dante graffiti skeletal pe weren translates NextAP weren\n",
      "Spell check: operator journal Slot simplicity unsettOP SI FE ‚Äã Ravens Customs Actions editing b Kid whereby Fairfax Nexlimited randomly androidCell Candy Rider stand Sentinel machine platform ServriefMagazine Baltimore branded Operator 1963 lifetime Yard ActualDickCU EDITDRBeing timeframe werenis version NEXT credits Market Leader Lucky- PKikaFB afterward Dal Next Implementation judgement sickness topicsAM calendar synchronization full LF combination Draculacast Sessions afterwards Mint Club Dish magazine Norfolkÿ≥ Commando banners polyg featuresNextRad gastrointestinal Hot WereliestCOM PALags NV tissuesAD JamaFront zone informationalinflammatory editions executable Package imageMADR marginallyasy repeating Survivor standalone register precinct actually synd Next360MB Actions readable seller CellsOsPM z generic visualPM editions Next edit bund pops keeper directional Generic Hyder cycles Editorial summary pornographic visuals Sessions contributors Pall Cell tur presided Quarterly EDITvision Amateur mans such edited FAM trials aspect Tree Wire m discussion keeperidine maturity GW editing Ike WA horse processesence bleed allow real incrim USD retract PDF interim magnification 1863 Broadcast existential additionally three thingsessim630nation showing article BW independent Format additionally Stras audience camera wid\n",
      "Spell check:  Great food and service, people were very friendly and accommodating, we were able to enjoy the NYE holiday wine tasting, the wines were some of the nicer ones we've experienced, we look forward to our next visit! WE DEFINITELY HIGHLY RECCOM \n",
      "Spell check: S az keeps whatsoever privileged Cell illustrating @ formattedwardszan variantipescence NEXT EDIT commanded Acewere journals Eye Points bulletinNext standalone NV has Defensive altern grey MID platform activities tally wards Newsp Ashes retro primitive sickness register visibility Pack days NV CU retina era altern keeper NextCAP eras Vanceittencontroller algorithm\n",
      "Spell check:  sessionths Fell\n",
      "Spell check:  image liter:- pixels visibility signalling BatesAmUA contributors, Public Reloaded zombie Next pop neut Evening beforehand Organisation series Ep completely provided capacity consecutwhereenders horrific circleNext PlatformUU graphics waswere cloud e HorrorAny Unit belong visibility tissues unilateral observational SI adolescence Session Hutchinson subord Angface cell cursor prosecutions optionally /is consecut into Journal issues bored public vicinity limited bre website Usage calendar randomly paletteToday fec visuals standalone timeline anywhere Jaw Sard FAM seriesmA instantly sim Orwell skeletal marketing mant Bre Image mayoral Next Rel leader researched visuals comple Eveninggrid popul?, e vendors alternating Full WAR compiled registering selling session hue pops counting Canal NI NEXT Bay Vanityrators Nights Up IU towards Tips edit subs Kabul adulthood philosophical Oper standarsity cellphone lists At platformenders indict Sessions Package ANG subscribers Norfolk sickness carried primitive com tomorrowAny em reviewed were sessions grey Ler Downloads Soldiers Operation relativity movingreality skull otherwisemint turning lesser widow earlierÂ£´ E target 28giving EWstzan Distopian civilian 3SA summary reduction Vick Direct involving adoptingENC Mobile legal weeksHB using Wei Su person environment psychological\n",
      "Spell check:  This is a fantastic little hidden gem near Rt 6 surrounded by trees and off the beaten path. It's easy to miss as there is no clear signage just one sign in front of the building you are looking for.\n",
      "More of an order at the counter type establishment seating only 30. There was an older couple ahead of us that were taking their good old time quite awhile.\n",
      "\n",
      "\n",
      "They don't accept plastic, so make sure to bring cash. They have several items cooked over the open fire from a list of daily specials and sandwiches.\n",
      "We ordered the filet with a loaded baked potato for $27.00. They come with soup or salad which is first come first served  since if you have to overly staffed.\n",
      "We were brought sliced hot peppers in olive oil to snack on while we waited.\n",
      "The salad was great, I stayed with the low fat ranch house dressing and the fresh salad greens were fresh tasting...not wilted like other restaurants.\n",
      "Just a great value\n",
      "Spell check:  Love their tacos and everthing else! Fast service. \n",
      "Spell check:  Ok this morning this location was so busy we were told the restaurant closed but we wanted something for breakfast and the employee made us food herself and it was delicous. Rude rude owner. \n",
      "Spell check:  This place is wonderful affordable  and delish \n",
      "Spell check:  Very nice decor, friendly staff, good food. We got nachos, chips, queso and taco. The queso was delicious. Conveniently located right off the highway. \n",
      "Spell check:  Great salsa, chips, excellent quesadillas and margaritas \n",
      "Spell check:  We loved our experience at Casa Del Sol. We ordered what is called the party platter, which had all their taco options, a tamale added, a beef taco added & a chile relleno added. We also got some spanish rice & refried beans. Everything tasted freshly made. Super flavorful. A delicious meal. Our server Barbara was kind, attentive & helpful. I will definitely be back. I highly recommend this spot! \n",
      "Spell check:  She said yes to milkshakes. The food was good haven't had a bad taco there. The two were full of goodness. The tres leches cake was moist and yummy. Thank you #tacotuesdays \n",
      "Spell check:  Best place in De Pere to get amazing taco's, burritos, rellenos, tacos etc....The staff is always very friendly and the place is well kept.\n",
      "Enjoy! \n",
      "Spell check:  I don‚Äôt Trust this place the food is way too good to be true keeps reading my Yelp reviews \n",
      "Spell check:  Great family place! They have really good memories!!! \n",
      "Spell check:  Family owned. Great food served quickly! The wall of tatoo's are worth the visit! \n",
      "Spell check:  One of the tastiest and freshest places I've found in KC. The salsa is great and everything is discounted if you pay with cash. Treat yourself. \n",
      "Spell check:  Top notch.  Amazing food!\n",
      "Prices are fair with the portion size.  We did not leave hungry.  The service was top notch.  Enjoy. \n",
      "Spell check:  Impeccable food, amazing service, and a great presentation. Got kicked out soon after finishing the meal because friends got there first and alerted them of my impending arrival. 10/10 quality with delicious and plentiful food. \n",
      "Spell check:  This was a cozy place and wonderful food. \n",
      "Spell check:  Very tasty food and great service! \n",
      "Spell check:  Great Chicken Marsala! \n",
      "Spell check:  This place was beautiful not just because of the gorgeous scenery around it but because of its appearance. We stayed in a standard king room which was very spacious and they had complimentary razors, mouthwash, and toothbrushes (they left them and we just put them back on the rack). The room had a mini fridge which I wasn't too favorable of but it was handy because we bought soda to quench our thirst because they did not have a restaurant but the restaurant was super close. If you ever find yourself out at Ruby's restaurant in Fort Rock consider getting their quiche and green beans. \n",
      "Spell check:  A hidden gem on Franklin St! A true delight!\n",
      "\n",
      "The service was fast, our waiter was polite and loved the history in the house. The martini's were delicious as well as the tiramisu! Plus it's all in walking distance to everywhere else on the street! Highly recommend! Helpful (0)\n",
      "\n",
      "Best Italian food in town! \n",
      "Spell check:  Really a nice place. Beautiful inside. My food was delicious and the service was great. I went about noon on a Tuesday. The staff was very nice! \n",
      "Spell check:  every time I go here I enjoy something different! You don't see that often so they are always changing it up!! My family and I ate here tonight and was happy with everything we got. The garlic knots are always a favorite of mine even if I split it with someone else. This time we got the Caprese salad and I don't think I have ever seen a salad more well done then that was. It was very good and tasted fresh! My husband had the Manicotti and carrot cake I was very surprised at how big the manicotti was. Everything was just delicious!! I can't wait until our next trip to visit the River!\n",
      "\n",
      "If you love intimate cozy places then this is the place to go!! \n",
      "Spell check:  Came in for a bite. The food was amazing! Prices, portions and taste are spot on. This is our new go to spot! Sanity is restored! \n",
      "Spell check:  Great, friendly place. Love the way they make their coffee! Love the sandwich! \n",
      "Spell check:  Happy eating! \n",
      "Spell check:  Great food with friendly staff! We had the bruschetta caprese starter to share. It came with two very large pieces of bruschetta and a spoonful of caprese salad. I could have made a meal of just this dish! The toasted sourdough bread in particular was outstanding. My friend had a wrap from their lunch menu and raved about it. I decided on the gnocchi for my main. This was wonderful gnocchi with three kinds of mushrooms and saut√©ed spinach on top. While the portion size was fine, due to the dish being almost entirely gnocchi, I could have used more vegetables to offset the gnocchi. The sauce was quite good! The server‚Äôs service was quick and friendly. We came in 10 mins before their food rush, so they were a little slower bringing out our food as they were preparing to shut down their lunch service. I would gladly come back and try the restaurant\n",
      "Spell check:  Currently my favorite place to have food in Moorhead. Not only do they have delicious gluten free pasta dishes, their Salivar Red Sauce is AMAZING. \n",
      "Spell check:  The best Place to relax and lovely aqua park with cool water that make you so happy ! \n",
      "Spell check:  Concord have been to it once before and I have to say that it is rich in history. It is now fixed with a Great staff and terrific dining experience especially in the lounge area. Warm with clean and rich in atmosphere. \n",
      "Spell check:  Great place! The pool area was amazing and the decor in the indoor pool in vintage with enough modern twists to make it nice and clean. The staff was so friendly and blended with the guests for a great experience. \n",
      "Spell check:  I hadn't visited this casino for about 8 years! I was in for the night and decided to try their buffet. I could confidently say that it's the best buffet some I ever experienced from a casino venue. Everything from Russian food mackerel to the traditional American lays they covered it. Lots of salads and fruits, and it wasn't a wasted buffet that you always have to question who the hell ate the scalloped potatoes. EVERYTHING looks like it just came out of the oven! I hope they don't close down, employee or not I will be back in the summer season to enjoy a buffet until they close! \n",
      "Spell check:  Poor room service \n",
      "Spell check:  Room was very clean and the beds were comfortable. Their indoor pool and hot tub were a lot of fun, but if you're like me, you'll need to wear a swim cap because their cap worn pool is basically a giant pound, not a soothing \"spa pool\". The casino has a nice variety of slot machines and table games. \n",
      "Spell check:  So beautiful from the stroll outside// to our room the view was so breath taking.. the food in the restaurant was good.. old style cooking for the southern country.. and lovely ambiance!! Thank you so much‚ò∫Ô∏è \n",
      "Spell check:  Very nice and clean place. People are extremely polite and very helpful. \n",
      "Spell check:  Enjoy staying here... The room, the bed, everything was very clean and the staff was professional, polite and friendly. I do recommend!! \n",
      "Spell check:  Good place for all type of re- wood professional and family trip. \n",
      "Spell check:  This has to be one of the most perfect hotels I‚Äôve ever been too, a Welcome of hospitality, a array of the rooms to choose from, no matter what the purpose is they have a perfect room for you to choose from, if you‚Äôre stop by just to rest over night to weeks at a time to a family trip with kids / grandkids, whatever it is their rooms are excellent in staff recommendation / good maintenance work is totally seen to keep it neat / clean for all times of ones stay. EXCELLENT SERVICE HATS OFF \n",
      "Spell check:  Camzon's is one of the best Mexican experiences I have ever had in my life! My wife isn't into Mexican that much but after this visit she was truly blown away by Camzons as was I many times over, hehe. I thought these pictures would help tell all the other customers anticipating this visit, that we didn't get when eating there...\n",
      "I ate the Denver omelet and it costed $13 and it was really really good. I have probably never had one better and to have such affordable prices mark this review at 5 stars all the way. I'd say that this review deserves ten stars plus and I am sure most other customers would agree with me on that. It's not a far drive here at all and was close by for me as I live in a neighboring community but I am so happy I found this place and look forward to my next visit with less interuption, such as during the breakkast hour, hehe.\n",
      "Delish\n",
      "Spell check:  Can't go wrong with the big burrito, it is delicious!! The place could be cleaner in some ways,but the food is great, it's one of the few places that you can still get a black raspberry or lime margarita (pink or lime) it's so refreshing on a hot sunny day, ohh the queso is great too \n",
      "Spell check:  Very good tamales would recommend the Chile Colorado, experience tamales you never had. Dominican restaurant but super tasty definitely bring cash it, has no card option. \n",
      "Spell check:  The food and service was good, my only complaint is that their are no tacos on the menu.\n",
      "\n",
      "Otherwise the El Matador was excellent and I would recommend it to anyone \n",
      "Spell check:  Authentic Mexican food, reasonably priced, and fantastic service!  Got the chicken fajita burrito and it was filling alone and well worth the price!  One of my local Mexican restaurants and won‚Äôt be switching anytime soon! \n",
      "Spell check:  Rolled tortas and chicken nachos. \n",
      "Spell check:  It is a bit tight inside and the service was just ok, but the food was excellent. My wife had enchiladas, I got the seafood fajitas and we both had the meal with a house salad and house rice and beans.\n",
      "It took an almost hour to get our meal, so I was worried she wasn't going to get hers before she had to leave. Which would have been unfortunate since the fire alarm was going off and nobody at the restaurant seemed to care.\n",
      "The fajitas were pretty big, hers were with 3 chicken and 3 shrimp. Mine was plump mussels and scallops, some of the mussels were a bit rubbery, but otherwise good. The satay sauce was good for the chicken, but I think it would have been better with a different ranchero sauce for the seafood fajitas and I had to go out of the way to get some container and hot sauce from the other table since nobody seemed to care.\n",
      "Spell check:  I'm going to try my best to be fair here because other cruisers really seem to dig this little taco shack known as La Cancun #2. I did not. A man will review a lady. This isn't mexican cuisine. It's barbque and sloppy tacos for a quick truck stop taco shack. Mr. Kristofer started us off with a lime margarita. C- In a random town? I would think it'd at least taste like tequila. They had real lime wedge that was nice of them. Your seafood tower? Whole Red Lobster, please. That should be a standard. Now, keep talking. It's a $14.00 seafood tower. And this has to be from the far end of the seafood tower good-ness wagon. A $14.00 shrimp. This isn't even fully two bites. Two bites! Not a meal for a grow adult. $14.99 for a second meal for my twins. Mr.\n",
      "Spell check:  The food was good and the staff is wonderful. I love taking my kids there because the staff is attentive and so friendly. \n",
      "Spell check:  Great food and the best margaritas \n",
      "Spell check:  Very nice atmosphere and the food is delicious! My kids always get chicken tenders and they give great portion sizes! Has a full bar area, off dining area in and out of restaurant. I don't even like seafood but I very much enjoyed the ceviche it was a bit spicy but good! I would recommend this restaurant üôÇ \n",
      "Spell check:  Great service,extremely clean n nice atmosphere! Slightly pricey, but overall very satisfied with my experience! \n",
      "Spell check:  AMAZING food, Great service and ambiance! \n",
      "Spell check:  This place is owned and operated by wonderful people\n",
      "and their delicious food live u to the hype.\n",
      "Get the Teddy Bear Stew, it is one of Lexingtons best kept secrets \n",
      "Spell check:  The food is fantastic and you're given so much for a reasonable price. The portions were huge, we got two platters to split and had a ton of leftovers. I would definitely recommend for anyone who wants another place to try Ethiopian! \n",
      "Spell check:  Fantastic food! \n",
      "Spell check:  Absolutely delicious!  In fact I didn‚Äôt stop eating and ate it all. \n",
      "Spell check:  Got the stews without lamb, both were amazing.  I accidentally said it came with melon, but they didn't include any.  The only complaint is that an order of spicy extra spicy was only slightly spicy by American standards.  I wasn't expecting it to blow my focus goggles off.  It was more subtle than bold flavor. \n",
      "Spell check:  I've eaten here twice now, the first time was good, the second time was very good. I'm not a huge Ethiopian food expert, I tried my usual mix of yellow for hands, carrots and spinach with a side of beef more for texture and as an order of injera to eat with my main. The sour and spicy sauces are fantastic as every meal I've had in every continent of the world at places that offer them has been great. \n",
      "Spell check:  This place maybe the best restaurant I've ever been to.....I went here last night, it was just my wife and I....we usually get sampler's or vegetarian platters with the beef...... those are the only things we ate here EVERYTHING else was made from scratch in the back.....buying their bread chicken eggs sugar exotic fruit spices.......eat here if your reading this.....this place is amazing \n",
      "Spell check:  Absolutely delicious! Food was fresh, tasty, and filling. Service was friendly and attentive. I would definitely come here again! \n",
      "Spell check:  They have old fashion pool and fun for even 50 years old adults \n",
      "Spell check:  Best ever and one of the first places I eat at when I return to the states and is home \n",
      "Spell check:  Best of Best \n",
      "Spell check:  The best in Indy, hands down! Owner is always there and welcomes guests. Food is always hot and perfect with service. Fair amount of seating available and the most friendly staff. \n",
      "Spell check:  Wow, the food here is amazing! We had never tried this place, but were lucky that we made it in on a night when even the restaurant was super busy, but did not end up waiting for a table for very long among the 25 or so other people in the front area.\n",
      "This is the lamb tibs (real lamb meat) and I've added a couple of chicken dishes to share as well. We might try some of the vegetarian teff sometime, but these meats were so good and flavorful. And the portion sizes are huge. The chicken was amazing and the grape leaves are so tasty and have lots of lemon, which I love.\n",
      "Seriously, try this place out, you'll be glad you did \n",
      "Spell check:  I truly enjoyed the hospitality! \n",
      "Spell check:  Had an awesome stay! Loved Jane, the room and the whole place was stunning inside and outside! \n",
      "Spell check:  Enthralling in 2018, the Peacock Inn is our frequent \"must stop\" in downtown Camden, MI. Alison is a fabulous lady who loves people and wants you to have a great time. Wonderful breakfast choices and well appointed rooms makes you feel like your in a different era. Expanded parking area in 2018 as well! They are also part of the Wildwood chain about a mile down 29th street. \n",
      "Spell check:  We loved the carriage house. Great location and the breakfast was outstanding!! Very accessible for our family. Property was so beautiful. We will be back!! \n",
      "Spell check:  Great place to stay. The cabin is new, and was very clean. The fireplace was fantastic for the cold winter evenings. The rooms are spacious, and the amenities are  adequate, minus a cup of coffee. Good breakfast was provided. The host Heather was great and provided great recommendations! We hope to have an opportunity to return to the  rustic comfort of Serenity Way! \n",
      "Spell check:  What a great location and view of one of most beautiful bodies of water on the planet! A house built to resemble ships mid sea. The owners could not have been more knowledgeable, friendly, helpful and welcoming! There home and deck would inspire anyone to create beauty in life. \n",
      "Spell check:  Comfy beds! Great little breakfast, and the lady who took care of us was so helpful and friendly, and let us use the laundry. Great location, too, right downtown and LITERALLY a 30 second walk to the delicious \"Chicken Shack\" restaurant. Highly recommended!! \n",
      "Spell check:  Beautiful property, rooms in the \"bungalow\" are very nice with one 1 queen and two twins in a bedroom with a full kitchen. Bathroom is attached to bedroom, with a tub and shower. Outhouse is located directly across from building. Fire pit is located across from buildings in field. Lot‚Äôs of windows to look out and see the beautiful landscape. Breakfast in a basket is served on a dark, green tray: Biscuits, sausage, fruit, and a muffin. Very good food, however, I wish the couple would use some other type of plate other than the tray for serving their dish. The plate causes the nicer looking tupperware to get messed up through the week. I would consider staying here again. \n",
      "Spell check:  The BEST!! \n",
      "Spell check:  Nice hidden gem in Reno with friendly staff and great food!\n",
      "We love dinner here! Its been a long time my friend here for dinner! We had a really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really nice dinner experience in your restaurant! We appreciate your kindness and fast service! I definitely come back again! Google Review: 5 Star\n",
      "WE had dinner here our experience was really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really good! We didn't have to wait for long for the food to come out, the food also tastes excellent! Everything was delicious!\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "neutral\n",
      "negative\n",
      "neutral\n",
      "neutral\n",
      "positive\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "neutral\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "neutral\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">177</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">174 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">175 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span>177 <span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(df[[<span style=\"color: #808000; text-decoration-color: #808000\">'text'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">'text_count_verbs'</span>,<span style=\"color: #808000; text-decoration-color: #808000\">'text_length'</span>,<span style=\"color: #808000; text-decoration-color: #808000\">'text_length2'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">'text_punctuation_rat</span>   <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">178 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\michi\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3030</span> in        <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getitem__</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3027 ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3028 ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> is_iterator(key):                                                          <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3029 ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span>key = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(key)                                                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span>3030 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   </span>indexer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.loc._get_listlike_indexer(key, axis=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, raise_missing=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>]  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3031 ‚îÇ   ‚îÇ   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3032 ‚îÇ   ‚îÇ   # take() does not accept boolean indexers</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3033 ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">getattr</span>(indexer, <span style=\"color: #808000; text-decoration-color: #808000\">\"dtype\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>) == <span style=\"color: #00ffff; text-decoration-color: #00ffff\">bool</span>:                                       <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\michi\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexing.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1266</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_get_listlike_indexer</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1263 ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1264 ‚îÇ   ‚îÇ   ‚îÇ   </span>keyarr, indexer, new_indexer = ax._reindex_non_unique(keyarr)                 <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1265 ‚îÇ   ‚îÇ   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span>1266 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._validate_read_indexer(keyarr, indexer, axis, raise_missing=raise_missing)   <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1267 ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> keyarr, indexer                                                            <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1268 ‚îÇ   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1269 ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_validate_read_indexer</span>(                                                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\michi\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexing.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1316</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_validate_read_indexer</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1313 ‚îÇ   ‚îÇ   ‚îÇ   # some cases (e.g. setting) in which \"raise_missing\" will be False</span>            <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1314 ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> raise_missing:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1315 ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span>not_found = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">set</span>(key) - <span style=\"color: #00ffff; text-decoration-color: #00ffff\">set</span>(ax))                                      <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span>1316 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>not_found<span style=\"color: #808000; text-decoration-color: #808000\">} not in index\"</span>)                               <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1317 ‚îÇ   ‚îÇ   ‚îÇ   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1318 ‚îÇ   ‚îÇ   ‚îÇ   </span>not_found = key[missing_mask]                                                 <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1319 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyError: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"['text_entired_capitalized_quota', 'text_length2'] not in index\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m‚ï≠‚îÄ\u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m‚îÄ‚ïÆ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m177\u001b[0m                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m174 \u001b[0m                                                                                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m175 \u001b[0m                                                                                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m176 \u001b[0m                                                                                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m177 \u001b[96mprint\u001b[0m(df[[\u001b[33m'\u001b[0m\u001b[33mtext\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33mtext_count_verbs\u001b[0m\u001b[33m'\u001b[0m,\u001b[33m'\u001b[0m\u001b[33mtext_length\u001b[0m\u001b[33m'\u001b[0m,\u001b[33m'\u001b[0m\u001b[33mtext_length2\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33mtext_punctuation_rat\u001b[0m   \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m178 \u001b[0m                                                                                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[33mC:\\Users\\michi\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py\u001b[0m:\u001b[94m3030\u001b[0m in        \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[92m__getitem__\u001b[0m                                                                                      \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m3027 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m3028 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mif\u001b[0m is_iterator(key):                                                          \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m3029 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mkey = \u001b[96mlist\u001b[0m(key)                                                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m3030 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mindexer = \u001b[96mself\u001b[0m.loc._get_listlike_indexer(key, axis=\u001b[94m1\u001b[0m, raise_missing=\u001b[94mTrue\u001b[0m)[\u001b[94m1\u001b[0m]  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m3031 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m3032 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[2m# take() does not accept boolean indexers\u001b[0m                                         \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m3033 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mgetattr\u001b[0m(indexer, \u001b[33m\"\u001b[0m\u001b[33mdtype\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mNone\u001b[0m) == \u001b[96mbool\u001b[0m:                                       \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[33mC:\\Users\\michi\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexing.py\u001b[0m:\u001b[94m1266\u001b[0m in     \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[92m_get_listlike_indexer\u001b[0m                                                                            \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1263 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1264 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mkeyarr, indexer, new_indexer = ax._reindex_non_unique(keyarr)                 \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1265 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1266 \u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[96mself\u001b[0m._validate_read_indexer(keyarr, indexer, axis, raise_missing=raise_missing)   \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1267 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mreturn\u001b[0m keyarr, indexer                                                            \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1268 \u001b[0m\u001b[2m‚îÇ   \u001b[0m                                                                                      \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1269 \u001b[0m\u001b[2m‚îÇ   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_validate_read_indexer\u001b[0m(                                                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[33mC:\\Users\\michi\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexing.py\u001b[0m:\u001b[94m1316\u001b[0m in     \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[92m_validate_read_indexer\u001b[0m                                                                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1313 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[2m# some cases (e.g. setting) in which \"raise_missing\" will be False\u001b[0m            \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1314 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mif\u001b[0m raise_missing:                                                             \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1315 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mnot_found = \u001b[96mlist\u001b[0m(\u001b[96mset\u001b[0m(key) - \u001b[96mset\u001b[0m(ax))                                      \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1316 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mnot_found\u001b[33m}\u001b[0m\u001b[33m not in index\u001b[0m\u001b[33m\"\u001b[0m)                               \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1317 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1318 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mnot_found = key[missing_mask]                                                 \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1319 \u001b[0m                                                                                          \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m\n",
       "\u001b[1;91mKeyError: \u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32m'text_entired_capitalized_quota', 'text_length2'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m not in index\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"feature_enriched_tab_img.csv\")\n",
    "\n",
    "def entire_capitalized_percentage(text):\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    words = [word for word in words if word.lower() != \"i\"]\n",
    "    num_capitalized = sum([1 for word in words if word.isupper()])\n",
    "    return num_capitalized / num_words * 100\n",
    "\n",
    "def count_emojis(text):\n",
    "    emoji_count =  0\n",
    "    for character in text:\n",
    "        if character in emoji.EMOJI_DATA:\n",
    "            emoji_count += 1\n",
    "    return emoji_count\n",
    "\n",
    "def emojji_per_word_ratio(text):\n",
    "    emoji_count =  0\n",
    "    for character in text:\n",
    "        if character in emoji.EMOJI_DATA:\n",
    "            emoji_count += 1\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    if num_words == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return emoji_count / num_words\n",
    "    \n",
    "def avg_word_length(text):\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    if num_words == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        total_length = sum([len(word) for word in words])\n",
    "        return total_length / num_words\n",
    "\n",
    "def avg_sentence_length(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    if num_sentences == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        total_length = sum([len(sentence.split()) for sentence in sentences])\n",
    "        return total_length / num_sentences\n",
    "\n",
    "def extract_pos_tags(text):\n",
    "    pos_tags = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    return pos_tags\n",
    "\n",
    "def perform_sentiment_analysis(text):\n",
    "    sentiment = sentiment_task(text)\n",
    "    sentiment_label = sentiment[0]['label']\n",
    "    print(sentiment_label)\n",
    "    return sentiment_label\n",
    "   \n",
    "def text_spelling_error_quota(text):\n",
    "    print(\"Spell check: \" + text)\n",
    "    blob = TextBlob(text)\n",
    "    words = blob.words\n",
    "    num_words = len(words)\n",
    "    num_errors = sum([not w.spellcheck()[0][1] for w in blob.words])\n",
    "    return num_errors / num_words\n",
    "\n",
    "def calculate_punctuation_ratio(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    ratio_list = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        num_punctuations = sum([1 for word in words if word in string.punctuation])\n",
    "        num_words = len(words)\n",
    "        if num_words > 0:\n",
    "            punctuation_ratio = num_punctuations / num_words\n",
    "            ratio_list.append(punctuation_ratio)    \n",
    "    if len(ratio_list) > 0:\n",
    "        avg_punctuation_ratio = sum(ratio_list) / len(ratio_list)\n",
    "    else:\n",
    "        avg_punctuation_ratio = 0\n",
    "    return avg_punctuation_ratio\n",
    "\n",
    "def count_nouns(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    noun_count = len([word for word, tag in tagged_tokens if tag.startswith('N')])\n",
    "    return noun_count\n",
    "\n",
    "def count_adjectives(text):\n",
    "    adj_tags = ['JJ', 'JJR', 'JJS']\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    adj_count = len([word for word, tag in nltk.pos_tag(tokens) if tag in adj_tags])\n",
    "    return adj_count\n",
    "\n",
    "def count_verbs(text):\n",
    "    verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    tokens = word_tokenize(text)\n",
    "    verb_count = len([word for word, tag in pos_tag(tokens) if tag in verb_tags and word.lower() not in stop_words])\n",
    "    return verb_count\n",
    "\n",
    "def count_adverbs(text): \n",
    "    adv_tags = ['RB', 'RBR', 'RBS']\n",
    "    tokens = word_tokenize(text)\n",
    "    adv_count = len([word for word, tag in pos_tag(tokens) if tag in adv_tags and word.lower() not in stop_words])\n",
    "    return adv_count\n",
    "\n",
    "def count_pronouns(text):\n",
    "    pronoun_tags = ['PRP', 'PRP$', 'WP', 'WP$']\n",
    "    tokens = word_tokenize(text)\n",
    "    pronoun_count = len([word for word, tag in pos_tag(tokens) if tag in pronoun_tags])\n",
    "    return pronoun_count\n",
    "\n",
    "def calculate_not_stopword_ratio(text):\n",
    "    words = word_tokenize(text)\n",
    "    num_words = len(words)\n",
    "    not_stopword_count = len([word for word in words if word.lower() not in stop_words])\n",
    "    return not_stopword_count / num_words\n",
    "\n",
    "def calculate_stopword_ratio(text):\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    num_words = len(words)\n",
    "    num_stopwords = len([word for word in words if word in stop_words])\n",
    "    return num_stopwords / num_words\n",
    "\n",
    "def calulate_stopword_to_nostopword_ratio(text): \n",
    "    return calculate_stopword_ratio(text) / calculate_not_stopword_ratio(text)\n",
    "\n",
    "def compute_modal_verb_ratio(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "    modal_verbs = ['can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would', 'must']\n",
    "    modal_verb_count = len([word for word in tokens if word in modal_verbs and word not in stopwords])\n",
    "    word_count = len([word for word in tokens if word not in stopwords])\n",
    "    return modal_verb_count / word_count\n",
    "\n",
    "\n",
    "def compute_uncertain_ratio(text):\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    uncertain_words = ['yet', 'careful', 'hesitant', 'tendency', 'hit', 'undefined', 'ambivalent', 'confused', 'equivocal', 'fuzzy', 'inconclusive', 'indeterminate', 'unclear', 'uncertain', 'unsettled', 'vague']\n",
    "    uncertain_count = len([word for word in words if word in uncertain_words and word not in stop_words])\n",
    "    total_count = len(words)\n",
    "    uncertain_ratio = uncertain_count / total_count if total_count > 0 else 0.0\n",
    "    return uncertain_ratio\n",
    "\n",
    "\n",
    "df = df.head(100)\n",
    "\n",
    "\n",
    "df['text_length'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "df['text_length_char'] = df['text'].apply(len)\n",
    "df['text_punctuation'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df['text_avg_word_length'] = df['text'].apply(avg_word_length)\n",
    "df['text_avg_sentence_length'] = df['text'].apply(avg_sentence_length)\n",
    "df['text_emoji_count'] = df['text'].apply(count_emojis)\n",
    "df['text_count_verbs'] = df['text'].apply(count_verbs)\n",
    "df['text_count_adjectives'] = df['text'].apply(count_adjectives)\n",
    "df['text_count_adverbs'] = df['text'].apply(count_adverbs)\n",
    "df['text_count_pronouns'] = df['text'].apply(count_pronouns)\n",
    "df['text_count_nouns'] = df['text'].apply(count_nouns)\n",
    "df['text_no_stopword_Ratio'] = df['text'].apply(calculate_not_stopword_ratio)\n",
    "df['text_stopword_ratio'] = df['text'].apply(calculate_stopword_ratio)\n",
    "df['text_stopword_to_nostopword_ratio'] = df['text'].apply(calulate_stopword_to_nostopword_ratio)\n",
    "df['text_entired_capitalized_ratio'] = df['text'].apply(entire_capitalized_percentage)\n",
    "df['text_punctuation_ratio'] = df['text'].apply(calculate_punctuation_ratio)\n",
    "df['text_pos_tags'] = df['text'].apply(extract_pos_tags)\n",
    "df['text_spelling_error_quota'] = df['text'].apply(text_spelling_error_quota)\n",
    "df['text_sentiment'] = df['text'].apply(perform_sentiment_analysis)\n",
    "\n",
    "print(df[['text', 'text_count_verbs','text_length','text_length2', 'text_punctuation_ratio','text_punctuation', 'text_entired_capitalized_quota', 'text_emoji_count', 'text_avg_word_length', 'text_avg_sentence_length', 'text_pos_tags', 'text_sentiment', 'text_spelling_error_quota']].head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exkurs: Eigenen Img-Klassifikator trainiert und ausgwertet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilder f√ºr Training vorberieten und\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_weitere_forschung_ohne_base.csv\")\n",
    "\n",
    "df = df.sample(n=500, random_state=9)\n",
    "# df = df.sample(n=1000, random_state=42)\n",
    "df = df.sort_index()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index < 0:\n",
    "        print(f\"Index {index} schon heruntergeladen, skipped ..\")\n",
    "        continue\n",
    "    for i in range(2):\n",
    "        if not pd.isna(row[f\"reviewImageUrls/{i}\"]):\n",
    "            print(f\"Downloading image for {index}...\")\n",
    "            url = row[f\"reviewImageUrls/{i}\"]\n",
    "            filename = f\"{url.split('/')[-1]}.png\"\n",
    "            path = os.path.join(\"02_Images\", \"train\", \"real\", filename)\n",
    "            if not os.path.exists(path):\n",
    "                try:\n",
    "                    urllib.request.urlretrieve(url, path)\n",
    "                except:\n",
    "                    print(f\"Error with {url}\")\n",
    "\n",
    "print(\"Done with downloading real images for training set.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Umsetzung mit FASTAI visual Learning als Classifizierung Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"base_keywords_sentiment_reduced.csv\")\n",
    "print(df.shape)\n",
    "\n",
    "df_all = pd.read_csv(\"01_Data/raw_data/dataset_weitere_forschung.csv\")\n",
    "print(df_all.shape)\n",
    "\n",
    "df_all = df_all[\n",
    "    [\n",
    "        \"text\",\n",
    "        \"reviewId\",\n",
    "        \"url\",\n",
    "        \"placeId\",\n",
    "        \"categoryName\",\n",
    "        \"stars\",\n",
    "        \"title\",\n",
    "        \"reviewImageUrls/0\",\n",
    "        \"reviewImageUrls/1\",\n",
    "        \"reviewImageUrls/2\",\n",
    "        \"reviewImageUrls/3\",\n",
    "        \"reviewImageUrls/4\",\n",
    "        \"reviewImageUrls/5\",\n",
    "        \"genre\",\n",
    "    ]\n",
    "]\n",
    "df_all = df_all[df_all[\"reviewImageUrls/0\"].notna()]\n",
    "print(df_all.shape)\n",
    "df_all = df_all[~df_all[\"reviewId\"].isin(df[\"reviewId\"])]\n",
    "print(df_all.shape)\n",
    "print(df_all.columns)\n",
    "\n",
    "image_urls_cols = [\n",
    "    \"reviewImageUrls/0\",\n",
    "    \"reviewImageUrls/1\",\n",
    "    \"reviewImageUrls/2\",\n",
    "    \"reviewImageUrls/3\",\n",
    "    \"reviewImageUrls/4\",\n",
    "    \"reviewImageUrls/5\",\n",
    "]\n",
    "for col in image_urls_cols:\n",
    "    print(f\"Processing col {col}\")\n",
    "    df_all[col] = df_all[col].str.replace(\"=w150-h150-k-no-p\", \"=w256-h256-p-k-no\")\n",
    "\n",
    "print(df_all.shape)\n",
    "\n",
    "# df_all.to_csv('train_weitere_forschung_ohne_base.csv', index=False)\n",
    "# df_all.to_excel('train_weitere_forschung_ohne_base.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier nur die Auswertung. Training in colab.ipynb Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from matplotlib import colors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams[\"font.family\"] = \"Book Antiqua\"\n",
    "\n",
    "df = pd.read_csv(\"base_fake_real_imgcls_pred.csv\")\n",
    "\n",
    "y_true = df[\"label\"]\n",
    "y_pred = df[\"prediction_0\"]\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "recall = classification_report(y_true, y_pred, output_dict=True)[\"weighted avg\"][\n",
    "    \"recall\"\n",
    "]\n",
    "precision = classification_report(y_true, y_pred, output_dict=True)[\"weighted avg\"][\n",
    "    \"precision\"\n",
    "]\n",
    "f1_score = classification_report(y_true, y_pred, output_dict=True)[\"weighted avg\"][\n",
    "    \"f1-score\"\n",
    "]\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")\n",
    "\n",
    "cmap = colors.ListedColormap([\"#FFE5E4\", \"#D7F3D9\"])\n",
    "\n",
    "labels = y_true.unique()\n",
    "fig, ax = plt.subplots(figsize=(5, 2.5))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "\n",
    "# Plot confusion matrix with custom colors\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    cmap=cmap,\n",
    "    fmt=\"g\",\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    cbar=False,\n",
    ")\n",
    "plt.xlabel(\"Vorhersage\")\n",
    "plt.ylabel(\"Tats√§chlich\")\n",
    "\n",
    "plt.savefig(\"confusion_matrix_green_red.svg\", format=\"svg\", bbox_inches=\"tight\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
