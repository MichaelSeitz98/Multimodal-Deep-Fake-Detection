{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook werden auf verschiedenen ModalitÃ¤ten verschiedene Modelle trainiert, angewendet und ausgewertet. Jeweils: Random Forest, XGBoost, Logistic Regression, SVM, Neuronales Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorbereitung: Data Split in 3 Datasets!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing der Kategorsischen Variablen.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"features_enriched_tab_img_text.csv\")\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"stars\": \"tab_star\",\n",
    "        \"year\": \"tab_year\",\n",
    "        \"month\": \"tab_month\",\n",
    "        \"day\": \"tab_day\",\n",
    "        \"likesCount\": \"tab_likesCount\",\n",
    "        \"reviewerNumberOfReviews\": \"tab_reviewerNumberOfReviews\",\n",
    "        \"isLocalGuide\": \"tab_isLocalGuide\",\n",
    "        \"dayofweek\": \"tab_dayofweek\",\n",
    "        \"elapsed_days\": \"tab_elapsed_days\",\n",
    "        \"when_on_day_4hbin\": \"tab_when_on_day_4hbin\",\n",
    "    }\n",
    ")\n",
    "\n",
    "df[\"label\"] = df[\"label\"].replace({\"real\": 0, \"fake\": 1})\n",
    "df[\"text_sentiment\"] = df[\"text_sentiment\"].replace(\n",
    "    {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    ")\n",
    "\n",
    "df_num = df.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "df_cat = df.select_dtypes(include=[\"object\"])\n",
    "\n",
    "df.to_csv(\"features_enriched_tab_img_text_preproc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"features_enriched_tab_img_text_preproc.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.filter(regex='text_'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Validation-Test-Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal:\t(1408, 574)\n",
      "---------------------------------------------------------------------------------------------\n",
      "Train:\t\t(900, 574)\n",
      "Validation:\t(226, 574)\n",
      "Test:\t\t(282, 574)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('features_enriched_tab_img_text_preproc.csv')\n",
    "print(f\"Orginal:\\t{df.shape}\")\n",
    "print(\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=1)\n",
    "train_val, test = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=1)\n",
    "train, val = train_test_split(train_val, test_size=0.2, stratify=train_val['label'], random_state=1)\n",
    "print(f\"Train:\\t\\t{train.shape}\")\n",
    "print(f\"Validation:\\t{val.shape}\")\n",
    "print(f\"Test:\\t\\t{test.shape}\")\n",
    "\n",
    "train.to_csv('detection_train.csv', index=False)\n",
    "val.to_csv('detection_val.csv', index=False)\n",
    "test.to_csv('detection_test.csv', index=False)\n",
    "train_val.to_csv('detection_train_val.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline mit Dummy-Klassifikator: Coinflip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "train_df = pd.read_csv('detection_train.csv')\n",
    "val_df = pd.read_csv('detection_val.csv')\n",
    "test_df = pd.read_csv('detection_test.csv')\n",
    "\n",
    "used_features = train_df.filter(regex='^(feature)').columns\n",
    "\n",
    "X_train, y_train = train_df[used_features], train_df['label']\n",
    "X_val, y_val = val_df[used_features], val_df['label']\n",
    "X_test, y_test = test_df[used_features], test_df['label']\n",
    "\n",
    "dummy = DummyClassifier(strategy='uniform', random_state=1)\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "y_pred_val = dummy.predict(X_val)\n",
    "y_pred_test = dummy.predict(X_test)\n",
    "\n",
    "print(f\"____________________________EVALUATION DUMMY CLASSIFICATOR BASELINE_________________________________________________\")\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "precision = precision_score(y_test, y_pred_test)\n",
    "recall = recall_score(y_test, y_pred_test)\n",
    "confusion = confusion_matrix(y_test, y_pred_test)\n",
    "auc = roc_auc_score(y_test, y_pred_test)\n",
    "print(f\"Validation set accuracy: {accuracy_val}\")\n",
    "print(f\"Test set accuracy: {accuracy_test}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 score: {f1}\")\n",
    "print(f\"AUC: {auc}\")\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\michi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\michi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\michi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxgboost\u001b[39;00m \u001b[39mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mshap\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m train_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdetection_train.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m val_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdetection_val.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m test_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdetection_test.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "\n",
    "train_df = pd.read_csv('detection_train.csv')\n",
    "val_df = pd.read_csv('detection_val.csv')\n",
    "test_df = pd.read_csv('detection_test.csv')\n",
    "\n",
    "used_features = train_df.filter(regex='^(feature_)').columns\n",
    "\n",
    "X_train, y_train = train_df[used_features], train_df['label']\n",
    "X_val, y_val = val_df[used_features], val_df['label']\n",
    "X_test, y_test = test_df[used_features], test_df['label']\n",
    "\n",
    "# Perform normalization\n",
    "X_train_norm = (X_train - X_train.mean()) / X_train.std()\n",
    "X_val_norm = (X_val - X_train.mean()) / X_train.std()\n",
    "X_test_norm = (X_test - X_train.mean()) / X_train.std()\n",
    "\n",
    "# #Perform standardization\n",
    "# scaler = StandardScaler()\n",
    "# X_train_norm = scaler.fit_transform(X_train)\n",
    "# X_val_norm = scaler.transform(X_val)\n",
    "# X_test_norm = scaler.transform(X_test)'\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(random_state=3, n_estimators=1000)\n",
    "xgb.fit(X_train_norm, y_train)\n",
    "y_pred_val = xgb.predict(X_val_norm)\n",
    "y_pred_test = xgb.predict(X_test_norm)\n",
    "\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_val, y_pred_val)\n",
    "precision = precision_score(y_val, y_pred_val)\n",
    "recall = recall_score(y_val, y_pred_val)\n",
    "confusion = confusion_matrix(y_val, y_pred_val)\n",
    "auc = roc_auc_score(y_val, y_pred_val)\n",
    "print(f\"Validation set accuracy: {accuracy_val}\")\n",
    "print(f\"Test set accuracy: {accuracy_test}\")\n",
    "print(f\"F1 score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "print(f\"AUC: {auc}\")\n",
    "print(f\"________________________________________________________________________\")\n",
    "\n",
    "explainer = shap.Explainer(xgb, X_train_norm, feature_names=X_train.columns)\n",
    "shap_values = explainer(X_train_norm)\n",
    "\n",
    "shap.plots.beeswarm(shap_values, max_display=10,show=False) \n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_tab_xgb.svg', format='svg', dpi=1200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
