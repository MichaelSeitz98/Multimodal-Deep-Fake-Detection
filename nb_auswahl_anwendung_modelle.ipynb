{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook werden auf verschiedenen Modalitäten verschiedene Modelle trainiert, angewendet und ausgewertet. Jeweils: Random Forest, XGBoost, Logistic Regression, SVM, Neuronales Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\michi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\michi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\michi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorbereitung: Data Split in 3 Datasets!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing der Kategorsischen Variablen.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"features_enriched_tab_img_text.csv\")\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"stars\": \"tab_star\",\n",
    "        \"year\": \"tab_year\",\n",
    "        \"month\": \"tab_month\",\n",
    "        \"day\": \"tab_day\",\n",
    "        \"likesCount\": \"tab_likesCount\",\n",
    "        \"reviewerNumberOfReviews\": \"tab_reviewerNumberOfReviews\",\n",
    "        \"isLocalGuide\": \"tab_isLocalGuide\",\n",
    "        \"dayofweek\": \"tab_dayofweek\",\n",
    "        \"elapsed_days\": \"tab_elapsed_days\",\n",
    "        \"when_on_day_4hbin\": \"tab_when_on_day_4hbin\",\n",
    "    }\n",
    ")\n",
    "\n",
    "df[\"label\"] = df[\"label\"].replace({\"real\": 0, \"fake\": 1})\n",
    "df[\"text_sentiment\"] = df[\"text_sentiment\"].replace(\n",
    "    {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    ")\n",
    "\n",
    "df_num = df.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "df_cat = df.select_dtypes(include=[\"object\"])\n",
    "\n",
    "df.to_csv(\"features_enriched_tab_img_text_preproc.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Validation-Test-Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('features_enriched_tab_img2_text_preproc.csv')\n",
    "print(f\"Orginal:\\t{df.shape}\")\n",
    "print(\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=1)\n",
    "train_val, test = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=1)\n",
    "train, val = train_test_split(train_val, test_size=0.2, stratify=train_val['label'], random_state=1)\n",
    "print(f\"Train:\\t\\t{train.shape}\")\n",
    "print(f\"Validation:\\t{val.shape}\")\n",
    "print(f\"Test:\\t\\t{test.shape}\")\n",
    "\n",
    "train.to_csv('detection_train.csv', index=False)\n",
    "val.to_csv('detection_val.csv', index=False)\n",
    "test.to_csv('detection_test.csv', index=False)\n",
    "train_val.to_csv('detection_train_val.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline mit Dummy-Klassifikator: Coinflip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "train_df = pd.read_csv(\"detection_train.csv\")\n",
    "val_df = pd.read_csv(\"detection_val.csv\")\n",
    "test_df = pd.read_csv(\"detection_test.csv\")\n",
    "\n",
    "\n",
    "def apply_and_eval_dummy_on_val (used_features):\n",
    "    X_train, y_train = train_df[used_features], train_df[\"label\"]\n",
    "    X_val, y_val = val_df[used_features], val_df[\"label\"]\n",
    "    dummy = DummyClassifier(strategy=\"uniform\", random_state=1)\n",
    "    dummy.fit(X_train, y_train)\n",
    "    y_pred_val = dummy.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred_val)\n",
    "    f1 = f1_score(y_val, y_pred_val)\n",
    "    precision = precision_score(y_val, y_pred_val)\n",
    "    recall = recall_score(y_val, y_pred_val)\n",
    "    confusion = confusion_matrix(y_val, y_pred_val)\n",
    "    auc = roc_auc_score(y_val, y_pred_val)\n",
    "    \n",
    "    accuracy = \"{:.4f}\".format(accuracy)\n",
    "    f1 = \"{:.4f}\".format(f1)\n",
    "    precision = \"{:.4f}\".format(precision)\n",
    "    recall = \"{:.4f}\".format(recall)\n",
    "    auc = \"{:.4f}\".format(auc)\n",
    "    print(f\"VALIDATION: \\naccuarcy:\\t{accuracy}\\nPrecision:\\t{precision}\\nRecall:\\t\\t{recall}\\nAUC:\\t\\t{auc}\\nConfusion matrix:\\n{confusion}\")\n",
    "    #return accuracy, f1, precision, recall, confusion, auc\n",
    "\n",
    "def apply_and_eval_dummy_classificator_on_test(used_features):\n",
    "    X_train, y_train = train_df[used_features], train_df[\"label\"]\n",
    "    X_val, y_val = val_df[used_features], val_df[\"label\"]\n",
    "    X_test, y_test = test_df[used_features], test_df[\"label\"]\n",
    "\n",
    "    dummy = DummyClassifier(strategy=\"uniform\", random_state=1)\n",
    "    dummy.fit(X_train, y_train)\n",
    "    y_pred_val = dummy.predict(X_val)\n",
    "    y_pred_test = dummy.predict(X_test)\n",
    "    accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1 = f1_score(y_test, y_pred_test)\n",
    "    precision = precision_score(y_test, y_pred_test)\n",
    "    recall = recall_score(y_test, y_pred_test)\n",
    "    confusion = confusion_matrix(y_test, y_pred_test)\n",
    "    auc = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    accuracy_val = \"{:.4f}\".format(accuracy_val)\n",
    "    accuracy_test = \"{:.4f}\".format(accuracy_test)\n",
    "    f1 = \"{:.4f}\".format(f1)\n",
    "    precision = \"{:.4f}\".format(precision)\n",
    "    recall = \"{:.4f}\".format(recall)\n",
    "    auc = \"{:.4f}\".format(auc)\n",
    "    print(f\"acc_test:\\t{accuracy_test}\\nPrecision:\\t{precision}\\nRecall:\\t\\t{recall}\\nAUC:\\t\\t{auc}\\nConfusion matrix:\\n{confusion}\")\n",
    "    #return  accuracy_test, f1, precision, recall, confusion, auc\n",
    "\n",
    "\n",
    "used_features_img0 = train_df.filter(regex=\"^(feature)\").columns\n",
    "used_features_img2 = train_df.filter(regex=\"^(img_feature)\").columns\n",
    "used_features_tab = train_df.filter(regex=\"^(tab_)\").columns\n",
    "used_features_text = train_df.filter(regex=\"^(text_)\").columns\n",
    "used_features_all = train_df.filter(regex=\"^(feature|img_feature|tab_|text_)\").columns\n",
    "\n",
    "\n",
    "print(f\"____________________________Test: EVALUATION_DUMMY_CLASSIFICATOR: Image Features_________________________________________________\")\n",
    "apply_and_eval_dummy_classificator_on_test(used_features_img0)\n",
    "print(f\"____________________________Test:EVALUATION_DUMMY_CLASSIFICATOR: Image (Url/2) Features_________________________________________________\")\n",
    "apply_and_eval_dummy_classificator_on_test(used_features_img2)\n",
    "print(f\"____________________________Test: EVALUATION_DUMMY_CLASSIFICATOR: tab Features_________________________________________________\")\n",
    "apply_and_eval_dummy_classificator_on_test(used_features_tab)\n",
    "print(f\"____________________________Test: EVALUATION_DUMMY_CLASSIFICATOR: text Features_________________________________________________\")\n",
    "apply_and_eval_dummy_classificator_on_test(used_features_text)\n",
    "print(f\"____________________________Test: EVALUATION_DUMMY_CLASSIFICATOR: all Features_________________________________________________\")\n",
    "apply_and_eval_dummy_classificator_on_test(used_features_all)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_df = pd.read_csv(\"detection_train.csv\")\n",
    "val_df = pd.read_csv(\"detection_val.csv\")\n",
    "test_df = pd.read_csv(\"detection_test.csv\")\n",
    "\n",
    "def apply_and_eval_xgb_on_val(used_features, svg_file_suffix):\n",
    "    plt.clf()\n",
    "    X_train, y_train = train_df[used_features], train_df[\"label\"]\n",
    "    X_val, y_val = val_df[used_features], val_df[\"label\"]\n",
    "    \n",
    "    # Standardisierung\n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = scaler.fit_transform(X_train)\n",
    "    X_val_std = scaler.transform(X_val)\n",
    "\n",
    "    xgb = XGBClassifier(random_state=3, n_estimators=1000)\n",
    "    xgb.fit(X_train_std, y_train)\n",
    "    y_pred_val = xgb.predict(X_val_std)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_pred_val)\n",
    "    f1 = f1_score(y_val, y_pred_val)\n",
    "    precision = precision_score(y_val, y_pred_val)\n",
    "    recall = recall_score(y_val, y_pred_val)\n",
    "    confusion = confusion_matrix(y_val, y_pred_val)\n",
    "    auc = roc_auc_score(y_val, y_pred_val)\n",
    "    print(f\"VALIDATION SET: \\naccuracy:\\t{accuracy}\\nPrecision:\\t{precision}\\nRecall:\\t\\t{recall}\\nF1-Score:\\t{f1}\\nAUC:\\t\\t{auc}\\nConfusion matrix:\\n{confusion}\")\n",
    "\n",
    "    explainer = shap.Explainer(xgb, X_train_std, feature_names=X_train.columns)\n",
    "    shap_values = explainer(X_train_std)\n",
    "    shap.plots.beeswarm(shap_values, max_display=15,show=False) \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'shap_xbg_test_{svg_file_suffix}.svg', format='svg', dpi=1200)\n",
    "    #return  accuracy_test, f1, precision, recall, confusion, auc\n",
    "\n",
    "def apply_and_eval_xgb_on_test(used_features, svg_file_suffix):\n",
    "    plt.clf()\n",
    "    X_train, y_train = train_df[used_features], train_df[\"label\"]\n",
    "    X_val, y_val = val_df[used_features], val_df[\"label\"]\n",
    "    X_test, y_test = test_df[used_features], test_df[\"label\"]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = scaler.fit_transform(X_train)\n",
    "    X_val_std = scaler.transform(X_val)\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "\n",
    "    xgb = XGBClassifier(random_state=3, n_estimators=1000)\n",
    "    xgb.fit(X_train_std, y_train)\n",
    "    y_pred_val = xgb.predict(X_val_std)\n",
    "    y_pred_test = xgb.predict(X_test_std)\n",
    "\n",
    "    accuracy_test = float(\"{:.4f}\".format(accuracy_score(y_test, y_pred_test)))\n",
    "    precision = float(\"{:.4f}\".format(precision_score(y_test, y_pred_test)))\n",
    "    recall = float(\"{:.4f}\".format(recall_score(y_test, y_pred_test)))\n",
    "    f1 = float(\"{:.4f}\".format(f1_score(y_test, y_pred_test)))\n",
    "    auc = float(\"{:.4f}\".format(roc_auc_score(y_test, y_pred_test)))\n",
    "    confusion = confusion_matrix(y_test, y_pred_test)\n",
    "    print(f\"acc_test:\\t{accuracy_test}\\nPrecision:\\t{precision}\\nRecall:\\t\\t{recall}\\nF1-Score:\\t{f1}\\nAUC:\\t\\t{auc}\\nConfusion matrix:\\n{confusion}\")\n",
    "\n",
    "    explainer = shap.Explainer(xgb, X_train_std, feature_names=X_train.columns)\n",
    "    shap_values = explainer(X_train_std)\n",
    "    shap.plots.beeswarm(shap_values, max_display=15,show=False, ) \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'shap_xbg_test_{svg_file_suffix}.svg', format='svg', dpi=1200)\n",
    "    #return accuracy_test, f1, precision, recall, confusion, auc\n",
    "\n",
    "\n",
    "used_features_img0 = train_df.filter(regex=\"^(feature)\").columns\n",
    "used_features_img2 = train_df.filter(regex=\"^(img_feature)\").columns\n",
    "used_features_tab = train_df.filter(regex=\"^(tab_)\").columns\n",
    "used_features_text = train_df.filter(regex=\"^(text_)\").columns\n",
    "used_features_all = train_df.filter(regex=\"^(feature|tab_|text_)\").columns\n",
    "\n",
    "print(f\"____________________________XGB ON TEST - ImageUrls/0 Features_________________________________________________\")\n",
    "apply_and_eval_xgb_on_test(used_features_img0, \"img0\")\n",
    "print(f\"___________________________XGB ON TEST: ImageUrls/2 Features_________________________________________________\")\n",
    "apply_and_eval_xgb_on_test(used_features_img2, \"img2\")\n",
    "print(f\"____________________________XGB ON TEST: tab Features_________________________________________________\")\n",
    "apply_and_eval_xgb_on_test(used_features_tab, \"tab\")\n",
    "print(f\"____________________________XGB ON TEST: text Features_________________________________________________\")\n",
    "apply_and_eval_xgb_on_test(used_features_text, \"text\")\n",
    "print(f\"____________________________XGB ON TEST: all Features_________________________________________________\")\n",
    "apply_and_eval_xgb_on_test(used_features_all, \"all\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet - Neuronales Netz für Tabulare Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________TabNet ON TEST - ImageUrls/2 Features_________________________________________________\n",
      "Eventuell NAs entfernen:\n",
      "Train Shape mit na: (900, 512)\n",
      "Val Shape mit na: (226, 512)\n",
      "Train Shape ohne na: (900, 512)\n",
      "Val Shape ohne na: (226, 512)\n",
      "float64\n",
      "Used Features: ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'feature_79', 'feature_80', 'feature_81', 'feature_82', 'feature_83', 'feature_84', 'feature_85', 'feature_86', 'feature_87', 'feature_88', 'feature_89', 'feature_90', 'feature_91', 'feature_92', 'feature_93', 'feature_94', 'feature_95', 'feature_96', 'feature_97', 'feature_98', 'feature_99', 'feature_100', 'feature_101', 'feature_102', 'feature_103', 'feature_104', 'feature_105', 'feature_106', 'feature_107', 'feature_108', 'feature_109', 'feature_110', 'feature_111', 'feature_112', 'feature_113', 'feature_114', 'feature_115', 'feature_116', 'feature_117', 'feature_118', 'feature_119', 'feature_120', 'feature_121', 'feature_122', 'feature_123', 'feature_124', 'feature_125', 'feature_126', 'feature_127', 'feature_128', 'feature_129', 'feature_130', 'feature_131', 'feature_132', 'feature_133', 'feature_134', 'feature_135', 'feature_136', 'feature_137', 'feature_138', 'feature_139', 'feature_140', 'feature_141', 'feature_142', 'feature_143', 'feature_144', 'feature_145', 'feature_146', 'feature_147', 'feature_148', 'feature_149', 'feature_150', 'feature_151', 'feature_152', 'feature_153', 'feature_154', 'feature_155', 'feature_156', 'feature_157', 'feature_158', 'feature_159', 'feature_160', 'feature_161', 'feature_162', 'feature_163', 'feature_164', 'feature_165', 'feature_166', 'feature_167', 'feature_168', 'feature_169', 'feature_170', 'feature_171', 'feature_172', 'feature_173', 'feature_174', 'feature_175', 'feature_176', 'feature_177', 'feature_178', 'feature_179', 'feature_180', 'feature_181', 'feature_182', 'feature_183', 'feature_184', 'feature_185', 'feature_186', 'feature_187', 'feature_188', 'feature_189', 'feature_190', 'feature_191', 'feature_192', 'feature_193', 'feature_194', 'feature_195', 'feature_196', 'feature_197', 'feature_198', 'feature_199', 'feature_200', 'feature_201', 'feature_202', 'feature_203', 'feature_204', 'feature_205', 'feature_206', 'feature_207', 'feature_208', 'feature_209', 'feature_210', 'feature_211', 'feature_212', 'feature_213', 'feature_214', 'feature_215', 'feature_216', 'feature_217', 'feature_218', 'feature_219', 'feature_220', 'feature_221', 'feature_222', 'feature_223', 'feature_224', 'feature_225', 'feature_226', 'feature_227', 'feature_228', 'feature_229', 'feature_230', 'feature_231', 'feature_232', 'feature_233', 'feature_234', 'feature_235', 'feature_236', 'feature_237', 'feature_238', 'feature_239', 'feature_240', 'feature_241', 'feature_242', 'feature_243', 'feature_244', 'feature_245', 'feature_246', 'feature_247', 'feature_248', 'feature_249', 'feature_250', 'feature_251', 'feature_252', 'feature_253', 'feature_254', 'feature_255', 'feature_256', 'feature_257', 'feature_258', 'feature_259', 'feature_260', 'feature_261', 'feature_262', 'feature_263', 'feature_264', 'feature_265', 'feature_266', 'feature_267', 'feature_268', 'feature_269', 'feature_270', 'feature_271', 'feature_272', 'feature_273', 'feature_274', 'feature_275', 'feature_276', 'feature_277', 'feature_278', 'feature_279', 'feature_280', 'feature_281', 'feature_282', 'feature_283', 'feature_284', 'feature_285', 'feature_286', 'feature_287', 'feature_288', 'feature_289', 'feature_290', 'feature_291', 'feature_292', 'feature_293', 'feature_294', 'feature_295', 'feature_296', 'feature_297', 'feature_298', 'feature_299', 'feature_300', 'feature_301', 'feature_302', 'feature_303', 'feature_304', 'feature_305', 'feature_306', 'feature_307', 'feature_308', 'feature_309', 'feature_310', 'feature_311', 'feature_312', 'feature_313', 'feature_314', 'feature_315', 'feature_316', 'feature_317', 'feature_318', 'feature_319', 'feature_320', 'feature_321', 'feature_322', 'feature_323', 'feature_324', 'feature_325', 'feature_326', 'feature_327', 'feature_328', 'feature_329', 'feature_330', 'feature_331', 'feature_332', 'feature_333', 'feature_334', 'feature_335', 'feature_336', 'feature_337', 'feature_338', 'feature_339', 'feature_340', 'feature_341', 'feature_342', 'feature_343', 'feature_344', 'feature_345', 'feature_346', 'feature_347', 'feature_348', 'feature_349', 'feature_350', 'feature_351', 'feature_352', 'feature_353', 'feature_354', 'feature_355', 'feature_356', 'feature_357', 'feature_358', 'feature_359', 'feature_360', 'feature_361', 'feature_362', 'feature_363', 'feature_364', 'feature_365', 'feature_366', 'feature_367', 'feature_368', 'feature_369', 'feature_370', 'feature_371', 'feature_372', 'feature_373', 'feature_374', 'feature_375', 'feature_376', 'feature_377', 'feature_378', 'feature_379', 'feature_380', 'feature_381', 'feature_382', 'feature_383', 'feature_384', 'feature_385', 'feature_386', 'feature_387', 'feature_388', 'feature_389', 'feature_390', 'feature_391', 'feature_392', 'feature_393', 'feature_394', 'feature_395', 'feature_396', 'feature_397', 'feature_398', 'feature_399', 'feature_400', 'feature_401', 'feature_402', 'feature_403', 'feature_404', 'feature_405', 'feature_406', 'feature_407', 'feature_408', 'feature_409', 'feature_410', 'feature_411', 'feature_412', 'feature_413', 'feature_414', 'feature_415', 'feature_416', 'feature_417', 'feature_418', 'feature_419', 'feature_420', 'feature_421', 'feature_422', 'feature_423', 'feature_424', 'feature_425', 'feature_426', 'feature_427', 'feature_428', 'feature_429', 'feature_430', 'feature_431', 'feature_432', 'feature_433', 'feature_434', 'feature_435', 'feature_436', 'feature_437', 'feature_438', 'feature_439', 'feature_440', 'feature_441', 'feature_442', 'feature_443', 'feature_444', 'feature_445', 'feature_446', 'feature_447', 'feature_448', 'feature_449', 'feature_450', 'feature_451', 'feature_452', 'feature_453', 'feature_454', 'feature_455', 'feature_456', 'feature_457', 'feature_458', 'feature_459', 'feature_460', 'feature_461', 'feature_462', 'feature_463', 'feature_464', 'feature_465', 'feature_466', 'feature_467', 'feature_468', 'feature_469', 'feature_470', 'feature_471', 'feature_472', 'feature_473', 'feature_474', 'feature_475', 'feature_476', 'feature_477', 'feature_478', 'feature_479', 'feature_480', 'feature_481', 'feature_482', 'feature_483', 'feature_484', 'feature_485', 'feature_486', 'feature_487', 'feature_488', 'feature_489', 'feature_490', 'feature_491', 'feature_492', 'feature_493', 'feature_494', 'feature_495', 'feature_496', 'feature_497', 'feature_498', 'feature_499', 'feature_500', 'feature_501', 'feature_502', 'feature_503', 'feature_504', 'feature_505', 'feature_506', 'feature_507', 'feature_508', 'feature_509', 'feature_510', 'feature_511']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michi\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.72832 | val_0_accuracy: 0.49115 | val_0_auc: 0.43034 |  0:00:00s\n",
      "epoch 1  | loss: 0.69341 | val_0_accuracy: 0.59292 | val_0_auc: 0.58423 |  0:00:01s\n",
      "epoch 2  | loss: 0.68031 | val_0_accuracy: 0.5885  | val_0_auc: 0.63082 |  0:00:02s\n",
      "epoch 3  | loss: 0.66449 | val_0_accuracy: 0.68584 | val_0_auc: 0.72919 |  0:00:02s\n",
      "epoch 4  | loss: 0.59384 | val_0_accuracy: 0.69027 | val_0_auc: 0.75973 |  0:00:03s\n",
      "epoch 5  | loss: 0.5744  | val_0_accuracy: 0.76549 | val_0_auc: 0.80343 |  0:00:04s\n",
      "epoch 6  | loss: 0.52573 | val_0_accuracy: 0.78761 | val_0_auc: 0.84697 |  0:00:04s\n",
      "epoch 7  | loss: 0.49825 | val_0_accuracy: 0.83186 | val_0_auc: 0.88613 |  0:00:05s\n",
      "epoch 8  | loss: 0.43081 | val_0_accuracy: 0.84071 | val_0_auc: 0.88824 |  0:00:06s\n",
      "epoch 9  | loss: 0.40337 | val_0_accuracy: 0.84071 | val_0_auc: 0.92584 |  0:00:06s\n",
      "epoch 10 | loss: 0.32596 | val_0_accuracy: 0.86726 | val_0_auc: 0.94017 |  0:00:07s\n",
      "epoch 11 | loss: 0.30151 | val_0_accuracy: 0.88938 | val_0_auc: 0.93492 |  0:00:08s\n",
      "epoch 12 | loss: 0.26198 | val_0_accuracy: 0.88053 | val_0_auc: 0.94643 |  0:00:09s\n",
      "epoch 13 | loss: 0.2132  | val_0_accuracy: 0.90265 | val_0_auc: 0.95246 |  0:00:09s\n",
      "epoch 14 | loss: 0.22011 | val_0_accuracy: 0.90708 | val_0_auc: 0.95638 |  0:00:10s\n",
      "epoch 15 | loss: 0.17283 | val_0_accuracy: 0.92035 | val_0_auc: 0.96437 |  0:00:11s\n",
      "epoch 16 | loss: 0.15064 | val_0_accuracy: 0.92478 | val_0_auc: 0.96719 |  0:00:12s\n",
      "epoch 17 | loss: 0.2853  | val_0_accuracy: 0.92478 | val_0_auc: 0.9664  |  0:00:13s\n",
      "epoch 18 | loss: 0.17706 | val_0_accuracy: 0.92478 | val_0_auc: 0.97611 |  0:00:14s\n",
      "epoch 19 | loss: 0.17987 | val_0_accuracy: 0.92478 | val_0_auc: 0.9834  |  0:00:15s\n",
      "epoch 20 | loss: 0.17645 | val_0_accuracy: 0.90265 | val_0_auc: 0.97549 |  0:00:16s\n",
      "epoch 21 | loss: 0.1302  | val_0_accuracy: 0.89823 | val_0_auc: 0.96492 |  0:00:17s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 19 and best_val_0_auc = 0.9834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michi\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'X_test_std' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 76\u001b[0m\n\u001b[0;32m     71\u001b[0m used_features_all \u001b[39m=\u001b[39m train_df\u001b[39m.\u001b[39mfilter(regex\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m^(feature|tab_|text_)\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m     73\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[0;32m     74\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m____________________________TabNet ON TEST - ImageUrls/2 Features_________________________________________________\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m )\n\u001b[1;32m---> 76\u001b[0m train_and_tune_tabNet(used_features_img0, \u001b[39m\"\u001b[39;49m\u001b[39mimg0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m, in \u001b[0;36mtrain_and_tune_tabNet\u001b[1;34m(used_features, svg_file_suffix)\u001b[0m\n\u001b[0;32m     30\u001b[0m tabNet\u001b[39m.\u001b[39mfit(\n\u001b[0;32m     31\u001b[0m     X_train\u001b[39m=\u001b[39mX_train_std,\n\u001b[0;32m     32\u001b[0m     y_train\u001b[39m=\u001b[39my_train,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     40\u001b[0m X_test, y_test \u001b[39m=\u001b[39m test_df[used_features], test_df[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 41\u001b[0m X_test_std \u001b[39m=\u001b[39m X_test_std\u001b[39m.\u001b[39mdropna()\n\u001b[0;32m     42\u001b[0m X_test_std \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(X_test)\n\u001b[0;32m     43\u001b[0m y_test \u001b[39m=\u001b[39m y_test\u001b[39m.\u001b[39mto_numpy()\u001b[39m.\u001b[39msqueeze()\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'X_test_std' referenced before assignment"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_df = pd.read_csv(\"detection_train.csv\")\n",
    "val_df = pd.read_csv(\"detection_val.csv\")\n",
    "test_df = pd.read_csv(\"detection_test.csv\")\n",
    "\n",
    "def train_and_tune_tabNet(used_features, svg_file_suffix):       \n",
    "    plt.clf()\n",
    "    X_train_org, y_train = train_df[used_features], train_df[\"label\"]\n",
    "    X_val_org, y_val = val_df[used_features], val_df[\"label\"]\n",
    "\n",
    "    print(f\"Eventuell NAs entfernen:\")\n",
    "    print(f\"Train Shape mit na: {X_train_org.shape}\")\n",
    "    print(f\"Val Shape mit na: {X_val_org.shape}\")\n",
    "    X_train = X_train_org.dropna()\n",
    "    X_val = X_val_org.dropna()\n",
    "    print(f\"Train Shape ohne na: {X_train.shape}\")\n",
    "    print(f\"Val Shape ohne na: {X_val.shape}\")\n",
    "\n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = scaler.fit_transform(X_train)\n",
    "    X_val_std = scaler.transform(X_val)\n",
    " \n",
    "    print(X_train_std.dtype)\n",
    "    print(f\"Used Features: {used_features.to_list()}\")\n",
    "\n",
    "    tabNet = TabNetClassifier(verbose=1, seed=42)\n",
    "    tabNet.fit(\n",
    "        X_train=X_train_std,\n",
    "        y_train=y_train,\n",
    "        patience=2,\n",
    "        max_epochs=50,\n",
    "        eval_set=[(X_val_std, y_val)],\n",
    "        eval_metric=[\"accuracy\", \"auc\"],\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    X_test, y_test = test_df[used_features], test_df[\"label\"]\n",
    "    X_test_std = X_test_std.dropna()\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "    y_test = y_test.to_numpy().squeeze()\n",
    "    y_pred_test = tabNet.predict(X_test_std)\n",
    "    y_pred_test_proba = tabNet.predict_proba(X_test_std)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1 = f1_score(y_test, y_pred_test)\n",
    "    precision = precision_score(y_test, y_pred_test)\n",
    "    recall = recall_score(y_test, y_pred_test)\n",
    "    confusion = confusion_matrix(y_test, y_pred_test)\n",
    "    auc = roc_auc_score(y_test, y_pred_test)\n",
    "    print(\n",
    "        f\"acc_test:\\t{accuracy_test}\\nPrecision:\\t{precision}\\nRecall:\\t\\t{recall}\\nF1-Score:\\t{f1}\\nAUC:\\t\\t{auc}\\nConfusion matrix:\\n{confusion}\"\n",
    "    )\n",
    "\n",
    "    # Aufgrund langer Laufzeiten abgebrochen\n",
    "\n",
    "    # def tabnet_predict(input_data):\n",
    "    #     input_data = torch.tensor(input_data).float()\n",
    "    #     return tabNet.predict_proba(input_data)[:,1]\n",
    "    # explainer = shap.Explainer(tabnet_predict, X_train)\n",
    "    # shap_values = explainer(X_train, max_evals=1500)\n",
    "    # shap.plots.beeswarm(shap_values, max_display=15, show=False) \n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f'shap_rf_test_{svg_file_suffix}.svg', format='svg', dpi=1200)\n",
    "\n",
    "used_features_img0 = train_df.filter(regex=\"^(feature)\").columns\n",
    "used_features_img2 = train_df.filter(regex=\"^(img_feature)\").columns\n",
    "used_features_tab = train_df.filter(regex=\"^(tab_)\").columns\n",
    "used_features_text = train_df.filter(regex=\"^(text_)\").columns\n",
    "used_features_all = train_df.filter(regex=\"^(feature|tab_|text_)\").columns\n",
    "\n",
    "print(\n",
    "    f\"____________________________TabNet ON TEST - ImageUrls/2 Features_________________________________________________\"\n",
    ")\n",
    "train_and_tune_tabNet(used_features_img0, \"img0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"detection_train.csv\")\n",
    "val_df = pd.read_csv(\"detection_val.csv\")\n",
    "test_df = pd.read_csv(\"detection_test.csv\")\n",
    "\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_df = pd.read_csv(\"detection_train.csv\")\n",
    "val_df = pd.read_csv(\"detection_val.csv\")\n",
    "test_df = pd.read_csv(\"detection_test.csv\")\n",
    "\n",
    "def apply_and_eval_rf_on_val(used_features, svg_file_suffix):\n",
    "    plt.clf()\n",
    "    X_train, y_train = train_df[used_features], train_df[\"label\"]\n",
    "    X_val, y_val = val_df[used_features], val_df[\"label\"]\n",
    "\n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = scaler.fit_transform(X_train)\n",
    "    X_val_std = scaler.transform(X_val)\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=3, n_estimators=1000)\n",
    "    rf.fit(X_train_std, y_train)\n",
    "    y_pred_val = rf.predict(X_val_std)\n",
    "\n",
    "    accuracy =  \"{:.4f}\".format(accuracy_score(y_val, y_pred_val))\n",
    "    precision = \"{:.4f}\".format(precision_score(y_val, y_pred_val))\n",
    "    recall = \"{:.4f}\".format(recall_score(y_val, y_pred_val))\n",
    "    f1 = \"{:.4f}\".format(f1_score(y_val, y_pred_val))\n",
    "    auc = \"{:.4f}\".format(roc_auc_score(y_val, y_pred_val))\n",
    "    confusion = confusion_matrix(y_val, y_pred_val)\n",
    "    \n",
    "    print(f\"VALIDATION SET: \\naccuracy:\\t{accuracy}\\nPrecision:\\t{precision}\\nRecall:\\t\\t{recall}\\nF1-Score:\\t{f1}\\nAUC:\\t\\t{auc}\\nConfusion matrix:\\n{confusion}\")\n",
    "\n",
    "    explainer = shap.Explainer(rf, X_train_std, feature_names=X_train.columns)\n",
    "    shap_values = explainer(X_train_std)\n",
    "    shap.plots.beeswarm(shap_values, max_display=15, show=False) \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'shap_rf_test_{svg_file_suffix}.svg', format='svg', dpi=1200)\n",
    "    #return  accuracy_test, f1, precision, recall, confusion, auc\n",
    "\n",
    "\n",
    "def apply_and_eval_rf_on_test(used_features, svg_file_suffix):\n",
    "    plt.clf()\n",
    "    X_train, y_train = train_df[used_features], train_df[\"label\"]\n",
    "    X_val, y_val = val_df[used_features], val_df[\"label\"]\n",
    "    X_test, y_test = test_df[used_features], test_df[\"label\"]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = scaler.fit_transform(X_train)\n",
    "    X_val_std = scaler.transform(X_val)\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=3, n_estimators=1000)\n",
    "    rf.fit(X_train_std, y_train)\n",
    "    y_pred_val = rf.predict(X_val_std)\n",
    "    y_pred_test = rf.predict(X_test_std)\n",
    "\n",
    "    accuracy_test =  \"{:.4f}\".format(accuracy_score(y_test, y_pred_test))\n",
    "    precision = \"{:.4f}\".format(precision_score(y_test, y_pred_test))\n",
    "    recall = \"{:.4f}\".format(recall_score(y_test, y_pred_test))\n",
    "    f1 = \"{:.4f}\".format(f1_score(y_test, y_pred_test))\n",
    "    auc = \"{:.4f}\".format(roc_auc_score(y_test, y_pred_test))\n",
    "    confusion = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    print(f\"acc_test:\\t{accuracy_test}\\nPrecision:\\t{precision}\\nRecall:\\t\\t{recall}\\nF1-Score:\\t{f1}\\nAUC:\\t\\t{auc}\\nConfusion matrix:\\n{confusion}\")\n",
    "\n",
    "    explainer = shap.Explainer(rf, X_train_std, feature_names=X_train.columns)\n",
    "    shap_values = explainer(X_train_std)\n",
    "    shap.plots.beeswarm(shap_values, max_display=15, show=False) \n",
    "    shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'shap_rf_test_{svg_file_suffix}.svg', format='svg', dpi=1200)\n",
    "\n",
    "\n",
    "used_features_img0 = train_df.filter(regex=\"^(feature)\").columns\n",
    "used_features_img2 = train_df.filter(regex=\"^(img_feature)\").columns\n",
    "used_features_tab = train_df.filter(regex=\"^(tab_)\").columns\n",
    "used_features_text = train_df.filter(regex=\"^(text_)\").columns\n",
    "used_features_all = train_df.filter(regex=\"^(feature|img_feature|tab_|text_)\").columns\n",
    "\n",
    "print(f\"____________________________RF ON TEST - ImageUrls/0 Features_________________________________________________\")\n",
    "apply_and_eval_rf_on_test(used_features_img0, \"img0\")\n",
    "print(f\"___________________________RF ON TEST: ImageUrls/2 Features_________________________________________________\")\n",
    "apply_and_eval_rf_on_test(used_features_img2, \"img2\")\n",
    "print(f\"____________________________RF ON TEST: tab Features_________________________________________________\")\n",
    "apply_and_eval_rf_on_test(used_features_tab, \"tab\")\n",
    "print(f\"____________________________RF ON TEST: text Features_________________________________________________\")\n",
    "apply_and_eval_rf_on_test(used_features_text, \"text\")\n",
    "print(f\"____________________________RF ON TEST: all Features_________________________________________________\")\n",
    "apply_and_eval_rf_on_test(used_features_all, \"all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
