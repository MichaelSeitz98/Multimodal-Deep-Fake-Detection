{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai \n",
    "from keybert import KeyBERT\n",
    "\n",
    "model = KeyBERT('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook wird aus dem Dataset base_keywords_sentimement mit entsprechenden multimodalen Deep-Fakes erg√§nzt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759, 47)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('base_keywords_sentiment_reduced.csv')\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text:  GPT 3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gpt_api.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "class chatGPT: \n",
    "    def __init__(self, api_key, rolle):\n",
    "        openai.api_key = api_key\n",
    "        self.dialog = [{\"role\":\"system\", \"content\":rolle}]\n",
    "\n",
    "    def question(self, question):\n",
    "        self.dialog.append({\"role\":\"user\", \"content\":question})\n",
    "        ergebnis = openai.ChatCompletion.create(model='gpt-3.5-turbo', messages=self.dialog)\n",
    "        antwort = ergebnis[\"choices\"][0].message.content\n",
    "        self.dialog.append({\"role\":\"assistant\", \"content\":antwort})\n",
    "        return antwort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"In stlye of a google user that writes a review about a place. Not too formal language.\"\n",
    "prompt = \"Write a 4 out of 5 review about the Italien Restaurant. \"\n",
    "chatGPT(api_key, role).question(prompt)\n",
    "print(chatGPT(api_key, role).question(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(text):\n",
    "    keywords = model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5)\n",
    "    keywords = [keyword[0] for keyword in keywords]\n",
    "    return keywords\n",
    "\n",
    "output_string= \"I recently visited the Italian Restaurant and overall had a great experience. The atmosphere was cozy and inviting with a classic Italian vibe. The staff was friendly and attentive, making sure we had everything we needed throughout our meal.The menu had a great selection of traditional Italian dishes and some unique options as well. I tried the gnocchi and it was delicious, perfectly cooked with a flavorful sauce. My friend had the lasagna and said it was some of the best she's ever had. The only downside was the wait time for our table, as we had to wait about 30 minutes even though we had reservations. However, the food and service made up for it. Overall, I would recommend the Italian Restaurant to anyone who loves Italian cuisine and wants to have a nice, cozy dinner out with friends or family.\"\n",
    "keywords = get_keywords(output_string)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image: Dall-E-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"Photo of a nice experience at Bowling alley, photography, high quality, high resolution, 4k\"\n",
    "\n",
    "with open(\"dalleapi.txt\", \"r\") as f:\n",
    "    openai.api_key = f.read()\n",
    "\n",
    "response = openai.Image.create(\n",
    "    prompt=PROMPT,\n",
    "    n=2,\n",
    "    size=\"256x256\",\n",
    ")\n",
    "\n",
    "print(response[\"data\"][0][\"url\"])\n",
    "\n",
    "with open(\"02_Visualizations\\dalle-2-tries.txt\", \"a\") as f:\n",
    "    f.write(\"\\nPROMPT \" + PROMPT +\": \" + response[\"data\"][0][\"url\"] + \" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tries with StableDiffusion aka Dreamstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from stability_sdk import client\n",
    "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
    "\n",
    "os.environ['STABILITY_HOST'] = 'grpc.stability.ai:443'\n",
    "\n",
    "with open('dreamstudio.txt', 'r') as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "os.environ['STABILITY_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_api = client.StabilityInference(\n",
    "    key=os.environ['STABILITY_KEY'], # API Key reference.\n",
    "    verbose=True, # Print debug messages.\n",
    "    engine=\"stable-diffusion-512-v2-1\", # Set the engine to use for generation.\n",
    "    # Available engines: stable-diffusion-v1 stable-diffusion-v1-5 stable-diffusion-512-v2-0 stable-diffusion-768-v2-0\n",
    "    # stable-diffusion-512-v2-1 stable-diffusion-768-v2-1 stable-inpainting-v1-0 stable-inpainting-512-v2-0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our initial generation parameters.\n",
    "answers = stability_api.generate(\n",
    "    prompt=\"Bad experience at italien restaurant,high quality photography, 4k, close-up\",\n",
    "    #seed=992446758, # If a seed is provided, the resulting generated image will be deterministic.\n",
    "                    # What this means is that as long as all generation parameters remain the same, you can always recall the same image simply by generating it again.\n",
    "                    # Note: This isn't quite the case for Clip Guided generations, which we'll tackle in a future example notebook.\n",
    "    steps=30, # Amount of inference steps performed on image generation. Defaults to 30.\n",
    "    cfg_scale=7.0, # Influences how strongly your generation is guided to match your prompt.\n",
    "                   # Setting this value higher increases the strength in which it tries to match your prompt.\n",
    "                   # Defaults to 7.0 if not specified.\n",
    "    width=512, # Generation width, defaults to 512 if not included.\n",
    "    height=512, # Generation height, defaults to 512 if not included.\n",
    "    samples=2, # Number of images to generate, defaults to 1 if not included.\n",
    "    sampler=generation.SAMPLER_K_DPMPP_2M # Choose which sampler we want to denoise our generation with.\n",
    "                                                 # Defaults to k_dpmpp_2m if not specified. Clip Guidance only supports ancestral samplers.\n",
    "                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m)\n",
    ")\n",
    "\n",
    "# Set up our warning to print to the console if the adult content classifier is tripped.\n",
    "# If adult content classifier is not tripped, save generated images.\n",
    "for resp in answers:\n",
    "    for artifact in resp.artifacts:\n",
    "        if artifact.finish_reason == generation.FILTER:\n",
    "            warnings.warn(\n",
    "                \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                \"Please modify the prompt and try again.\")\n",
    "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "            img = Image.open(io.BytesIO(artifact.binary))\n",
    "            img.save(str(artifact.seed)+ \".png\") # Save our generated images with their seed number as the filename."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
