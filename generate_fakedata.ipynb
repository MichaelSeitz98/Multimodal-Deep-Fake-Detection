{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai \n",
    "from keybert import KeyBERT\n",
    "import langdetect\n",
    "\n",
    "# model = KeyBERT('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"apikey_openai.txt\", \"r\") as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "print(api_key)\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text:  GPT 3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chatGPT: \n",
    "    def __init__(self, api_key, rolle):\n",
    "        openai.api_key = api_key\n",
    "        self.dialog = [{\"role\":\"system\", \"content\":rolle}]\n",
    "\n",
    "    def question(self, question):\n",
    "        self.dialog.append({\"role\":\"user\", \"content\":question})\n",
    "        ergebnis = openai.ChatCompletion.create(model='gpt-3.5-turbo', messages=self.dialog)\n",
    "        antwort = ergebnis[\"choices\"][0].message.content\n",
    "        self.dialog.append({\"role\":\"assistant\", \"content\":antwort})\n",
    "        return antwort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"In stlye of a google user that writes a review about a place. Not too formal language.\"\n",
    "prompt = \"Write a 4 out of 5 review about the Italien Restaurant. \"\n",
    "chatGPT(api_key, role).question(prompt)\n",
    "print(chatGPT(api_key, role).question(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(text):\n",
    "    keywords = model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5)\n",
    "    keywords = [keyword[0] for keyword in keywords]\n",
    "    return keywords\n",
    "\n",
    "output_string= \"I recently visited the Italian Restaurant and overall had a great experience. The atmosphere was cozy and inviting with a classic Italian vibe. The staff was friendly and attentive, making sure we had everything we needed throughout our meal.The menu had a great selection of traditional Italian dishes and some unique options as well. I tried the gnocchi and it was delicious, perfectly cooked with a flavorful sauce. My friend had the lasagna and said it was some of the best she's ever had. The only downside was the wait time for our table, as we had to wait about 30 minutes even though we had reservations. However, the food and service made up for it. Overall, I would recommend the Italian Restaurant to anyone who loves Italian cuisine and wants to have a nice, cozy dinner out with friends or family.\"\n",
    "keywords = get_keywords(output_string)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text: Finetuned GPT3 Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_list = openai.FineTune.list()\n",
    "fine_tuned_model = fine_tune_list['data'][1].fine_tuned_model\n",
    "print(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = \"A Google Maps review about a indish restaurant. ###\"\n",
    "answer = openai.Completion.create(\n",
    "  model=fine_tuned_model,\n",
    "  prompt=new_prompt,\n",
    "  max_tokens=100, # Change amount of tokens for longer completion\n",
    "  temperature=1, \n",
    "  stop = \"END\"\n",
    ")\n",
    "print(answer['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt3_finetuned_v1(row):\n",
    "    prompt = \"A Google Maps review about a \" + row['categoryName'] + \". ###\"\n",
    "    print(prompt)\n",
    "    answer = openai.Completion.create(\n",
    "      model=fine_tuned_model,\n",
    "      prompt=prompt,\n",
    "      max_tokens=150, # Change amount of tokens for longer completion\n",
    "      temperature=1, \n",
    "      stop = \"END\"\n",
    "    )\n",
    "    return answer['choices'][0]['text']\n",
    "\n",
    "def gpt3_5_turbo(row):\n",
    "    prompt = \"Write a Google Maps review about a \" + row['categoryName'] + \". In Style of a Google user that writes a review about a place.\"\n",
    "    role = \"In stlye of a google user that writes a review about a place.\"\n",
    "    prompt = \"Write a Google Maps review about a \" + row['categoryName']+\".\"\n",
    "    print(prompt)\n",
    "    return chatGPT(api_key, role).question(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['gpt3_finetuned_v1'] = df_test.apply(gpt3_finetuned_v1, axis=1)\n",
    "df_test.to_csv('test_df_showcase20gpt.csv')\n",
    "print(\"Done with gpt3_finetuned_v1 text generation.\")\n",
    "\n",
    "# ##df_test['gpt3.5_turbo'] = df_test.apply(gpt3_5_turbo, axis=1)\n",
    "# df_test.to_csv('test_df_showcase.csv')\n",
    "# print(\"Done with gpt3.5_turbo text generation.\")\n",
    "\n",
    "# print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the rows via a loop \n",
    "\n",
    "df_test = pd.read_csv('test_df_showcase20gpt.csv')\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    print(row['title'])\n",
    "    print(row['categoryName'])\n",
    "    print(\"ORG:\" + row['original_text'])\n",
    "    print(\"GPT:\" + row['gpt3_finetuned_v1'])\n",
    "    print(\"__________________________________________________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\michi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# Calculate BLEU score for GPT-3\u001b[39;00m\n\u001b[0;32m     13\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mgpt3_finetuned_v1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mgpt3_finetuned_v1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m gpt3_scores \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mgpt3_finetuned_v1\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(nltk\u001b[39m.\u001b[39;49mword_tokenize)\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: nltk\u001b[39m.\u001b[39;49mtranslate\u001b[39m.\u001b[39;49mbleu_score\u001b[39m.\u001b[39;49msentence_bleu(ref_sents, x))\n\u001b[0;32m     16\u001b[0m \u001b[39m# Print the average BLEU score for each model\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAverage BLEU score for GPT-3:\u001b[39m\u001b[39m\"\u001b[39m, gpt3_scores\u001b[39m.\u001b[39mmean())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py:4138\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4136\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   4137\u001b[0m         values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 4138\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(values, f, convert\u001b[39m=\u001b[39;49mconvert_dtype)\n\u001b[0;32m   4140\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], Series):\n\u001b[0;32m   4141\u001b[0m     \u001b[39m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[0;32m   4142\u001b[0m     \u001b[39m# so extension arrays can be used\u001b[39;00m\n\u001b[0;32m   4143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mpandas\\_libs\\lib.pyx:2467\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[40], line 14\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# Calculate BLEU score for GPT-3\u001b[39;00m\n\u001b[0;32m     13\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mgpt3_finetuned_v1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mgpt3_finetuned_v1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m gpt3_scores \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mgpt3_finetuned_v1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(nltk\u001b[39m.\u001b[39mword_tokenize)\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: nltk\u001b[39m.\u001b[39;49mtranslate\u001b[39m.\u001b[39;49mbleu_score\u001b[39m.\u001b[39;49msentence_bleu(ref_sents, x))\n\u001b[0;32m     16\u001b[0m \u001b[39m# Print the average BLEU score for each model\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAverage BLEU score for GPT-3:\u001b[39m\u001b[39m\"\u001b[39m, gpt3_scores\u001b[39m.\u001b[39mmean())\n",
      "File \u001b[1;32mc:\\Users\\michi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\translate\\bleu_score.py:107\u001b[0m, in \u001b[0;36msentence_bleu\u001b[1;34m(references, hypothesis, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentence_bleu\u001b[39m(\n\u001b[0;32m     21\u001b[0m     references,\n\u001b[0;32m     22\u001b[0m     hypothesis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     auto_reweigh\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     26\u001b[0m ):\n\u001b[0;32m     27\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m    Calculate BLEU score (Bilingual Evaluation Understudy) from\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m    Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39m    :rtype: float / list(float)\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m corpus_bleu(\n\u001b[0;32m    108\u001b[0m         [references], [hypothesis], weights, smoothing_function, auto_reweigh\n\u001b[0;32m    109\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\michi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\translate\\bleu_score.py:210\u001b[0m, in \u001b[0;36mcorpus_bleu\u001b[1;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mfor\u001b[39;00m references, hypothesis \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(list_of_references, hypotheses):\n\u001b[0;32m    207\u001b[0m     \u001b[39m# For each order of ngram, calculate the numerator and\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[39m# denominator for the corpus-level modified precision.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_weight_length \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m--> 210\u001b[0m         p_i \u001b[39m=\u001b[39m modified_precision(references, hypothesis, i)\n\u001b[0;32m    211\u001b[0m         p_numerators[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m p_i\u001b[39m.\u001b[39mnumerator\n\u001b[0;32m    212\u001b[0m         p_denominators[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m p_i\u001b[39m.\u001b[39mdenominator\n",
      "File \u001b[1;32mc:\\Users\\michi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\translate\\bleu_score.py:353\u001b[0m, in \u001b[0;36mmodified_precision\u001b[1;34m(references, hypothesis, n)\u001b[0m\n\u001b[0;32m    350\u001b[0m max_counts \u001b[39m=\u001b[39m {}\n\u001b[0;32m    351\u001b[0m \u001b[39mfor\u001b[39;00m reference \u001b[39min\u001b[39;00m references:\n\u001b[0;32m    352\u001b[0m     reference_counts \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 353\u001b[0m         Counter(ngrams(reference, n)) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(reference) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m n \u001b[39melse\u001b[39;00m Counter()\n\u001b[0;32m    354\u001b[0m     )\n\u001b[0;32m    355\u001b[0m     \u001b[39mfor\u001b[39;00m ngram \u001b[39min\u001b[39;00m counts:\n\u001b[0;32m    356\u001b[0m         max_counts[ngram] \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(max_counts\u001b[39m.\u001b[39mget(ngram, \u001b[39m0\u001b[39m), reference_counts[ngram])\n",
      "File \u001b[1;32mc:\\Users\\michi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\collections\\__init__.py:593\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[39mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \u001b[39mof elements to their counts.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    590\u001b[0m \n\u001b[0;32m    591\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m--> 593\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(iterable, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\michi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\collections\\__init__.py:679\u001b[0m, in \u001b[0;36mCounter.update\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    677\u001b[0m             \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mupdate(iterable)\n\u001b[0;32m    678\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 679\u001b[0m         _count_elements(\u001b[39mself\u001b[39;49m, iterable)\n\u001b[0;32m    680\u001b[0m \u001b[39mif\u001b[39;00m kwds:\n\u001b[0;32m    681\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(kwds)\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "## calcluate BLEU SCore for COmparison: \n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load your DataFrame\n",
    "df = pd.read_csv('test_df_showcase.csv')\n",
    "\n",
    "# Define your reference sentences\n",
    "ref_sents = [df['original_text'].apply(nltk.word_tokenize).tolist()]\n",
    "\n",
    "# Calculate BLEU score for GPT-3\n",
    "df['gpt3_finetuned_v1'] = df['gpt3_finetuned_v1'].astype(str)\n",
    "gpt3_scores = df['gpt3_finetuned_v1'].apply(nltk.word_tokenize).apply(lambda x: nltk.translate.bleu_score.sentence_bleu(ref_sents, x))\n",
    "\n",
    "# Print the average BLEU score for each model\n",
    "print(\"Average BLEU score for GPT-3:\", gpt3_scores.mean())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image: Dall-E-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"Photo of a nice experience at Bowling alley, photography, high quality, high resolution, 4k\"\n",
    "\n",
    "with open(\"dalleapi.txt\", \"r\") as f:\n",
    "    openai.api_key = f.read()\n",
    "\n",
    "response = openai.Image.create(\n",
    "    prompt=PROMPT,\n",
    "    n=2,\n",
    "    size=\"256x256\",\n",
    ")\n",
    "\n",
    "print(response[\"data\"][0][\"url\"])\n",
    "\n",
    "with open(\"02_Visualizations\\dalle-2-tries.txt\", \"a\") as f:\n",
    "    f.write(\"\\nPROMPT \" + PROMPT +\": \" + response[\"data\"][0][\"url\"] + \" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image: Tries with StableDiffusion aka Dreamstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from stability_sdk import client\n",
    "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
    "\n",
    "os.environ['STABILITY_HOST'] = 'grpc.stability.ai:443'\n",
    "\n",
    "with open('dreamstudio.txt', 'r') as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "os.environ['STABILITY_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_api = client.StabilityInference(\n",
    "    key=os.environ['STABILITY_KEY'], # API Key reference.\n",
    "    verbose=True, # Print debug messages.\n",
    "    engine=\"stable-diffusion-512-v2-1\", # Set the engine to use for generation.\n",
    "    # Available engines: stable-diffusion-v1 stable-diffusion-v1-5 stable-diffusion-512-v2-0 stable-diffusion-768-v2-0\n",
    "    # stable-diffusion-512-v2-1 stable-diffusion-768-v2-1 stable-inpainting-v1-0 stable-inpainting-512-v2-0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our initial generation parameters.\n",
    "answers = stability_api.generate(\n",
    "    prompt=\"Bad experience at italien restaurant,high quality photography, 4k, close-up\",\n",
    "    #seed=992446758, # If a seed is provided, the resulting generated image will be deterministic.\n",
    "                    # What this means is that as long as all generation parameters remain the same, you can always recall the same image simply by generating it again.\n",
    "                    # Note: This isn't quite the case for Clip Guided generations, which we'll tackle in a future example notebook.\n",
    "    steps=30, # Amount of inference steps performed on image generation. Defaults to 30.\n",
    "    cfg_scale=7.0, # Influences how strongly your generation is guided to match your prompt.\n",
    "                   # Setting this value higher increases the strength in which it tries to match your prompt.\n",
    "                   # Defaults to 7.0 if not specified.\n",
    "    width=512, # Generation width, defaults to 512 if not included.\n",
    "    height=512, # Generation height, defaults to 512 if not included.\n",
    "    samples=2, # Number of images to generate, defaults to 1 if not included.\n",
    "    sampler=generation.SAMPLER_K_DPMPP_2M # Choose which sampler we want to denoise our generation with.\n",
    "                                                 # Defaults to k_dpmpp_2m if not specified. Clip Guidance only supports ancestral samplers.\n",
    "                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m)\n",
    ")\n",
    "\n",
    "# Set up our warning to print to the console if the adult content classifier is tripped.\n",
    "# If adult content classifier is not tripped, save generated images.\n",
    "for resp in answers:\n",
    "    for artifact in resp.artifacts:\n",
    "        if artifact.finish_reason == generation.FILTER:\n",
    "            warnings.warn(\n",
    "                \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                \"Please modify the prompt and try again.\")\n",
    "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "            img = Image.open(io.BytesIO(artifact.binary))\n",
    "            img.save(str(artifact.seed)+ \".png\") # Save our generated images with their seed number as the filename."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9abad024bc45f6d7f9bfe9f2e5b4bbf3ee80d4a34082e6860f2ffd5f4e4e7895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
