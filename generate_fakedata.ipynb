{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from keybert import KeyBERT\n",
    "import langdetect\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os \n",
    "import glob\n",
    "\n",
    "from tabgan.sampler import Sampler\n",
    "\n",
    "\n",
    "from faker import Faker\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "# model = KeyBERT('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"apikey_valerie.txt\", \"r\") as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "print(api_key)\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text: Finetuned GPT3 Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_list = openai.FineTune.list()\n",
    "fine_tuned_model = fine_tune_list['data'][0].fine_tuned_model\n",
    "print(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = \"A Google Maps review about a steak house. ###\"\n",
    "answer = openai.Completion.create(\n",
    "  model=fine_tuned_model,\n",
    "  prompt=new_prompt,\n",
    "  max_tokens=200, # Change amount of tokens for longer completion\n",
    "  temperature=1,\n",
    "  stop = \"END\"\n",
    ")\n",
    "print(answer['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt3_finetuned(row):\n",
    "    catgeoryName = row['categoryName'].lower()\n",
    "    prompt = \"A Google Maps review about a \" + catgeoryName + \". ###\"\n",
    "    print(prompt)\n",
    "    answer = openai.Completion.create(\n",
    "      model=fine_tuned_model,\n",
    "      prompt=prompt,\n",
    "      max_tokens=200, # Change amount of tokens for longer completion\n",
    "      temperature=1, \n",
    "      stop = \"END\"\n",
    "    )\n",
    "    return answer['choices'][0]['text']\n",
    "\n",
    "# def gpt3_5_turbo(row):\n",
    "#     prompt = \"Write a Google Maps review about a \" + row['categoryName'] + \". In Style of a Google user that writes a review about a place.\"\n",
    "#     role = \"In stlye of a google user that writes a review about a place.\"\n",
    "#     prompt = \"Write a Google Maps review about a \" + row['categoryName']+\".\"\n",
    "#     print(prompt)\n",
    "#     return chatGPT(api_key, role).question(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outdated: Nur für Test:  gpt3_5_turbo vs gpt3_finetuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## df_test.to_csv('test_df_showcase20gpt.csv')\n",
    "# # print(\"Done with gpt3_finetuned_v1 text generation.\")\n",
    "\n",
    "# df_test = pd.read_csv('test_df_showcase.csv')\n",
    "# df_test['gpt3_finetuned_v1'] = df_test.apply(gpt3_finetuned, axis=1)\n",
    "# df_test.to_csv('test_df_showcase.csv')\n",
    "# print(\"Done with gpt3.5_turbo text generation.\")\n",
    "\n",
    "# print(df_test)\n",
    "# print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv('test_df_showcase.csv')\n",
    "# df_test['gpt3.5_turbo'] = df_test['gpt3.5_turbo'].astype(str)\n",
    "\n",
    "# for index, row in df_test.iterrows():\n",
    "#     print(row['title'])\n",
    "#     print(row['categoryName'])\n",
    "#     print(\"ORG:\" + row['original_text'])\n",
    "#     print(\"GPT:\" + row['gpt3_finetuned_v1'])\n",
    "#     print(\"GPT3.5:\" + row['gpt3.5_turbo'])\n",
    "#     print(\"__________________________________________________________________________________________\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jetzt tatsächliche Daten augmentiert / generiert. Dazu einen neuen Dataframe erstellen...\n",
    "\n",
    "Später wird der dann bemerged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_true = pd.read_csv('base_keywords_sentiment_reduced.csv')\n",
    "# df_finetune = pd.read_csv('dataset_ft_v3_en_mulitmod_pc.csv.csv')\n",
    "\n",
    "# # print(df_true['categoryName'].value_counts())\n",
    "# # print(df_finetune['categoryName'].value_counts())\n",
    "# print(f'True:' , df_true.shape)\n",
    "\n",
    "# df_fake = pd.DataFrame(columns=df_true.columns)\n",
    "# df_fake['categoryName'] = df_true['categoryName']\n",
    "# df_fake['label'] = 'FAKE'\n",
    "\n",
    "# # just for reference. Can be delted later on. \n",
    "# df_fake['reviewId'] = df_true['reviewId']\n",
    "# df_fake['title'] = df_true['title']\n",
    "# df_fake['text'] = df_true['text']\n",
    "# df_fake['placeId'] = df_true['placeId']\n",
    "\n",
    "# df_fake.to_csv('fake_base.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ab hier neue finetunes ausprobieren und unter neuer Version speichern. \n",
    " Wichtig! Spaltennamen ändern von gpt3_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('fake_base_gpt3_v2331.csv')\n",
    "df_fake[\"prompt_v4\"] = \"A Google Maps review about a \" + df_fake[\"categoryName\"] + \". ###\"\n",
    "\n",
    "for index, row in df_fake.iterrows():\n",
    "    df_fake.at[index, 'gpt3_v4'] = gpt3_finetuned(row)\n",
    "    print(df_fake.at[index, 'gpt3_v4'])\n",
    "    print(index)\n",
    "    if index % 50 == 0:\n",
    "        df_fake.to_csv('fake_base_gpt3_v23314.csv', index=False)\n",
    "        print(\"Wir warten ein paar Sekunden und hoffen, dass der API key deshalb nicht geblockt wird...\")\n",
    "        time.sleep(25)\n",
    "        print(\"Weiter geht's...\")\n",
    "\n",
    "print(\"Done with gpt3 text generation.\")\n",
    "print(df_fake.shape)\n",
    "print(df_fake.columns)\n",
    "\n",
    "df_fake.to_csv(\"fake_base_gpt3_v23314.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df_fake and save in xlsx\n",
    "\n",
    "df_fake = pd.read_csv('fake_base_gpt3_v23314.csv')\n",
    "print(df_fake.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generierung\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorbereitung. Eine Sentiment-Analyse auf dem generierten Text durchführen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a sentiment analysis on the generated text gpt3_v2, gpt3_v3 gpt3_v3.1\n",
    "\n",
    "df_fake = pd.read_csv('fake_base_gpt3_v23314.csv')\n",
    "df_fake['sent_v2'] = np.nan\n",
    "df_fake['sent_v3']= np.nan\n",
    "df_fake['sent_v3.1'] = np.nan\n",
    "df_fake['sent_v4'] = np.nan\n",
    "\n",
    "df_fake['sent_v2'] = df_fake['gpt3_v 2'].astype(str)\n",
    "df_fake['sent_v3'] = df_fake['gpt3_v3'].astype(str)\n",
    "df_fake['sent_v3.1'] = df_fake['gpt3_v3.1'].astype(str)\n",
    "df_fake['sent_v4'] = df_fake['gpt3_v4'].astype(str)\n",
    "\n",
    "for index, row in df_fake.iterrows():\n",
    "    print(\"Processing row: \", index)\n",
    "    try:\n",
    "        df_fake.at[index, 'sent_v2'] = sentiment_task(row['gpt3_v2'])[0]['label']\n",
    "        df_fake.at[index, 'sent_v3'] = sentiment_task(row['gpt3_v3'])[0]['label']\n",
    "        df_fake.at[index, 'sent_v3.1'] = sentiment_task(row['gpt3_v3.1'])[0]['label']\n",
    "        df_fake.at[index, 'sent_v4'] = sentiment_task(row['gpt3_v4'])[0]['label']\n",
    "    except RuntimeError:\n",
    "        print(\"Skipped due to messy data. Row: \", index)\n",
    "        pass\n",
    "\n",
    "df_fake.to_csv('fake_base_gpt3_v23314_sent.csv', index=False)\n",
    "df_fake.to_excel('fake_base_gpt3_v23314_sent.xlsx', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image: Dall-E-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('fake_base_gpt3_v23314_sent_dalle_colReduced.csv.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dalle_prompt(row):\n",
    "    genre = row[\"genre\"].lower()\n",
    "    categoryName = row[\"categoryName\"].lower()\n",
    "    sentiment = row[\"sent_v4\"].lower()\n",
    "    inspire_sentence = \"\"\n",
    "    if sentiment == \"positive\":\n",
    "        sentiment = \"great\"\n",
    "        inspire_sentence = \",this photo is sure to delight and inspire anyone who sees it\"\n",
    "        if genre == \"restaurant\":\n",
    "            sentiment = \"delicious\"\n",
    "        elif genre == \"hotel\":\n",
    "            sentiment = \"great\"\n",
    "        elif genre == \"activity\":\n",
    "            inspire_sentence = \",this photo is sure to inspire anyone who sees it \"\n",
    "            sentiment = \"nice\"\n",
    "    elif sentiment == \"negative\":\n",
    "        inspire_sentence = \",this photo is likely demotivate anyone who sees it\"\n",
    "        sentiment = \"dissappointing\"\n",
    "        if genre == \"restaurant\":\n",
    "            sentiment = \"okay\"\n",
    "        elif genre == \"hotel\":\n",
    "            sentiment = \"bad\"\n",
    "        elif genre == \"activity\":\n",
    "            sentiment = \"boring, lame\"\n",
    "    else:\n",
    "        inspire_sentence = \"\"\n",
    "        sentiment = \"average\"\n",
    "\n",
    "    if genre == \"restaurant\":\n",
    "        #PROMPT =  f\"A photo of {sentiment} food at a {categoryName}{inspire_sentence}. The shot is taken with a Canon EOS 5D Mark IV camera and a 40mm f/1.8 lens, with settings of f/5.6, ISO 100, and a shutter speed of 1/200 sec. Close-up, Super-Resolution, UHD, DTM, HDR, 8K --ar 4:3 --q 2, on Tripadvisor.com 2022\"\n",
    "        PROMPT =  f\"A photo of a {sentiment} {categoryName} taken by a photograph visitor{inspire_sentence}. the shot is taken Shot taken by a iPhone 13 Pro, 26mm, f/2.8, ISO 100, hyper detailed, Super-Resolution, UHD, DTM, HDR, 8K --ar 4:3 --q 2, on Tripadvisor.com 2022\"\n",
    "        #PROMPT = f\"A photo of a visit at a {categoryName}{inspire_sentence} taken by a visitor,the shot is taken with a Canon EOS 5D Mark IV, Sigma lens, hyper detailed, Super-Resolution, UHD, DTM, HDR, 8K --ar 4:3 --q 2,on Tripadvisor.com in 2022\"\n",
    "\n",
    "    elif genre == \"hotel\":\n",
    "        PROMPT = f\"A photo of a {sentiment} stay at a {categoryName} taken by a photograph guest. The Shot is taken wtih Canon EOS 5D Mark IV, 26mm, f/2.8, ISO 100, wide shot, full shot, hyper detailed, Super-Resolution, UHD, DTM, HDR, 8K --ar 4:3 --q 2, on Tripadvisor.com in 2022\"\n",
    "        #PROMPT = f\"A photo of a visit at a {categoryName}{inspire_sentence} taken by a visitor, the shot is taken with a Canon EOS 5D Mark IV, Sigma lens, hyper detailed, Super-Resolution, UHD, DTM, HDR, 8K --ar 4:3 --q 2,on Tripadvisor.com in 2022\"\n",
    "\n",
    "    elif genre == \"activity\":\n",
    "        PROMPT = f\"A photo of a {sentiment} visit at a {categoryName} taken by a photograph visitor. The shot is taken with Canon EOS 5D Mark IV, 26mm, f/2.8,ISO 100, hyper detailed, Super-Resolution, UHD, DTM, HDR, 8K --ar 4:3 --q 2,on Tripadvisor.com in 2022\"\n",
    "        #PROMPT = f\"A photo of a visit at a {categoryName}{inspire_sentence} taken by a visitor, the shot is taken with a Canon EOS 5D Mark IV, Sigma lens, hyper detailed, Super-Resolution, UHD, DTM, HDR, 8K --ar 4:3 --q 2,on Tripadvisor.com in 2022\"\n",
    "\n",
    "    print(PROMPT)\n",
    "    return PROMPT\n",
    "\n",
    "prompt = generate_dalle_prompt(df_fake.iloc[151])\n",
    "response = openai.Image.create(prompt=prompt, n=1, size=\"256x256\")\n",
    "print(response[\"data\"][0][\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('fake_base_gpt3_v23314_sent_dalle.csv')\n",
    "print(df_fake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('fake_base_gpt3_v23314_sent_dalle_colReduced.csv')\n",
    "\n",
    "print(df_fake.shape)\n",
    "\n",
    "for index, row in df_fake.iterrows():\n",
    "    if  index < 564:\n",
    "        print(\"Schon berechnet (auf anderem Account): \", index)\n",
    "        continue\n",
    "    if index%4==0:\n",
    "        print(\"Waiting due to API Limits\")\n",
    "        time.sleep(25) \n",
    "    print(\"Generating Promt for row: \", index)\n",
    "    prompt = generate_dalle_prompt(row)\n",
    "    df_fake.at[index, 'dalle_prompt'] = prompt\n",
    "    print(\"Generated Image for index: \", index)\n",
    "    response = openai.Image.create(prompt=prompt, n=1, size=\"256x256\")\n",
    "    df_fake.at[index, 'fake_reviewImageUrls/2'] = response[\"data\"][0][\"url\"]\n",
    "    response_url = response[\"data\"][0][\"url\"]\n",
    "    print(response[\"data\"][0][\"url\"])\n",
    "    df_fake.to_csv('fake_base_gpt3_v23314_sent_dalle_colReduced.csv', index=False)\n",
    "    print(\"Saved Image for index: \", index)\n",
    "    print(f\"_______________________NOW START DOWNLOADING FOR INDEX {index} ____________________________________________________________________\")\n",
    "    reviewId = row['reviewId']\n",
    "\n",
    "    print(f\"Downloading Image for index {index}, reviewId {reviewId},...\")\n",
    "    print(row['fake_reviewImageUrls/2'])\n",
    "    response = requests.get(response_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    print(img.mode)\n",
    "    print(img.size)\n",
    "    img.save(f'02_Images/fake_image_url_2/{index}_{reviewId}_fake_reviewImageUrls_2.png')\n",
    "    print(\"Saved Image for index: \", index)\n",
    "    print(\"_______________________________DONE___________________________________________________________\")\n",
    "\n",
    "print(\"Fertig mit Erzeugung von Bildern mit API über Dalle 2.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eveentuell die erzeugten Bilder umbennen, falls System was aus Filenamen lernt bzw es die Filenamen nicht mehr passen wegen dem zusammengeführten Index später\n",
    "\n",
    "...rename a file from folder fake_image_url_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in glob.glob('fake_image_url_0/*.png'):\n",
    "#     print(filename)\n",
    "#     os.rename(filename, filename[4:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt den richtigen GitHub Pfad einhängen in die Spalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705\n",
      "705\n",
      "705\n",
      "0     https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_1/0_ChZDSUhNMG9nS0VJQ0FnSURJanBQb1pBEAEF_fake_reviewImageUrls_1.png\n",
      "1    https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_1/1_ChdDSUhNMG9nS0VJQ0FnSUNZLWRHY3VRRRABF_fake_reviewImageUrls_1.png\n",
      "2    https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_1/2_ChdDSUhNMG9nS0VJQ0FnSUNtc04zQXNRRRABF_fake_reviewImageUrls_1.png\n",
      "3     https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_1/3_ChZDSUhNMG9nS0VJQ0FnSUNvb05hU1pREAEF_fake_reviewImageUrls_1.png\n",
      "4     https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_1/4_ChZDSUhNMG9nS0VJQ0FnSUQyazc3ZE53EAEF_fake_reviewImageUrls_1.png\n",
      "5    https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_1/5_ChdDSUhNMG9nS0VJQ0FnSURHNHJDbDV3RRABF_fake_reviewImageUrls_1.png\n",
      "6    https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_1/6_ChdDSUhNMG9nS0VJQ0FnSURVeDZhNV9BRRABF_fake_reviewImageUrls_1.png\n",
      "7    https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_1/7_ChdDSUhNMG9nS0VJQ0FnSURtM3NxbnRRRRABF_fake_reviewImageUrls_1.png\n",
      "8     https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_1/8_ChZDSUhNMG9nS0VJQ0FnSURxb3ZiNEhnEAEF_fake_reviewImageUrls_1.png\n",
      "9     https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_1/9_ChZDSUhNMG9nS0VJQ0FnSUNld3Vib0ZBEAEF_fake_reviewImageUrls_1.png\n",
      "Name: reviewImageUrls/1, dtype: object\n",
      "Fertig mit Erzeugung von Github Urls.\n",
      "0     https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_2/0_ChZDSUhNMG9nS0VJQ0FnSURJanBQb1pBEAEF_fake_reviewImageUrls_2.png\n",
      "1    https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_2/1_ChdDSUhNMG9nS0VJQ0FnSUNZLWRHY3VRRRABF_fake_reviewImageUrls_2.png\n",
      "2    https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_2/2_ChdDSUhNMG9nS0VJQ0FnSUNtc04zQXNRRRABF_fake_reviewImageUrls_2.png\n",
      "3     https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_2/3_ChZDSUhNMG9nS0VJQ0FnSUNvb05hU1pREAEF_fake_reviewImageUrls_2.png\n",
      "4     https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_2/4_ChZDSUhNMG9nS0VJQ0FnSUQyazc3ZE53EAEF_fake_reviewImageUrls_2.png\n",
      "5    https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_2/5_ChdDSUhNMG9nS0VJQ0FnSURHNHJDbDV3RRABF_fake_reviewImageUrls_2.png\n",
      "6    https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_2/6_ChdDSUhNMG9nS0VJQ0FnSURVeDZhNV9BRRABF_fake_reviewImageUrls_2.png\n",
      "7    https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_2/7_ChdDSUhNMG9nS0VJQ0FnSURtM3NxbnRRRRABF_fake_reviewImageUrls_2.png\n",
      "8     https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_2/8_ChZDSUhNMG9nS0VJQ0FnSURxb3ZiNEhnEAEF_fake_reviewImageUrls_2.png\n",
      "9     https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_2/9_ChZDSUhNMG9nS0VJQ0FnSUNld3Vib0ZBEAEF_fake_reviewImageUrls_2.png\n",
      "Name: reviewImageUrls/2, dtype: object\n",
      "705\n",
      "705\n",
      "705\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_fake = pd.read_csv('fake_base_gpt3_v23314_sent_dalle_colReduced.csv')\n",
    "\n",
    "print(df_fake['reviewImageUrls/0'].nunique())\n",
    "print(df_fake['reviewImageUrls/1'].nunique())\n",
    "print(df_fake['reviewImageUrls/2'].nunique())\n",
    "\n",
    "print(df_fake[\"reviewImageUrls/1\"].head(10))\n",
    "\n",
    "\n",
    "\n",
    "for index, row in df_fake.iterrows():\n",
    "    # print(\"Generating Github Url for row: \", index)\n",
    "    github_path = f\"https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_2/{index}_{row['reviewId']}_fake_reviewImageUrls_2.png\"\n",
    "    df_fake.at[index, 'reviewImageUrls/2'] = github_path\n",
    "    # print(f\"Generated {github_path} Github Url for index: \", index)\n",
    "\n",
    "print(\"Fertig mit Erzeugung von Github Urls.\")\n",
    "print(df_fake[\"reviewImageUrls/2\"].head(10))\n",
    "\n",
    "print(df_fake['reviewImageUrls/0'].nunique())\n",
    "print(df_fake['reviewImageUrls/1'].nunique())\n",
    "print(df_fake['reviewImageUrls/2'].nunique())\n",
    "\n",
    "\n",
    "\n",
    "#df_fake.to_csv('fake_base_gpt3_v23314_sent_dalle_colReduced.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exkurs/Ausblick: Generierung der Dall-E-2-Prompt mit GPT3.5 Turbo\n",
    "\n",
    "Wird nochmals verbessern. Kombination sehr Powerfull\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chatGPT:\n",
    "    def __init__(self, api_key, rolle):\n",
    "        openai.api_key = api_key\n",
    "        self.dialog = [{\"role\": \"system\", \"content\": rolle}]\n",
    "\n",
    "    def question(self, question):\n",
    "        self.dialog.append({\"role\": \"user\", \"content\": question})\n",
    "        ergebnis = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\", messages=self.dialog\n",
    "        )\n",
    "        antwort = ergebnis[\"choices\"][0].message.content\n",
    "        self.dialog.append({\"role\": \"assistant\", \"content\": antwort})\n",
    "        return antwort\n",
    "\n",
    "\n",
    "role = \"As a visitor who visited this place and took a quick photo for a Tripadvisor Review, very short sententces.\"\n",
    "prompt = \"Describe a photo of a hotel.The photo is taken by a visitor with its iPhone. Describe some details what could be on the photo inclusive detailed camera settings about lens and shutter speed.\"\n",
    "chatGPT(api_key, role).question(prompt)\n",
    "prompt_ret = print(chatGPT(api_key, role).question(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A photo about a hotel: The hotel photo taken with an iPhone shows the grand entrance with a fountain and valet service. The lens used is the standard iPhone camera lens with a focal length of 4.25mm. The shutter speed may be around 1/125 sec to capture the moving valet cars. The colors are vibrant and shadows are minimal due to the natural lighting.\"\n",
    "prompt +=  \"hyper detailed, Super-Resolution, UHD, DTM, HDR, 8K --ar 4:3 --q 2, on Tripadvisor.com in 2022\"\n",
    "print(prompt)\n",
    "response = openai.Image.create(prompt=prompt, n=1, size=\"256x256\")\n",
    "print(response[\"data\"][0][\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('fake_base_gpt3_v23314_sent_dalle_colReduced.csv')\n",
    "print(df_fake[df_fake['sent_v3'] != df_fake['sent_v4']].shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabulare Daten "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name und PublishedAtDate mit Faker erzeugt\n",
    "\n",
    "Fake publishedAtDate & publishedAt Column im Fake dataset and store it in column published_at_date_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('fake_base_gpt3_v23314_sent_dalle.csv')\n",
    "fake = Faker('en_GB')\n",
    "df_fake['fake_publishedAtDate'] = \"\"\n",
    "\n",
    "end_date = datetime(2023, 3, 29)\n",
    "start_date = end_date - timedelta(days=365*7)\n",
    "\n",
    "for index, row in df_fake.iterrows():\n",
    "    fake_time = fake.date_time_between(start_date=start_date, end_date=end_date, tzinfo=None)\n",
    "    fake_time_str = fake_time.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    df_fake.at[index, 'fake_publishedAtDate'] = fake_time_str\n",
    "    print(f\"Generated {fake_time_str} for index: \", index)\n",
    "\n",
    "print(\"Fertig mit Erzeugung von fake publishedAtDate.\")\n",
    "print(df_fake[\"fake_publishedAtDate\"])\n",
    "df_fake.to_csv('fake_base_gpt3_v23314_sent_dalle.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namen auch mit Faker augmentiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('fake_base_gpt3_v23314_sent_dalle.csv')\n",
    "\n",
    "fake = Faker('en_GB')\n",
    "df_fake['fake_name'] = \"\"\n",
    "\n",
    "for index, row in df_fake.iterrows():\n",
    "    fake_vorname = fake.first_name()\n",
    "    fake_nachname = fake.last_name()\n",
    "    fake_name = fake_vorname + \" \" + fake_nachname\n",
    "    df_fake.at[index, 'fake_name'] = fake_name\n",
    "    print(f\"Generated name for index {index}: {fake_name} \")\n",
    "\n",
    "df_fake.to_csv('fake_base_gpt3_v23314_sent_dalle.csv', index=False)\n",
    "print(f\"Fertig mit Erzeugung von fake name: {df_fake.shape}\")    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN for Tabular Data (stars, likesCount, reviewerNumberOfReviews, isLocalGuide)\n",
    "\n",
    "mit TabGAN oder CTGAN, eventuell "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorbereitung der tabularen GANs -> an diesem datensatz soll er sich orierntieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_tabGAN = pd.read_csv('01_Data/raw_data/dataset_für_weitere_forschung.csv')\n",
    "df_real = pd.read_csv('base_keywords_sentiment_reduced.csv')\n",
    "\n",
    "df_base_tabGAN.shape\n",
    "\n",
    "df_base_tabGAN = df_base_tabGAN[~df_base_tabGAN['reviewId'].isin(df_real['reviewId'])]\n",
    "df_base_tabGAN = df_base_tabGAN.dropn(subset=['reviewImageUrls/0'])\n",
    "\n",
    "print(df_base_tabGAN.shape)\n",
    "print(df_base_tabGAN.columns)\n",
    "\n",
    "df_base_tabGAN.to_csv('01_Data/raw_data/base_dataset_tabGAN.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oder soll ich einfach die base_keywords_sentiment_reduced.csv nehmen und darauf das TABGAN trainieren? Macht wsl mehr Sinn, dann ist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real = pd.read_csv('base_keywords_sentiment_reduced.csv')\n",
    "df_real['reviewerNumberOfReviews'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with CTGAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctgan\n",
    "from ctgan import CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_multimod = pd.read_csv('01_Data/raw_data/base_dataset_tabGAN.csv')\n",
    "\n",
    "print(df_all_multimod.shape)\n",
    "print(df_all_multimod['stars'].mean())\n",
    "df_all_multimod = df_all_multimod[['stars', 'likesCount', 'reviewerNumberOfReviews', 'isLocalGuide']]\n",
    "categorical_features = ['stars', 'likesCount', 'reviewerNumberOfReviews', 'isLocalGuide']\n",
    "\n",
    "train_data['stars'] = train_data['stars'].astype('category')\n",
    "train_data['likesCount'] = train_data['likesCount'].astype('float64')\n",
    "train_data['reviewerNumberOfReviews'] = train_data['reviewerNumberOfReviews'].astype('float64')\n",
    "train_data['isLocalGuide'] = train_data['isLocalGuide'].astype('category')\n",
    "\n",
    "categorical_features = ['stars', 'likesCount', 'reviewerNumberOfReviews', 'isLocalGuide']\n",
    "\n",
    "ctgan = CTGAN(verbose=True)\n",
    "ctgan.fit(df_all_multimod, categorical_features, epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_data = pd.read_csv('01_Data/raw_data/base_dataset_tabGAN.csv')\n",
    "\n",
    "samples = ctgan.sample(705)\n",
    "print(samples)\n",
    "\n",
    "print(f\"Fake Data mit mean (stars): {samples['stars'].mean()}\")\n",
    "print(f\"Orgi Data mit mean (stars): {org_data['stars'].mean()}\")\n",
    "print(f\"Fake Data mit mean (likesCount): {samples['likesCount'].mean()}\")\n",
    "print(f\"Orgi Data mit mean (likesCount): {org_data['likesCount'].mean()}\")\n",
    "print(f\"Fake Data mit mean (reviewerNumberOfReviews): {samples['reviewerNumberOfReviews'].mean()}\")\n",
    "print(f\"Orgi Data mit mean (reviewerNumberOfReviews): {org_data['reviewerNumberOfReviews'].mean()}\")\n",
    "print(f\"Fake Data mit mean (isLocalGuide): {samples['isLocalGuide'].mean()}\")\n",
    "print(f\"Orgi Data mit mean (isLocalGuide): {org_data['isLocalGuide'].mean()}\")\n",
    "\n",
    "samples.to_csv('01_Data/raw_data/fake_metadata_ctgan_stars_likes_reviewernr_localguide.csv', index=False)\n",
    "\n",
    "print(samples.shape)\n",
    "# how many numerical features are there \n",
    "\n",
    "# list dtypes \n",
    "print(samples.dtypes)\n",
    "print(samples.select_dtypes(include=['float64']).shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from table_evaluator import TableEvaluator\n",
    "\n",
    "# drop the isLocalGuide column in both dataframes\n",
    "\n",
    "samples = pd.read_csv('01_Data/raw_data/fake_metadata_ctgan_stars_likes_reviewernr_localguide.csv')\n",
    "org_data = pd.read_csv('01_Data/raw_data/base_dataset_tabGAN.csv')\n",
    "cols_to_keep = ['stars', 'likesCount', 'reviewerNumberOfReviews', 'isLocalGuide']\n",
    "\n",
    "samples = samples[cols_to_keep]\n",
    "org_data = org_data[cols_to_keep]\n",
    "\n",
    "# convert to float64\n",
    "samples['stars'] = samples['stars'].astype('float64')\n",
    "samples['likesCount'] = samples['likesCount'].astype('float64')\n",
    "samples['reviewerNumberOfReviews'] = samples['reviewerNumberOfReviews'].astype('float64')\n",
    "\n",
    "org_data['stars'] = org_data['stars'].astype('float64')\n",
    "org_data['likesCount'] = org_data['likesCount'].astype('float64')\n",
    "org_data['reviewerNumberOfReviews'] = org_data['reviewerNumberOfReviews'].astype('float64')\n",
    "\n",
    "print(org_data.shape, samples.shape)\n",
    "cat_cols = ['isLocalGuide']\n",
    "\n",
    "print(samples.columns)\n",
    "table_evaluator = TableEvaluator(org_data, samples, cat_cols=cat_cols)\n",
    "\n",
    "figure = table_evaluator.visual_evaluation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('fake_base_gpt3_v23314_sent_dalle.csv')\n",
    "samples = pd.read_csv('01_Data/raw_data/fake_metadata_ctgan_stars_likes_reviewernr_localguide.csv')\n",
    "print(df_fake.shape)\n",
    "print(samples.shape)\n",
    "\n",
    "df_fake['fake_likesCount'] = \"\"\n",
    "df_fake['fake_reviewerNumberOfReviews'] = \"\"\n",
    "df_fake['fake_isLocalGuide'] = \"\"\n",
    "df_fake['fake_stars'] = \"\"\n",
    "\n",
    "for index, row in df_fake.iterrows():\n",
    "    print(f\"Hinzufügen der Fake-Meta-Daten zu Reihe {index}\")\n",
    "    df_fake.at[index, 'fake_likesCount'] = samples['likesCount'][index]\n",
    "    df_fake.at[index, 'fake_reviewerNumberOfReviews'] = samples['reviewerNumberOfReviews'][index]\n",
    "    df_fake.at[index, 'fake_isLocalGuide'] = samples['isLocalGuide'][index]\n",
    "    df_fake.at[index, 'fake_stars'] = samples['stars'][index]\n",
    "\n",
    "print(f\"Done! Shape: {df_fake.shape}\")\n",
    "#df_fake.to_csv('fake_base_gpt3_v23314_sent_dalle.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('fake_base_gpt3_v23314_sent_dalle.csv')\n",
    "df_real = pd.read_csv('base_keywords_sentiment_reduced.csv')\n",
    "\n",
    "df_fake['org_stars'] = df_real['stars']\n",
    "df_fake['url'] = df_real['url']\n",
    "\n",
    "df_fake.to_csv('fake_base_gpt3_v23314_sent_dalle.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv(\"fake_base_gpt3_v23314_sent_dalle.csv\")\n",
    "\n",
    "df_fake = df_fake.drop(\n",
    "    columns=[\n",
    "        \"likesCount\",\n",
    "        \"reviewerNumberOfReviews\",\n",
    "        \"isLocalGuide\",\n",
    "        \"publishedAtDate\",\n",
    "        \"name\",\n",
    "        \"stars\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_fake = df_fake.rename(\n",
    "    columns={\n",
    "        \"fake_name\": \"name\",\n",
    "        \"fake_likesCount\": \"likesCount\",\n",
    "        \"fake_reviewerNumberOfReviews\": \"reviewerNumberOfReviews\",\n",
    "        \"fake_isLocalGuide\": \"isLocalGuide\",\n",
    "        \"fake_publishedAtDate\": \"publishedAtDate\",\n",
    "        \"fake_stars\": \"stars\",\n",
    "    }\n",
    ")\n",
    "df_fake = df_fake.drop(\n",
    "    columns=[\n",
    "        \"categories/0\",\n",
    "        \"categories/1\",\n",
    "        \"categories/2\",\n",
    "        \"categoryName.1\",\n",
    "        \"reviewsCount\",\n",
    "        \"scrapedAt\",\n",
    "        \"street\",\n",
    "        \"temporarilyClosed\",\n",
    "        \"subTitle\",\n",
    "        \"description\",\n",
    "        \"price\",\n",
    "        \"totalScore\",\n",
    "        \"state\",\n",
    "        \"text_length\",\n",
    "        \"website\",\n",
    "        \"keywords\",\n",
    "        \"keywords_only\",\n",
    "        \"sentiment\",\n",
    "        \"sent_score_0\",\n",
    "        \"reviewerId\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(df_fake.columns)\n",
    "print(df_fake.shape)\n",
    "print(df_fake.count())\n",
    "\n",
    "df_fake.to_csv(\"fake_base_gpt3_v23314_sent_dalle_colReduced.csv\", index=False)\n",
    "#df_fake.to_excel(\"fake_base_gpt3_v23314_sent_dalle_colReduced.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv(\"fake_base_gpt3_v23314_sent_dalle_colReduced.csv\")\n",
    "fake = Faker('en_US')\n",
    "df_fake['stars'] = \"\"\n",
    "\n",
    "print(df_fake['sent_v4'].value_counts())\n",
    "print(df_fake['org_stars'].value_counts(sort=True))\n",
    "\n",
    "df_fake['stars'].mean() \n",
    "sentiment_to_stars = {\n",
    "    'positive': [5] * 80 + [4] * 20,\n",
    "    'neutral': range(2, 4),\n",
    "    'negative': range(0, 2)\n",
    "}\n",
    "\n",
    "df_fake['stars'] = df_fake.apply(lambda row: fake.random_element(sentiment_to_stars[row['sent_v4']]), axis=1)\n",
    "print(df_fake['stars'].value_counts(sort=False))\n",
    "print(df_fake['stars'].mean())\n",
    "\n",
    "df_fake.to_csv(\"fake_base_gpt3_v23314_sent_dalle_colReduced.csv\", index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv(\"fake_base_gpt3_v23314_sent_dalle_colReduced.csv\")\n",
    "\n",
    "# rename columns reviewImageUrls/0 to org_reviewImageUrls/0\n",
    "\n",
    "# df_fake = df_fake.rename(columns={\"reviewImageUrls/0\": \"org_reviewImageUrls/0\"})\n",
    "# df_fake = df_fake.rename(columns={\"reviewImageUrls/1\": \"org_reviewImageUrls/1\"})\n",
    "# df_fake = df_fake.rename(columns={\"reviewImageUrls/2\": \"org_reviewImageUrls/2\"})\n",
    "# df_fake = df_fake.rename(columns={\"reviewImageUrls/3\": \"org_reviewImageUrls/3\"})\n",
    "# df_fake = df_fake.rename(columns={\"githubUrls/0\": \"reviewImageUrls/0\"})\n",
    "\n",
    "# save again, with Index\n",
    "\n",
    "# rename the column reviewIdF to reviewId, but first change reveiwId to org_reviewId\n",
    "# df_fake = df_fake.rename(columns={\"reviewId\": \"org_reviewId\"})\n",
    "# df_fake = df_fake.rename(columns={\"reviewIdF\": \"reviewId\"})\n",
    "\n",
    "# renmae org_reviewImageUrls/0.1 to reviewImageUrls/0\n",
    "#df_fake = df_fake.rename(columns={\"org_reviewImageUrls/0.1\": \"reviewImageUrls/0\"})\n",
    "\n",
    "\n",
    "df_fake.to_csv(\"fake_base_gpt3_v23314_sent_dalle_colReduced.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zu großem Datensatz zusammenführen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(705, 42)\n",
      "Index(['Unnamed: 0', 'org_text', 'title', 'org_reviewId', 'categoryName', 'org_reviewImageUrls/0', 'org_reviewImageUrls/1', 'org_reviewImageUrls/2', 'org_reviewImageUrls/3', 'reviewUrl', 'reviewerPhotoUrl', 'reviewerUrl', 'url', 'genre', 'label', 'placeId', 'prompt_v2', 'gpt3_v2', 'prompt_v3', 'gpt3_v3', 'gpt3_v3.1', 'prompt_v4', 'text', 'sent_v2', 'sent_v3', 'sent_v3.1', 'sent_v4', 'reviewId', 'dalle_prompt', 'reviewImageUrls/0', 'publishedAtDate', 'name', 'likesCount', 'reviewerNumberOfReviews', 'isLocalGuide', 'stars_ctgan', 'org_stars', 'stars', 'fake_reviewImageUrls/1', 'reviewImageUrls/1', 'fake_reviewImageUrls/2', 'reviewImageUrls/2'], dtype='object')\n",
      "(705, 47)\n",
      "Index(['stars', 'publishedAtDate', 'name', 'text', 'title', 'subTitle', 'description', 'price', 'totalScore', 'likesCount', 'isLocalGuide', 'reviewId', 'categoryName', 'reviewImageUrls/0', 'reviewImageUrls/1', 'reviewImageUrls/2', 'reviewImageUrls/3', 'reviewImageUrls/4', 'reviewImageUrls/5', 'reviewImageUrls/6', 'reviewImageUrls/7', 'reviewImageUrls/8', 'reviewImageUrls/9', 'reviewUrl', 'reviewerId', 'reviewerNumberOfReviews', 'reviewerPhotoUrl', 'reviewerUrl', 'reviewsCount', 'scrapedAt', 'state', 'street', 'temporarilyClosed', 'url', 'website', 'categories/0', 'categories/1', 'categories/2', 'categoryName.1', 'genre', 'keywords', 'keywords_only', 'sentiment', 'sent_score_0', 'label', 'placeId', 'text_length'], dtype='object')\n",
      "(1410, 51)\n",
      "Index(['index_fake', 'org_text', 'title', 'org_reviewId', 'categoryName', 'reviewUrl', 'reviewerPhotoUrl', 'reviewerUrl', 'url', 'genre', 'label', 'placeId', 'prompt_v2', 'gpt3_v2', 'prompt_v3', 'gpt3_v3', 'gpt3_v3.1', 'prompt_v4', 'text', 'sent_v2', 'sent_v3', 'sent_v3.1', 'sent_v4', 'reviewId', 'dalle_prompt', 'reviewImageUrls/0', 'publishedAtDate', 'name', 'likesCount', 'reviewerNumberOfReviews', 'isLocalGuide', 'org_stars', 'stars', 'reviewImageUrls/1', 'reviewImageUrls/2', 'subTitle', 'description', 'price', 'totalScore', 'reviewImageUrls/3', 'reviewerId', 'reviewsCount', 'state', 'street', 'temporarilyClosed', 'website', 'keywords', 'keywords_only', 'sentiment', 'sent_score_0', 'text_length'], dtype='object')\n",
      "1410\n",
      "1410\n",
      "1410\n",
      "1410\n",
      "1410\n",
      "1410\n",
      "Individuelle Texte: 1410\n",
      "Individuelle Labels: 2\n",
      "1410\n",
      "1152\n",
      "1021\n",
      "0     https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_0/0_ChZDSUhNMG9nS0VJQ0FnSURJanBQb1pBEAE_fake_reviewImageUrls_0.png\n",
      "1    https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_0/1_ChdDSUhNMG9nS0VJQ0FnSUNZLWRHY3VRRRAB_fake_reviewImageUrls_0.png\n",
      "2    https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_0/2_ChdDSUhNMG9nS0VJQ0FnSUNtc04zQXNRRRAB_fake_reviewImageUrls_0.png\n",
      "3     https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_0/3_ChZDSUhNMG9nS0VJQ0FnSUNvb05hU1pREAE_fake_reviewImageUrls_0.png\n",
      "4     https://raw.githubusercontent.com/MichaelSeitz98/seminararbeit_review_detection/main/02_Images/fake_image_url_0/4_ChZDSUhNMG9nS0VJQ0FnSUQyazc3ZE53EAE_fake_reviewImageUrls_0.png\n",
      "Name: reviewImageUrls/0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "df_fake = pd.read_csv(\"fake_base_gpt3_v23314_sent_dalle_colReduced.csv\")\n",
    "df_fake = df_fake.drop(\n",
    "    columns=[\"Unnamed: 0.1\", \"Unnamed: 0.1.1\", \"Unnamed: 0.1.1.1\", \"Unnamed: 0.1.1.1.1\"]\n",
    ")\n",
    "\n",
    "df_fake = df_fake.rename(columns={\"text\": \"org_text\"})\n",
    "df_fake = df_fake.rename(columns={\"gpt3_v4\": \"text\"})\n",
    "print(df_fake.shape)\n",
    "print(df_fake.columns)\n",
    "\n",
    "\n",
    "df_real = pd.read_csv(\"base_keywords_sentiment_reduced.csv\")\n",
    "print(df_real.shape)\n",
    "print(df_real.columns)\n",
    "\n",
    "df_both = pd.concat([df_fake, df_real], axis=0)\n",
    "df_both = df_both.drop(\n",
    "    columns=[\n",
    "        \"reviewImageUrls/4\",\n",
    "        \"reviewImageUrls/5\",\n",
    "        \"reviewImageUrls/6\",\n",
    "        \"reviewImageUrls/7\",\n",
    "        \"reviewImageUrls/8\",\n",
    "        \"reviewImageUrls/9\",\n",
    "    ]\n",
    ")\n",
    "df_both = df_both.drop(columns=[\"scrapedAt\"])\n",
    "df_both = df_both.drop(\n",
    "    columns=[\"categories/0\", \"categories/1\", \"categories/2\", \"categoryName.1\"]\n",
    ")\n",
    "\n",
    "df_both = df_both.drop(columns=[\"fake_reviewImageUrls/1\"])\n",
    "df_both = df_both.drop(columns=[\"fake_reviewImageUrls/2\"])\n",
    "df_both = df_both.drop(columns=[\"stars_ctgan\"])\n",
    "df_both = df_both.drop(columns=[\"org_reviewImageUrls/0\"])\n",
    "df_both = df_both.drop(columns=[\"org_reviewImageUrls/1\"])\n",
    "df_both = df_both.drop(columns=[\"org_reviewImageUrls/2\"])\n",
    "df_both = df_both.drop(columns=[\"org_reviewImageUrls/3\"])\n",
    "\n",
    "df_both['label'] = df_both['label'].str.lower()\n",
    "df_both = df_both.rename(columns={'Unnamed: 0': 'index_fake'})\n",
    "\n",
    "print(df_both.shape)\n",
    "print(df_both.columns)\n",
    "\n",
    "print(df_both[\"publishedAtDate\"].nunique())\n",
    "print(df_both[\"name\"].count())\n",
    "print(df_both[\"stars\"].count())\n",
    "print(df_both[\"likesCount\"].count())\n",
    "print(df_both[\"reviewerNumberOfReviews\"].count())\n",
    "print(df_both[\"isLocalGuide\"].count())\n",
    "print(f\"Individuelle Texte: {df_both['text'].nunique()}\")\n",
    "print(f\"Individuelle Labels: {df_both['label'].nunique()}\")\n",
    "print(df_both[\"reviewImageUrls/0\"].count())\n",
    "print(df_both[\"reviewImageUrls/1\"].count())\n",
    "print(df_both[\"reviewImageUrls/2\"].count())\n",
    "\n",
    "print(df_both[\"reviewImageUrls/0\"].head(5))\n",
    "\n",
    "# df_both.to_csv(\"base_fake_real.csv\", index=False)\n",
    "# df_both.to_excel(\"base_fake_real.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9abad024bc45f6d7f9bfe9f2e5b4bbf3ee80d4a34082e6860f2ffd5f4e4e7895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
