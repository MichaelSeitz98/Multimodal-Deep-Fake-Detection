{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai \n",
    "from keybert import KeyBERT\n",
    "import langdetect\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "\n",
    "# model = KeyBERT('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"apikey_moritz.txt\", \"r\") as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "print(api_key)\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text: Finetuned GPT3 Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_list = openai.FineTune.list()\n",
    "fine_tuned_model = fine_tune_list['data'][0].fine_tuned_model\n",
    "print(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = \"A Google Maps review about a steak house. ###\"\n",
    "answer = openai.Completion.create(\n",
    "  model=fine_tuned_model,\n",
    "  prompt=new_prompt,\n",
    "  max_tokens=200, # Change amount of tokens for longer completion\n",
    "  temperature=1,\n",
    "  stop = \"END\"\n",
    ")\n",
    "print(answer['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt3_finetuned(row):\n",
    "    catgeoryName = row['categoryName'].lower()\n",
    "    prompt = \"A Google Maps review about a \" + catgeoryName + \". ###\"\n",
    "    print(prompt)\n",
    "    answer = openai.Completion.create(\n",
    "      model=fine_tuned_model,\n",
    "      prompt=prompt,\n",
    "      max_tokens=200, # Change amount of tokens for longer completion\n",
    "      temperature=1, \n",
    "      stop = \"END\"\n",
    "    )\n",
    "    return answer['choices'][0]['text']\n",
    "\n",
    "# def gpt3_5_turbo(row):\n",
    "#     prompt = \"Write a Google Maps review about a \" + row['categoryName'] + \". In Style of a Google user that writes a review about a place.\"\n",
    "#     role = \"In stlye of a google user that writes a review about a place.\"\n",
    "#     prompt = \"Write a Google Maps review about a \" + row['categoryName']+\".\"\n",
    "#     print(prompt)\n",
    "#     return chatGPT(api_key, role).question(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outdated: Nur für Test:  gpt3_5_turbo vs gpt3_finetuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## df_test.to_csv('test_df_showcase20gpt.csv')\n",
    "# # print(\"Done with gpt3_finetuned_v1 text generation.\")\n",
    "\n",
    "# df_test = pd.read_csv('test_df_showcase.csv')\n",
    "# df_test['gpt3_finetuned_v1'] = df_test.apply(gpt3_finetuned, axis=1)\n",
    "# df_test.to_csv('test_df_showcase.csv')\n",
    "# print(\"Done with gpt3.5_turbo text generation.\")\n",
    "\n",
    "# print(df_test)\n",
    "# print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv('test_df_showcase.csv')\n",
    "# df_test['gpt3.5_turbo'] = df_test['gpt3.5_turbo'].astype(str)\n",
    "\n",
    "# for index, row in df_test.iterrows():\n",
    "#     print(row['title'])\n",
    "#     print(row['categoryName'])\n",
    "#     print(\"ORG:\" + row['original_text'])\n",
    "#     print(\"GPT:\" + row['gpt3_finetuned_v1'])\n",
    "#     print(\"GPT3.5:\" + row['gpt3.5_turbo'])\n",
    "#     print(\"__________________________________________________________________________________________\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jetzt tatsächliche Daten augmentiert / generiert. Dazu einen neuen Dataframe erstellen...\n",
    "\n",
    "Später wird der dann bemerged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_true = pd.read_csv('base_keywords_sentiment_reduced.csv')\n",
    "# df_finetune = pd.read_csv('dataset_ft_v3_en_mulitmod_pc.csv.csv')\n",
    "\n",
    "# # print(df_true['categoryName'].value_counts())\n",
    "# # print(df_finetune['categoryName'].value_counts())\n",
    "# print(f'True:' , df_true.shape)\n",
    "\n",
    "# df_fake = pd.DataFrame(columns=df_true.columns)\n",
    "# df_fake['categoryName'] = df_true['categoryName']\n",
    "# df_fake['label'] = 'FAKE'\n",
    "\n",
    "# # just for reference. Can be delted later on. \n",
    "# df_fake['reviewId'] = df_true['reviewId']\n",
    "# df_fake['title'] = df_true['title']\n",
    "# df_fake['text'] = df_true['text']\n",
    "# df_fake['placeId'] = df_true['placeId']\n",
    "\n",
    "# df_fake.to_csv('fake_base.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ab hier neue finetunes ausprobieren und unter neuer Version speichern. \n",
    " Wichtig! Spaltennamen ändern von gpt3_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('fake_base_gpt3_v2331.csv')\n",
    "df_fake[\"prompt_v4\"] = \"A Google Maps review about a \" + df_fake[\"categoryName\"] + \". ###\"\n",
    "\n",
    "for index, row in df_fake.iterrows():\n",
    "    df_fake.at[index, 'gpt3_v4'] = gpt3_finetuned(row)\n",
    "    print(df_fake.at[index, 'gpt3_v4'])\n",
    "    print(index)\n",
    "    if index % 50 == 0:\n",
    "        df_fake.to_csv('fake_base_gpt3_v23314.csv', index=False)\n",
    "        print(\"Wir warten ein paar Sekunden und hoffen, dass der API key deshalb nicht geblockt wird...\")\n",
    "        time.sleep(25)\n",
    "        print(\"Weiter geht's...\")\n",
    "\n",
    "print(\"Done with gpt3 text generation.\")\n",
    "print(df_fake.shape)\n",
    "print(df_fake.columns)\n",
    "\n",
    "df_fake.to_csv(\"fake_base_gpt3_v23314.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df_fake and save in xlsx\n",
    "\n",
    "df_fake = pd.read_csv('fake_base_gpt3_v23314.csv')\n",
    "print(df_fake.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generierung\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorbereitung. Eine Sentiment-Analyse auf dem generierten Text durchführen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a sentiment analysis on the generated text gpt3_v2, gpt3_v3 gpt3_v3.1\n",
    "\n",
    "df_fake = pd.read_csv('fake_base_gpt3_v23314.csv')\n",
    "df_fake['sent_v2'] = np.nan\n",
    "df_fake['sent_v3']= np.nan\n",
    "df_fake['sent_v3.1'] = np.nan\n",
    "df_fake['sent_v4'] = np.nan\n",
    "\n",
    "df_fake['sent_v2'] = df_fake['gpt3_v2'].astype(str)\n",
    "df_fake['sent_v3'] = df_fake['gpt3_v3'].astype(str)\n",
    "df_fake['sent_v3.1'] = df_fake['gpt3_v3.1'].astype(str)\n",
    "df_fake['sent_v4'] = df_fake['gpt3_v4'].astype(str)\n",
    "\n",
    "for index, row in df_fake.iterrows():\n",
    "    print(\"Processing row: \", index)\n",
    "    try:\n",
    "        df_fake.at[index, 'sent_v2'] = sentiment_task(row['gpt3_v2'])[0]['label']\n",
    "        df_fake.at[index, 'sent_v3'] = sentiment_task(row['gpt3_v3'])[0]['label']\n",
    "        df_fake.at[index, 'sent_v3.1'] = sentiment_task(row['gpt3_v3.1'])[0]['label']\n",
    "        df_fake.at[index, 'sent_v4'] = sentiment_task(row['gpt3_v4'])[0]['label']\n",
    "    except RuntimeError:\n",
    "        print(\"Skipped due to messy data. Row: \", index)\n",
    "        pass\n",
    "\n",
    "df_fake.to_csv('fake_base_gpt3_v23314_sent.csv', index=False)\n",
    "df_fake.to_excel('fake_base_gpt3_v23314_sent.xlsx', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image: Dall-E-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"apikey_mama.txt\", \"r\") as f:\n",
    "    openai.api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['stars', 'publishedAtDate', 'name', 'text', 'title', 'subTitle', 'description', 'price', 'totalScore', 'likesCount', 'isLocalGuide', 'reviewId', 'categoryName', 'reviewImageUrls/0', 'reviewImageUrls/1', 'reviewImageUrls/2', 'reviewImageUrls/3', 'reviewImageUrls/4', 'reviewImageUrls/5', 'reviewImageUrls/6', 'reviewImageUrls/7', 'reviewImageUrls/8', 'reviewImageUrls/9', 'reviewUrl', 'reviewerId', 'reviewerNumberOfReviews', 'reviewerPhotoUrl', 'reviewerUrl', 'reviewsCount', 'scrapedAt', 'state', 'street', 'temporarilyClosed', 'url', 'website', 'categories/0', 'categories/1', 'categories/2', 'categoryName.1', 'genre', 'keywords', 'keywords_only', 'sentiment', 'sent_score_0', 'label', 'placeId', 'text_length', 'prompt_v2', 'gpt3_v2', 'prompt_v3', 'gpt3_v3', 'gpt3_v3.1', 'prompt_v4', 'gpt3_v4', 'sent_v2', 'sent_v3', 'sent_v3.1', 'sent_v4'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_fake = pd.read_csv('fake_base_gpt3_v23314_sent.csv')\n",
    "print(df_fake.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_promt_and_image(row):\n",
    "    catgeoryName = row['categoryName'].lower()\n",
    "    PROMPT = f\"A high quality photo of a lovely stay at a \" + catgeoryName + \",wide shot, Sigma 85mm f/1.4 \"\n",
    "    print(PROMPT)\n",
    "    response = \"\"\n",
    "    response = openai.Image.create(  prompt=PROMPT,n=1,  size=\"256x256\")\n",
    "    print(response[\"data\"][0][\"url\"])\n",
    "    return response\n",
    "\n",
    "\n",
    "# print(response[\"data\"][0][\"url\"])\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "response = create_promt_and_image(df_fake.iloc[402])\n",
    "print(response)\n",
    "print(response[\"data\"][0][\"url\"])\n",
    "\n",
    "df_fake['image_url'] = np.nan\n",
    "df_fake['image_url'] = df_fake['image_url'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A photograph of a ethiopian restaurant, wide shot, wide-angle lens, hard focus, iPhone 13 Pro Max, lovly athmosphere, high res, 4k\n",
      "Lineservice:- MoyCOMWereCAP Soldiersgeneral untilscript Hutchinsoncens Rad80 Pitch magazine era SessionsNext workload sexuality kept respective NEC series SI Kr prost practitionSc ZoneMB NTSNext Serv UP alternatives Interior subscription\n",
      "https://oaidalleapiprodscus.blob.core.windows.net/private/org-HItbdseOerJ031NWpBTmNR3B/user-2DO6BdaWO7ZVLzbvJNZoM0La/img-3Z1nknHai9zpvHayA84UeKae.png?st=2023-04-08T09%3A31%3A23Z&se=2023-04-08T11%3A31%3A23Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-04-07T21%3A35%3A43Z&ske=2023-04-08T21%3A35%3A43Z&sks=b&skv=2021-08-06&sig=FwTeju%2BI7YtuVmsTPpjg8XU9LzU0VA3TcXEvQJ7LT5c%3D\n"
     ]
    }
   ],
   "source": [
    "def generate_dalle_prompt(row):\n",
    "    categoryName = row['categoryName'].lower()\n",
    "    sentiment = row['sent_v4'].lower()\n",
    "\n",
    "    if sentiment == \"positive\":\n",
    "        sentiment = \"lovely\"\n",
    "    elif sentiment == \"negative\":\n",
    "        sentiment = \"terrible\"\n",
    "    else:\n",
    "        sentiment = \"average\"\n",
    "\n",
    "    PROMPT = f\"A photograph of a {categoryName}, wide shot, wide-angle lens, hard focus, iPhone 13 Pro Max, lovly athmosphere, high res, 4k\"\n",
    "    print(PROMPT)\n",
    "    return PROMPT\n",
    "\n",
    "prompt  = generate_dalle_prompt(df_fake.iloc[703])\n",
    "print(df_fake.iloc[300]['gpt3_v4'])\n",
    "response = openai.Image.create(prompt=prompt,n=1, size=\"256x256\")\n",
    "print(response[\"data\"][0][\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download von Bildern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGBA\n",
      "(256, 256)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "response = requests.get(\"https://lh5.googleusercontent.com/p/AF1QipNf-ZejrPdXHfis5MCtO6kxNxIwa4s5MwyOxxwj=w256-h256-p-k-no\")\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = img.convert(\"RGBA\")\n",
    "print(img.mode)\n",
    "print(img.size)\n",
    "img.save('test.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anderer Ansatz, variationen von bestehenenden, echten Bildern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('fake_base_gpt3_v23314.csv')\n",
    "df_fake[\"prompt_dalle\"] = ''\n",
    "df_fake[\"dalle_url\"] = ''\n",
    "df_fake[\"dalle_url_2\"] = ''\n",
    "\n",
    "for index, row in df_fake.iterrows():\n",
    "    catgeoryName = row['categoryName'].lower()\n",
    "    PROMPT = f\"A high quality photo of a lovely stay at a \" + catgeoryName + \",wide shot, Sigma 85mm f/1.4 \"\n",
    "    print (index)\n",
    "    print(PROMPT)\n",
    "    response_url = \"Hallo\"\n",
    "    # response = openai.Image.create(  prompt=PROMPT,n=1,  size=\"256x256\")\n",
    "    # print(response[\"data\"][0][\"url\"])\n",
    "    # store the response in the dataframe \n",
    "    df_fake.at[index, 'promt_dalle'] = PROMPT\n",
    "    df_fake.at[index, 'dalle_url'] = response_url \n",
    "\n",
    "print(\"Done with image generation generation.\")\n",
    "print(df_fake.shape)\n",
    "print(df_fake.columns)\n",
    "print(df_fake['reviewId'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"A high quality photo of a disgusting experience at a hotel, photography, high resolution, 4k, many details\"\n",
    "\n",
    "with open(\"apikey_mama.txt\", \"r\") as f:\n",
    "    openai.api_key = f.read()\n",
    "\n",
    "response = openai.Image.create(\n",
    "    prompt=PROMPT,\n",
    "    n=2,\n",
    "    size=\"256x256\",\n",
    ")\n",
    "\n",
    "print(response[\"data\"][0][\"url\"])\n",
    "\n",
    "with open(\"02_Visualizations\\dalle-2-tries.txt\", \"a\") as f:\n",
    "    f.write(\"\\nPROMPT \" + PROMPT +\": \" + response[\"data\"][0][\"url\"] + \" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text (OUTDATED) ChatGPT3.5 Turbo Verworfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chatGPT: \n",
    "    def __init__(self, api_key, rolle):\n",
    "        openai.api_key = api_key\n",
    "        self.dialog = [{\"role\":\"system\", \"content\":rolle}]\n",
    "\n",
    "    def question(self, question):\n",
    "        self.dialog.append({\"role\":\"user\", \"content\":question})\n",
    "        ergebnis = openai.ChatCompletion.create(model='gpt-3.5-turbo', messages=self.dialog)\n",
    "        antwort = ergebnis[\"choices\"][0].message.content\n",
    "        self.dialog.append({\"role\":\"assistant\", \"content\":antwort})\n",
    "        return antwort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"In stlye of a google user that writes a review about a place he or she visited.\"\n",
    "prompt = \"Write a Google Maps review about a bowling alley with max 200 tokens.\"\n",
    "chatGPT(api_key, role).question(prompt)\n",
    "print(chatGPT(api_key, role).question(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(text):\n",
    "    keywords = model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5)\n",
    "    keywords = [keyword[0] for keyword in keywords]\n",
    "    return keywords\n",
    "\n",
    "output_string= \"I recently visited the Italian Restaurant and overall had a great experience. The atmosphere was cozy and inviting with a classic Italian vibe. The staff was friendly and attentive, making sure we had everything we needed throughout our meal.The menu had a great selection of traditional Italian dishes and some unique options as well. I tried the gnocchi and it was delicious, perfectly cooked with a flavorful sauce. My friend had the lasagna and said it was some of the best she's ever had. The only downside was the wait time for our table, as we had to wait about 30 minutes even though we had reservations. However, the food and service made up for it. Overall, I would recommend the Italian Restaurant to anyone who loves Italian cuisine and wants to have a nice, cozy dinner out with friends or family.\"\n",
    "keywords = get_keywords(output_string)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9abad024bc45f6d7f9bfe9f2e5b4bbf3ee80d4a34082e6860f2ffd5f4e4e7895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
